{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1bc89b464a0a42759ca999736cce5745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1bb7460ee5d14921911f6895d0abf799",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fbdb47a9b5de43ce82924bdc176f8b0a",
              "IPY_MODEL_5ed3cf7808d846dcb8c8f87925eac741",
              "IPY_MODEL_fea4196dd9704b6b9582efc11b2ced6f"
            ]
          }
        },
        "1bb7460ee5d14921911f6895d0abf799": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fbdb47a9b5de43ce82924bdc176f8b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ad309faa3194495fa540eac683c664aa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5268b99020834444901940c85f56a701"
          }
        },
        "5ed3cf7808d846dcb8c8f87925eac741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6f6410968aa743a1b9835eca1447709d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77c57fff48db4ead895a5f04a279c672"
          }
        },
        "fea4196dd9704b6b9582efc11b2ced6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_082b360b26cb457a86747711abf433d1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 339kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d492daa58ad249acbca93bfa3c2add3d"
          }
        },
        "ad309faa3194495fa540eac683c664aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5268b99020834444901940c85f56a701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f6410968aa743a1b9835eca1447709d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77c57fff48db4ead895a5f04a279c672": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "082b360b26cb457a86747711abf433d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d492daa58ad249acbca93bfa3c2add3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "260ab876c4fc40f5b9bf4ce88d0a4a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e003c1f9d14f461f93e97c042481b891",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d005865cab1d496eb2323f9732bb4297",
              "IPY_MODEL_e66618f9598442f7a8aa95c330b92abb",
              "IPY_MODEL_e81316254fc640ddb73f43b58d155c49"
            ]
          }
        },
        "e003c1f9d14f461f93e97c042481b891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d005865cab1d496eb2323f9732bb4297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9ad5f6f227d84865ab33c8a57c53022e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_98c415837e464ace83dc9559a08b6c3f"
          }
        },
        "e66618f9598442f7a8aa95c330b92abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14cf1973ce5e42929197465d2d2d5b54",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea38dc91ea4746148e158de8d5dba740"
          }
        },
        "e81316254fc640ddb73f43b58d155c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c79ca9b9a1dc496fbdad101be28cf930",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 786B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c00878d5dbe84a79b564644b0d14bf2f"
          }
        },
        "9ad5f6f227d84865ab33c8a57c53022e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "98c415837e464ace83dc9559a08b6c3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14cf1973ce5e42929197465d2d2d5b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea38dc91ea4746148e158de8d5dba740": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c79ca9b9a1dc496fbdad101be28cf930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c00878d5dbe84a79b564644b0d14bf2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f21ffd6fbccd45c4ac3bebadf26c9c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8e424aa8f25c431b9960acd311375ffb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2f545cc418264e2f9c89652e65556475",
              "IPY_MODEL_efbcfb67a3704338b5a3c089ac62a8a1",
              "IPY_MODEL_b500e1f989524b66a9824fae4da26945"
            ]
          }
        },
        "8e424aa8f25c431b9960acd311375ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f545cc418264e2f9c89652e65556475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_534e664e4e08457da9312b3842d49e3a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e3018247a4d4d1f89d3f63dbc966d4a"
          }
        },
        "efbcfb67a3704338b5a3c089ac62a8a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_497b9d0822ff4dc3b420aed716bdd0d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ace35eda31334712af73130343ec53b9"
          }
        },
        "b500e1f989524b66a9824fae4da26945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a334ae97058c4ec098cc04601867889c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 683kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c6fceecb9924d5ba10a3c1165a8d219"
          }
        },
        "534e664e4e08457da9312b3842d49e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e3018247a4d4d1f89d3f63dbc966d4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "497b9d0822ff4dc3b420aed716bdd0d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ace35eda31334712af73130343ec53b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a334ae97058c4ec098cc04601867889c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c6fceecb9924d5ba10a3c1165a8d219": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b2e233303d5496f8c4ec6866538ec45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0eb53e9b435a4e9e91ed1efbd41190f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2618e46223b74b728b63e3c6c1787372",
              "IPY_MODEL_c77fd1cf89694874ab605d855ad95504",
              "IPY_MODEL_3f56f6c2344a4c30b65b8488757d4b83"
            ]
          }
        },
        "0eb53e9b435a4e9e91ed1efbd41190f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2618e46223b74b728b63e3c6c1787372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7ae4ddc80db54481b560987c6bbd2484",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89d62af10d344502871879f422ca3700"
          }
        },
        "c77fd1cf89694874ab605d855ad95504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eb9f444f3041488da32ef6fa74c1c907",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58cb57bf54504b2abd20b4736863e185"
          }
        },
        "3f56f6c2344a4c30b65b8488757d4b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1d9ab1933a8c4ef3acf5b5c130d7f0c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 16.7kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1069d1b71034c7eb55802980f177fd8"
          }
        },
        "7ae4ddc80db54481b560987c6bbd2484": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "89d62af10d344502871879f422ca3700": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb9f444f3041488da32ef6fa74c1c907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58cb57bf54504b2abd20b4736863e185": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d9ab1933a8c4ef3acf5b5c130d7f0c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1069d1b71034c7eb55802980f177fd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1dbb2a9fa0ff4b7d813e698f9484e52a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_759c6e1287bf4d2d9182cc23492c0828",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3c604953ba5a44ef9c4be7e84f08e8a7",
              "IPY_MODEL_3b30df9695854793b6f1bf88473d20ed",
              "IPY_MODEL_63f23eae3c8446d98c03d88a8fb1af12"
            ]
          }
        },
        "759c6e1287bf4d2d9182cc23492c0828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c604953ba5a44ef9c4be7e84f08e8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_acbcb06dfe884bab855a2d254a7cd2ac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea11adcda1c14ebb8b2c7a2c50ea857e"
          }
        },
        "3b30df9695854793b6f1bf88473d20ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d3ba7e28885e42f39edaa94c2f120ef8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 536063208,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 536063208,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb90dcd73cec457fbbeaf90d5818d2bb"
          }
        },
        "63f23eae3c8446d98c03d88a8fb1af12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_96370a3f0e5648ed937db6b23c53401c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 536M/536M [00:10&lt;00:00, 51.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_98693b494739426ab057ae8d2b815071"
          }
        },
        "acbcb06dfe884bab855a2d254a7cd2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea11adcda1c14ebb8b2c7a2c50ea857e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3ba7e28885e42f39edaa94c2f120ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb90dcd73cec457fbbeaf90d5818d2bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96370a3f0e5648ed937db6b23c53401c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "98693b494739426ab057ae8d2b815071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ad291db492a4d1b8aa69a7208d1394f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4312b80f2182456a94cfd9d36975564f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a468e89e13934168a2718b4327c13db6",
              "IPY_MODEL_c63e676cbbd8422ca7af91db59bc6659",
              "IPY_MODEL_55a32112bdcd4ec9a1060ca4f7be57f5"
            ]
          }
        },
        "4312b80f2182456a94cfd9d36975564f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a468e89e13934168a2718b4327c13db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5f5d2da65c614d5091df023aacb279bd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f18c736bd9134439b023e46dbfcbc8dc"
          }
        },
        "c63e676cbbd8422ca7af91db59bc6659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8edc65ca8c97465897bf545dee0a9246",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5d1249c48ac46c88d9cd847f1d0413f"
          }
        },
        "55a32112bdcd4ec9a1060ca4f7be57f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_99edb36129be4fa1b06876bf06420acb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 344kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_44f09c14f77146d28145427268002611"
          }
        },
        "5f5d2da65c614d5091df023aacb279bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f18c736bd9134439b023e46dbfcbc8dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8edc65ca8c97465897bf545dee0a9246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5d1249c48ac46c88d9cd847f1d0413f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99edb36129be4fa1b06876bf06420acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "44f09c14f77146d28145427268002611": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ecd2dd344494defb31a795ed18baaaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_592194713aa044218f962a97cea167f1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b7a5030d4aee4a37b307465683254992",
              "IPY_MODEL_55885d99d16945c68c9df8849097cd2d",
              "IPY_MODEL_f97736ea510f47209f63276570e24463"
            ]
          }
        },
        "592194713aa044218f962a97cea167f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b7a5030d4aee4a37b307465683254992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ba4292efb4d049e088e606942154a728",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a68d3a33e12460fa05003b9f7619f83"
          }
        },
        "55885d99d16945c68c9df8849097cd2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6bc3fc3211014660b0e5e6025786c341",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9bec7b473f44c92b6595a8e48996c6d"
          }
        },
        "f97736ea510f47209f63276570e24463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_14e74e2c72b9463da5e153e1981e9f49",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 700B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_697767841a7044d0abc8943f16988349"
          }
        },
        "ba4292efb4d049e088e606942154a728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a68d3a33e12460fa05003b9f7619f83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6bc3fc3211014660b0e5e6025786c341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9bec7b473f44c92b6595a8e48996c6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14e74e2c72b9463da5e153e1981e9f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "697767841a7044d0abc8943f16988349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82595a1be51b47b2bc5dfcfdccff4e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d345115f7cc341faa93cfceb39e672a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_274a103ba08e4602a21da3ae72d0c7e0",
              "IPY_MODEL_5b39929f19244a8cad6c23eeb8b116d6",
              "IPY_MODEL_818042d59c6b4f7cb12c7bf00229abcb"
            ]
          }
        },
        "d345115f7cc341faa93cfceb39e672a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "274a103ba08e4602a21da3ae72d0c7e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_085286364f0540a09c3ea34de9089d76",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0fae6398dd94b8583bd25b74c24eb75"
          }
        },
        "5b39929f19244a8cad6c23eeb8b116d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5781b57987734d1e9012c4b9f641d968",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e192cb1873044af9db3706bf0f920a6"
          }
        },
        "818042d59c6b4f7cb12c7bf00229abcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8f8915dd82504e218eddcf2fae80c55e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 451kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9794830910147e6a98e76b2c3eecd7f"
          }
        },
        "085286364f0540a09c3ea34de9089d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0fae6398dd94b8583bd25b74c24eb75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5781b57987734d1e9012c4b9f641d968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e192cb1873044af9db3706bf0f920a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f8915dd82504e218eddcf2fae80c55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9794830910147e6a98e76b2c3eecd7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57bde018bd3e4383a7e06478a8af3b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_80aa79a7eaa744f2bd6d775ca4593b1e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4fd241c138784f379c77418683bb049b",
              "IPY_MODEL_0fa1c49da19043c98d4e0fde5f7d45e1",
              "IPY_MODEL_4a4311ab37f7460c88735416a4c6e4a1"
            ]
          }
        },
        "80aa79a7eaa744f2bd6d775ca4593b1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4fd241c138784f379c77418683bb049b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f30f705ecfdc4540812fc6cd7bb06f43",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e648a095a0634e5ea6569ebefe064b50"
          }
        },
        "0fa1c49da19043c98d4e0fde5f7d45e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_645f895abc814602bcb33c7c9c0d149b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01999dc18a3d4f2693bf9ddb46e96ff8"
          }
        },
        "4a4311ab37f7460c88735416a4c6e4a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_29706cc4091c4d528d2033858910339c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442/442 [00:00&lt;00:00, 12.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1f092d0088b44288bec29ff4eabffeec"
          }
        },
        "f30f705ecfdc4540812fc6cd7bb06f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e648a095a0634e5ea6569ebefe064b50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "645f895abc814602bcb33c7c9c0d149b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01999dc18a3d4f2693bf9ddb46e96ff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29706cc4091c4d528d2033858910339c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1f092d0088b44288bec29ff4eabffeec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7865560c421a43b98e56c12e9970b151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fac9ed17e7d1488dbc823535e05d46ab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9ad7e89f941d432db99e43c02ea7acf2",
              "IPY_MODEL_8a76912634fd403db6ca570566a39a4d",
              "IPY_MODEL_fafdb0d5dbe4471b9a816b35531fd924"
            ]
          }
        },
        "fac9ed17e7d1488dbc823535e05d46ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ad7e89f941d432db99e43c02ea7acf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e0b0d190a52a4d26a6e8f73e61046863",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a2d0b7a896f499fb72dbffa4a530d5f"
          }
        },
        "8a76912634fd403db6ca570566a39a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d7c703b1d8094a30bb74a9518f48e474",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 363423424,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 363423424,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5f9e6a15c87a44e3bd6c97d6420b7107"
          }
        },
        "fafdb0d5dbe4471b9a816b35531fd924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_02e594755f7645418cfc60d571177e71",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 363M/363M [00:07&lt;00:00, 50.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f034ccf421d4c42aeb1494781e43e8f"
          }
        },
        "e0b0d190a52a4d26a6e8f73e61046863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a2d0b7a896f499fb72dbffa4a530d5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7c703b1d8094a30bb74a9518f48e474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5f9e6a15c87a44e3bd6c97d6420b7107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02e594755f7645418cfc60d571177e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f034ccf421d4c42aeb1494781e43e8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "047085fb30f74f0dae77bc30e54f66ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5284a31f3247468b87927b046fad298e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c731b8f0118142dbac11a77588906e60",
              "IPY_MODEL_e99c20b9d98546ffa98c73c6cc6382d3",
              "IPY_MODEL_59e37d56dff241e68ab0fa4642b0a285"
            ]
          }
        },
        "5284a31f3247468b87927b046fad298e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c731b8f0118142dbac11a77588906e60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_016f2dbf643b4c3d810b5ff11b71181b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_39fa781f4d6645c09369927a2e48967e"
          }
        },
        "e99c20b9d98546ffa98c73c6cc6382d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b5e54f3e430c464c918e988a72ef1dd9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d04edde83c24ec7aaa2886942750f34"
          }
        },
        "59e37d56dff241e68ab0fa4642b0a285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fb6b6e3b1c4c4195b84d6ea42f718261",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 1.03MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1cb5c2119a954fc1bb4c892ebc97a11f"
          }
        },
        "016f2dbf643b4c3d810b5ff11b71181b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "39fa781f4d6645c09369927a2e48967e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b5e54f3e430c464c918e988a72ef1dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d04edde83c24ec7aaa2886942750f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb6b6e3b1c4c4195b84d6ea42f718261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1cb5c2119a954fc1bb4c892ebc97a11f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b4574d194e8407299b93048c8477e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b1cca1ffdec4bd0940b8cb8574f6472",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9326be5c7c0043bdb91057064c5b13e5",
              "IPY_MODEL_41c76385d40a48f2b971a985f6c97292",
              "IPY_MODEL_df4524ce84d8427d91aa30b9d63aa109"
            ]
          }
        },
        "0b1cca1ffdec4bd0940b8cb8574f6472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9326be5c7c0043bdb91057064c5b13e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1e475390e58b4ffc8d8486fed42c3198",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6a85cbcaed64f8eb90dbbaa28d52f0c"
          }
        },
        "41c76385d40a48f2b971a985f6c97292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b8d0b7e6f2fd406cb63e9c61ac2058a8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_593fa558a2264ac18ee91ce3de1eb90e"
          }
        },
        "df4524ce84d8427d91aa30b9d63aa109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6e4f3f7aa2a744908c358e0c6446bb29",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 472kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f6450a2e1e824ac387c0bcad34edca90"
          }
        },
        "1e475390e58b4ffc8d8486fed42c3198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6a85cbcaed64f8eb90dbbaa28d52f0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b8d0b7e6f2fd406cb63e9c61ac2058a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "593fa558a2264ac18ee91ce3de1eb90e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6e4f3f7aa2a744908c358e0c6446bb29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f6450a2e1e824ac387c0bcad34edca90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3ada041d0404875ae888a78def96270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_34babe09f7564360bf52a130a0eb8796",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dc13ed3875334ee7a291e083dc0e6e56",
              "IPY_MODEL_e14730124df2449aad8565757ce9fa6b",
              "IPY_MODEL_81a98778e70b49759155c2c68a296ece"
            ]
          }
        },
        "34babe09f7564360bf52a130a0eb8796": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc13ed3875334ee7a291e083dc0e6e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_44fa5fd2c7f5408bb14b71925a9c0fec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8ae93a4e63a4e078837a226e7ad585b"
          }
        },
        "e14730124df2449aad8565757ce9fa6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_006a76227aec4057a3705393e7bfef23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38e49c308792438e9d216c05c170d9c3"
          }
        },
        "81a98778e70b49759155c2c68a296ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d76ae1a3d43348aba518d739c5c30157",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:01&lt;00:00, 1.61MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f6dfd0590c24232bfda21a1a7d9980d"
          }
        },
        "44fa5fd2c7f5408bb14b71925a9c0fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8ae93a4e63a4e078837a226e7ad585b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "006a76227aec4057a3705393e7bfef23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38e49c308792438e9d216c05c170d9c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d76ae1a3d43348aba518d739c5c30157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f6dfd0590c24232bfda21a1a7d9980d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e8df76fb5f63473ebdd0228b46f5c735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ef84f418d42040878415b7f5df3fc278",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_35f81e05ac0b4dbbbb18055054ef7401",
              "IPY_MODEL_5bdc17d3574b479d84f0ea9ce8c0d1f9",
              "IPY_MODEL_3fd05eb8422546ef8923b16e63e6edfb"
            ]
          }
        },
        "ef84f418d42040878415b7f5df3fc278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35f81e05ac0b4dbbbb18055054ef7401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6f326629c81f4fc2a77d5a83336547e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f58bbf25779a40788577c8907bcf9adc"
          }
        },
        "5bdc17d3574b479d84f0ea9ce8c0d1f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_897a87741d01423ab9e2d1494ee4e8b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b30de39e82e14b6aa334d68076b53d82"
          }
        },
        "3fd05eb8422546ef8923b16e63e6edfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9c2fe6335bdd4ed698bf201ffcbe33b1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:00&lt;00:00, 13.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2bd8c97d2eb94333a86aaebb7cf2cc62"
          }
        },
        "6f326629c81f4fc2a77d5a83336547e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f58bbf25779a40788577c8907bcf9adc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "897a87741d01423ab9e2d1494ee4e8b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b30de39e82e14b6aa334d68076b53d82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c2fe6335bdd4ed698bf201ffcbe33b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2bd8c97d2eb94333a86aaebb7cf2cc62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b6164c23a594fb4b39f693d6b67144a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_df5deae47e8649d0ad7b232e448e8a5c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6331b2a99377403298d9b6a4a4edd09f",
              "IPY_MODEL_c24532f120b64d61a4552bb0abc52c22",
              "IPY_MODEL_fafb2632ebf44651b4981a2075258cbc"
            ]
          }
        },
        "df5deae47e8649d0ad7b232e448e8a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6331b2a99377403298d9b6a4a4edd09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2b7adb1b5b734c21aa3cd13c1241bf7d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8bbf13678eea4fb9b4f7784764bbbbcb"
          }
        },
        "c24532f120b64d61a4552bb0abc52c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2d15ea9cdf164672bf30bfdd6d35bec9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 657434796,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 657434796,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8df8082e3708414eb3e187da572b2c70"
          }
        },
        "fafb2632ebf44651b4981a2075258cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6261c905d668418cb56b60410247896c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 657M/657M [00:12&lt;00:00, 51.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_18a66a5342e74cb2ad6a84f7d6c8f4ba"
          }
        },
        "2b7adb1b5b734c21aa3cd13c1241bf7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8bbf13678eea4fb9b4f7784764bbbbcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d15ea9cdf164672bf30bfdd6d35bec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8df8082e3708414eb3e187da572b2c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6261c905d668418cb56b60410247896c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "18a66a5342e74cb2ad6a84f7d6c8f4ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83cae10301884d0db9ef998fb680b8c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4f0d51e5f22f451e9d368e645b82c91d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fe7e8e62ce7840dfa35d2c6282131a01",
              "IPY_MODEL_4858d767d45048f1a09004d54fc3783e",
              "IPY_MODEL_396033aca22d494981b3114f69592e9f"
            ]
          }
        },
        "4f0d51e5f22f451e9d368e645b82c91d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe7e8e62ce7840dfa35d2c6282131a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_33b7b7162cc94071818d0552949131d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45148355ce174a1d8303a22a672ee995"
          }
        },
        "4858d767d45048f1a09004d54fc3783e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1fdf1ff6836448448ddea70ea242e3ec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c641ab1e85f048bdae17e4445c481f70"
          }
        },
        "396033aca22d494981b3114f69592e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4e97f304624940b6b59b2c7aa1f10123",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 300kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fd849249e74348fcba3d2396811c0df3"
          }
        },
        "33b7b7162cc94071818d0552949131d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "45148355ce174a1d8303a22a672ee995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1fdf1ff6836448448ddea70ea242e3ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c641ab1e85f048bdae17e4445c481f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e97f304624940b6b59b2c7aa1f10123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fd849249e74348fcba3d2396811c0df3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85ecefb004044903acec77ea9fcbe562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2a7ab256553e41f1a86f3d51e3dc421f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_18d0215066ec450da2851df40dd00f00",
              "IPY_MODEL_0eb73ddd04b848f5aa86a3d1883870d7",
              "IPY_MODEL_0d3ee564c82144acada2397a15e0765c"
            ]
          }
        },
        "2a7ab256553e41f1a86f3d51e3dc421f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18d0215066ec450da2851df40dd00f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bb3c7d3b5a0a46eb877e560e83eeffb3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4b13e4b5671e42679ac57bc230a101b5"
          }
        },
        "0eb73ddd04b848f5aa86a3d1883870d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c259509efc1e4484b04e87c8880f4bd8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 29,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 29,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0a3ce617ec34fe39e76d2aea4c70c8d"
          }
        },
        "0d3ee564c82144acada2397a15e0765c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b23d8d8685844441a190adba05c25282",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29.0/29.0 [00:00&lt;00:00, 858B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_767c2044d20041e5b65ac6bd5ea3fcd7"
          }
        },
        "bb3c7d3b5a0a46eb877e560e83eeffb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4b13e4b5671e42679ac57bc230a101b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c259509efc1e4484b04e87c8880f4bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0a3ce617ec34fe39e76d2aea4c70c8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b23d8d8685844441a190adba05c25282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "767c2044d20041e5b65ac6bd5ea3fcd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb23a70eaccf41989d3e94b2936a48f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2a89d3c97a9c41099b7e0176c9bd80d2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aac84d5a09b54fa487c0be34209162c2",
              "IPY_MODEL_335a780478424cc7ac96938c9ccaad4b",
              "IPY_MODEL_cda670f8490a49c1ae59d922258a8ec1"
            ]
          }
        },
        "2a89d3c97a9c41099b7e0176c9bd80d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aac84d5a09b54fa487c0be34209162c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_787e6dd55a11426c9a7faa7c33da1993",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1969313b0d244c62a0549fd2e446b59b"
          }
        },
        "335a780478424cc7ac96938c9ccaad4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_50ae3b4e1a8e4f8db096153326c56065",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_84293a7365ef435e9bb18156afc25a6d"
          }
        },
        "cda670f8490a49c1ae59d922258a8ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b2acc80f35374b1cb259fabaa018efac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 659kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7c15616a49b84e9cad517916fcec16b4"
          }
        },
        "787e6dd55a11426c9a7faa7c33da1993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1969313b0d244c62a0549fd2e446b59b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50ae3b4e1a8e4f8db096153326c56065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "84293a7365ef435e9bb18156afc25a6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2acc80f35374b1cb259fabaa018efac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7c15616a49b84e9cad517916fcec16b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a45fbb288134488879b641114ed0bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ef1b7b8f807c46b8a4706608c4e2a213",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a4f1c3b50850406aa07bc0a011b1c3af",
              "IPY_MODEL_38e2107faba048ac92bb856ef03dccb8",
              "IPY_MODEL_2b64310279ac467db553e3bc0b201442"
            ]
          }
        },
        "ef1b7b8f807c46b8a4706608c4e2a213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4f1c3b50850406aa07bc0a011b1c3af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4d52e8addb0a4713815d58a80e220d10",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c2976880e4f54a8dba19778e485138bc"
          }
        },
        "38e2107faba048ac92bb856ef03dccb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_98be053bcfeb49378a78be32b9e5b793",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 665,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 665,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1369281495784322a5cfd8dc7248e6be"
          }
        },
        "2b64310279ac467db553e3bc0b201442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6f7a661522d94a4bbccf3ceca04176d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 665/665 [00:00&lt;00:00, 22.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09daa6ed88bb41fb9b66665ca7953790"
          }
        },
        "4d52e8addb0a4713815d58a80e220d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c2976880e4f54a8dba19778e485138bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98be053bcfeb49378a78be32b9e5b793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1369281495784322a5cfd8dc7248e6be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f7a661522d94a4bbccf3ceca04176d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09daa6ed88bb41fb9b66665ca7953790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "706db006b0734feebcfec2adcbe41128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_33db4f70411a4e9289668203d42ad167",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c7d61b0ad9104502b7ceca452cfe92a6",
              "IPY_MODEL_c313b7fda794418d85c78e96243e51f9",
              "IPY_MODEL_1cf966ecc2464682b0beb1204720d0ad"
            ]
          }
        },
        "33db4f70411a4e9289668203d42ad167": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7d61b0ad9104502b7ceca452cfe92a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4355dfc619254294926bd4c02f800923",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_725c6fe8b52f4830937e36c5452a64fe"
          }
        },
        "c313b7fda794418d85c78e96243e51f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6b481346deda4432897b6bb96f72066a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 54466044,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 54466044,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11c404741e75477cbc68f68a0c1e798f"
          }
        },
        "1cf966ecc2464682b0beb1204720d0ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5280e616d09a49d89ed8da16c70b5b24",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 54.5M/54.5M [00:05&lt;00:00, 12.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f40d122bc0e4dbfa8cccd2d0c7ded4b"
          }
        },
        "4355dfc619254294926bd4c02f800923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "725c6fe8b52f4830937e36c5452a64fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b481346deda4432897b6bb96f72066a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11c404741e75477cbc68f68a0c1e798f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5280e616d09a49d89ed8da16c70b5b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f40d122bc0e4dbfa8cccd2d0c7ded4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18fb4bc6dd48459dbb3ab95586b3876c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7c619a2e87ec47e2938baf7782feb785",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_895be44dfa604b9483ee95cedc1b649a",
              "IPY_MODEL_5671b45347384167bb07d1abadfdb0ce",
              "IPY_MODEL_c5b45b35855d456db62c6e50a5c924f5"
            ]
          }
        },
        "7c619a2e87ec47e2938baf7782feb785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "895be44dfa604b9483ee95cedc1b649a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f8055d772eca49cca9d4a9d590d9a366",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d284bc664b446c584c44ed2f9dd519a"
          }
        },
        "5671b45347384167bb07d1abadfdb0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6ce40f2ce4e5442fbcfc641e44ebf67a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 798011,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 798011,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93aae4bafa6446b2ac3b24114aeac89b"
          }
        },
        "c5b45b35855d456db62c6e50a5c924f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7954b30193884e6fa093140ef9d57359",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 798k/798k [00:00&lt;00:00, 926kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a3affd358fba4d61b07f7148befc6f6e"
          }
        },
        "f8055d772eca49cca9d4a9d590d9a366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d284bc664b446c584c44ed2f9dd519a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ce40f2ce4e5442fbcfc641e44ebf67a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93aae4bafa6446b2ac3b24114aeac89b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7954b30193884e6fa093140ef9d57359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a3affd358fba4d61b07f7148befc6f6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0a9cd21c63d4717b2dd06f7e6bb154b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0f290214dc1144cd8acc84360342a33e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8ecaaee415ae4f6f92724fd77087308e",
              "IPY_MODEL_2573b86f82f545f88c016aefa01500e4",
              "IPY_MODEL_10344061cb604a3da9b7ac93a88c491e"
            ]
          }
        },
        "0f290214dc1144cd8acc84360342a33e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ecaaee415ae4f6f92724fd77087308e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bf92ff70ac4e47d48f00c8d4a4e4acf9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc9e538bfe4f4172a86f54d42f341e7c"
          }
        },
        "2573b86f82f545f88c016aefa01500e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b6095f9287d84b099edd7c5ff8205585",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1382015,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1382015,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e82054308b8245959a2d820a22fc5214"
          }
        },
        "10344061cb604a3da9b7ac93a88c491e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4be78969bd0547bc81c792ed7e30389d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.38M/1.38M [00:01&lt;00:00, 1.94MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2c6937fe0e54e95996d65b0697e4708"
          }
        },
        "bf92ff70ac4e47d48f00c8d4a4e4acf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc9e538bfe4f4172a86f54d42f341e7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6095f9287d84b099edd7c5ff8205585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e82054308b8245959a2d820a22fc5214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4be78969bd0547bc81c792ed7e30389d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2c6937fe0e54e95996d65b0697e4708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0323cab25574abab5a3e3d2e0fe6ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_27884f2787974c44b8d3d66ac066dec5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_622f1b4133c24a0d98d74e23c8e9b499",
              "IPY_MODEL_df0c11ad643047db9c56026bdc2d516c",
              "IPY_MODEL_e5d213d611e549909bf5dfb0d091b2c3"
            ]
          }
        },
        "27884f2787974c44b8d3d66ac066dec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "622f1b4133c24a0d98d74e23c8e9b499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1dd9f0eb48a34b42972b1a26cf603df0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0962ecdb5611408a8d6b5f930f56d688"
          }
        },
        "df0c11ad643047db9c56026bdc2d516c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aa07eace19504d14b331086f229634e0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 760,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05964f643aae4750aeb0137f3a383273"
          }
        },
        "e5d213d611e549909bf5dfb0d091b2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_60da7387ca954436942c7cfd23093d82",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 760/760 [00:00&lt;00:00, 26.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_192c5f7c59994803872cbfe54723fec8"
          }
        },
        "1dd9f0eb48a34b42972b1a26cf603df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0962ecdb5611408a8d6b5f930f56d688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa07eace19504d14b331086f229634e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05964f643aae4750aeb0137f3a383273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60da7387ca954436942c7cfd23093d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "192c5f7c59994803872cbfe54723fec8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b8244ab9cb7479f8fe0718d1b40bf7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_af58a7ed94c54a0fabd0ae2bf6139745",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3929ac3d82144c1f9ce765b333209080",
              "IPY_MODEL_fddee7d4ebe54b9aaa04b9ad235a0078",
              "IPY_MODEL_f156aae8a7f1456b8c14f1f8d8634f30"
            ]
          }
        },
        "af58a7ed94c54a0fabd0ae2bf6139745": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3929ac3d82144c1f9ce765b333209080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8a37b006e74f476ea4d27d5cdddee73b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1f15e9b95cd646c8b56ce09610c58604"
          }
        },
        "fddee7d4ebe54b9aaa04b9ad235a0078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_85059a7cdcee46178b00f580aa5bbb95",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 565485600,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 565485600,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ae5517f2a0e4b2d98bfca1093c06aa0"
          }
        },
        "f156aae8a7f1456b8c14f1f8d8634f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b3d01e7bb644483192ad62daa1f36dbe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 565M/565M [00:10&lt;00:00, 53.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5847e3ce41034c2c896910ecf4cf1b48"
          }
        },
        "8a37b006e74f476ea4d27d5cdddee73b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1f15e9b95cd646c8b56ce09610c58604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85059a7cdcee46178b00f580aa5bbb95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ae5517f2a0e4b2d98bfca1093c06aa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3d01e7bb644483192ad62daa1f36dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5847e3ce41034c2c896910ecf4cf1b48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-Tfuv_FLMqg",
        "outputId": "7cab9ea6-9ab7-4cb4-a227-9ffd3dd29fe6"
      },
      "source": [
        "# misc\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import ast\n",
        "import warnings\n",
        "import math\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import plot_importance\n",
        "import seaborn as sns\n",
        "\n",
        "# data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# ML\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "\n",
        "!pip install scikit-plot\n",
        "import scikitplot as skplt\n",
        "import xgboost "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmC9BSNA8Rvo"
      },
      "source": [
        "#### Import and Preprocess Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6j_HE2fSE_M",
        "outputId": "465a1fe1-8aad-451c-d7bb-716ca22a6053"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/Datasets')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgXK7HB00n-A"
      },
      "source": [
        "headlines = pd.read_excel(\"Headlines_Tokenized.xlsx\")\n",
        "df = pd.read_csv(\"DatasetFull.csv\", encoding = 'latin-1')\n",
        "af_headlines = pd.read_csv(\"AdFontes Article Classifications.csv\", encoding = \"latin-1\")\n",
        "babe = pd.read_csv(\"final_labels_SG2.xlsx - Sheet1.csv\")\n",
        "df_copy = pd.read_csv(\"DatasetFull.csv\", encoding = 'latin-1')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHN666otgBEE",
        "outputId": "164513cd-6020-4471-e55d-14202be6c280"
      },
      "source": [
        "print(df[\"Bias\"].value_counts())\n",
        "print(\"Total dataset length:\", len(df[\"Bias\"]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AllSides Media Bias Rating: Center             4168\n",
            "AllSides Media Bias Rating: Lean Left          4029\n",
            "AllSides Media Bias Rating: Lean Right         3862\n",
            "AllSides Media Bias Rating: Left               2847\n",
            "AllSides Media Bias Rating: Right              1635\n",
            "Political News Media Bias Rating: Not Rated      26\n",
            "AllSides Media Bias Rating: Mixed                17\n",
            "Name: Bias, dtype: int64\n",
            "Total dataset length: 16584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_0dwO1vkHXL",
        "outputId": "36c96728-e7dd-47aa-a66e-21267d1028ad"
      },
      "source": [
        "print(len(af_headlines))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PByf_DlSu5aD"
      },
      "source": [
        "df['Label'] = df.Bias.apply(lambda x: 1 if x == \"AllSides Media Bias Rating: Left\" or x == \"AllSides Media Bias Rating: Right\" else (0 if x == \"AllSides Media Bias Rating: Center\" else -99))\n",
        "headlines['Label'] = headlines.Bias.apply(lambda x: 1 if x == \"AllSides Media Bias Rating: Left\" or x == \"AllSides Media Bias Rating: Right\" else (0 if x == \"AllSides Media Bias Rating: Center\" else -99))\n",
        "af_headlines[\"Label\"] = af_headlines.Bias.apply(lambda x: 1 if (x > 10) | (x < -10) else (0 if (x < 5) & (x > -5) else -99))\n",
        "babe['Label'] = babe.label_bias.apply(lambda x: 1 if x == \"Biased\" else 0)\n",
        "df_copy['Label'] = df_copy.Bias.apply(lambda x: 0 if x == \"AllSides Media Bias Rating: Center\" else 1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUr6NngZ_Kec"
      },
      "source": [
        "# resampled dataframe to contain equal number of biased and neutral observations, where biased is defined as everything that is not rated \"center\"\n",
        "new_df = df_copy.groupby('Label').apply(lambda x: x.sample(n=4168)).reset_index(drop = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn0DqrozC7vA",
        "outputId": "ce221715-b3e2-43d8-c426-95626d563b82"
      },
      "source": [
        "new_df['Label'].value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    4168\n",
              "0    4168\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHEKfnuDu5tD"
      },
      "source": [
        "df = df[df[\"Label\"] > -50]\n",
        "df = df.reset_index()\n",
        "headlines = headlines[headlines[\"Label\"] > -50]\n",
        "headlines = headlines.reset_index()\n",
        "af_headlines = af_headlines[af_headlines[\"Label\"] > -50]\n",
        "af_headlines = af_headlines.reset_index()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFimogqas8hl",
        "outputId": "7c7b4d4b-64a0-4b49-8474-7d3ca2c68c95"
      },
      "source": [
        "# cut observations with less than 25 characters\n",
        "\n",
        "df['Cut'] = None\n",
        "for index, row in df.iterrows():\n",
        "    if len(df[\"Headline\"][index]) < 25:\n",
        "        df[\"Cut\"][index] = 0\n",
        "    else:\n",
        "        df[\"Cut\"][index] = 1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "pvqlmQZQKh6B",
        "outputId": "de342aae-8664-48b0-ab28-ce3ac814acd9"
      },
      "source": [
        "# examples of unbiased headlines \n",
        "df[df['Cut'] == 0].sample(n = 5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Source</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Text</th>\n",
              "      <th>Bias</th>\n",
              "      <th>Subject_Tag</th>\n",
              "      <th>Date</th>\n",
              "      <th>Label</th>\n",
              "      <th>Cut</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4740</th>\n",
              "      <td>8563</td>\n",
              "      <td>The American Spectator</td>\n",
              "      <td>Predictions for 2018</td>\n",
              "      <td>If you liked 2017, youll love 2018.</td>\n",
              "      <td>AllSides Media Bias Rating: Right</td>\n",
              "      <td>Holidays</td>\n",
              "      <td>January 1st, 2018</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6059</th>\n",
              "      <td>11144</td>\n",
              "      <td>HuffPost</td>\n",
              "      <td>Texas Toast</td>\n",
              "      <td>Facing an increasingly narrow path to the nomi...</td>\n",
              "      <td>AllSides Media Bias Rating: Left</td>\n",
              "      <td>Presidential Elections, Elections</td>\n",
              "      <td>May 3rd, 2016</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>2648</td>\n",
              "      <td>Rich Lowry</td>\n",
              "      <td>Mark Zuckerberg Is Right</td>\n",
              "      <td>ANALYSIS\\r\\nMark Zuckerberg clearly hasnt got...</td>\n",
              "      <td>AllSides Media Bias Rating: Right</td>\n",
              "      <td>Free Speech, Civil Rights, Facebook, Social Me...</td>\n",
              "      <td>July 10th, 2020</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1660</th>\n",
              "      <td>3039</td>\n",
              "      <td>American Psychological Association</td>\n",
              "      <td>COVID-19 and suicide</td>\n",
              "      <td>COVID-19 has brought a raft of intense new str...</td>\n",
              "      <td>AllSides Media Bias Rating: Center</td>\n",
              "      <td>Science, Suicide, Coronavirus, Mental Health, ...</td>\n",
              "      <td>May 25th, 2020</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5393</th>\n",
              "      <td>9735</td>\n",
              "      <td>Breitbart News</td>\n",
              "      <td>House Smashes Obamacare</td>\n",
              "      <td>House Republicans passed a revised version of ...</td>\n",
              "      <td>AllSides Media Bias Rating: Right</td>\n",
              "      <td>Healthcare</td>\n",
              "      <td>May 4th, 2017</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      index                              Source  ... Label Cut\n",
              "4740   8563              The American Spectator  ...     1   0\n",
              "6059  11144                            HuffPost  ...     1   0\n",
              "1439   2648                          Rich Lowry  ...     1   0\n",
              "1660   3039  American Psychological Association  ...     0   0\n",
              "5393   9735                      Breitbart News  ...     1   0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SesYk5auG1D"
      },
      "source": [
        "df = df[df[\"Cut\"] == 1]\n",
        "df = df.reset_index()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BUS53i_wy83"
      },
      "source": [
        "df = df.drop(\"level_0\", axis = 1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30EdUlvHlwNi"
      },
      "source": [
        "df = df.drop(\"Cut\", axis = 1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkLSQCf6umrS",
        "outputId": "7300f270-189b-480b-a0e8-a9618883faf3"
      },
      "source": [
        "# cut headlines with less than 25 characters\n",
        "af_headlines['Cut'] = None\n",
        "for index, row in af_headlines.iterrows():\n",
        "    if len(af_headlines[\"Headline\"][index]) < 25:\n",
        "        af_headlines[\"Cut\"][index] = 0\n",
        "    else:\n",
        "        af_headlines[\"Cut\"][index] = 1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM1Kv8QYuv8n"
      },
      "source": [
        "af_headlines = af_headlines[af_headlines[\"Cut\"] == 1]\n",
        "af_headlines = af_headlines.reset_index()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr9UtARUtA4g"
      },
      "source": [
        "af_headlines = af_headlines.drop(\"level_0\", axis = 1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZtv7Durl2oi"
      },
      "source": [
        "af_headlines = af_headlines.drop(\"Cut\", axis = 1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoFVR4K4vRut",
        "outputId": "abf528bf-42ce-48ae-cd63-d2b9caf549b6"
      },
      "source": [
        "df['Label'].value_counts()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    4423\n",
              "0    4153\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylN-YjMD8Rvv",
        "outputId": "aad011b4-f314-4dc4-9344-29b39c788974"
      },
      "source": [
        "af_headlines['Label'].value_counts()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2497\n",
              "1    1887\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGx6S5zcRj79",
        "outputId": "8a67bec1-ddc8-4428-a149-c331bbf3c9d3"
      },
      "source": [
        "babe['Label'].value_counts()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1864\n",
              "1    1810\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvzCtUzNw7QT",
        "outputId": "4331813f-eae2-410e-d913-c990344eb46e"
      },
      "source": [
        "new_df['Label'].value_counts()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    4168\n",
              "0    4168\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFSizPkyvxE9"
      },
      "source": [
        "x = df.drop(['Label', 'Bias'], 1)\n",
        "y = df[['Label']]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQOhqyfXsn8G"
      },
      "source": [
        "x_b = babe[['text']]\n",
        "y_b = babe[['Label']]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk3A8fzzv8Re"
      },
      "source": [
        "# train-test split\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(x, y,\n",
        "                                                                                                             test_size = 0.10, random_state = 42)\n",
        "train_features1, val_features, train_labels1, val_labels = train_test_split(train_features, train_labels,\n",
        "                                                                                                             test_size = 0.10, random_state = 42)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnmOBC8qHGcj",
        "outputId": "73ca9a0b-61ae-43b4-e0f1-4ecfd481968a"
      },
      "source": [
        "# lets see a couple examples to use in our section on politcal bias vs media bias\n",
        "import random\n",
        "bias_babe = babe[babe[\"Label\"] == 1]\n",
        "bias_babe.reset_index(inplace = True)\n",
        "bias_df = df[df[\"Label\"] == 1]\n",
        "bias_df.reset_index(inplace = True)\n",
        "bias_af = af_headlines[af_headlines['Label'] == 1]\n",
        "bias_af.reset_index(inplace = True)\n",
        "random.seed(1014)\n",
        "ran_babe = random.randint(0, len(bias_babe))\n",
        "ran_af = random.randint(0, len(bias_af))\n",
        "print(bias_babe[\"text\"].iloc[ran_babe])\n",
        "print(bias_af[\"Headline\"].iloc[ran_af])\n",
        "print(\"--------------------------------\")\n",
        "ran_babe = random.randint(0, len(bias_babe))\n",
        "ran_af = random.randint(0, len(bias_af))\n",
        "print(bias_babe[\"text\"].iloc[ran_babe])\n",
        "print(bias_af[\"Headline\"].iloc[ran_af])\n",
        "print(\"--------------------------------\")\n",
        "ran_babe = random.randint(0, len(bias_babe))\n",
        "ran_af = random.randint(0, len(bias_af))\n",
        "print(bias_babe[\"text\"].iloc[ran_babe])\n",
        "print(bias_af[\"Headline\"].iloc[ran_af])\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "President of Belarus refuses to cancel sports and scoffs at ‘Psychosis’ of coronavirus measures.\n",
            "Bigot, Alleged Pedophile Roy Moore Tops GOP Senate Field in Poll\n",
            "--------------------------------\n",
            "For one thing, Trump’s impatience now threatens to undo the measures of the last ten weeks. Thanks in part to the foment of anti-lockdown protesters tacitly endorsed by Trump, governors are under increasing political pressure to reopen quickly—perhaps too soon.\n",
            "Media Fail to Identify Xenophobia as Biden Says Trump Rolled Over for Chinese\n",
            "--------------------------------\n",
            "After Donald Trump’s outrageous photo op in which he used federal police to attack peaceful protesters so that he could walk to a church to hold up a Bible, others noted how inappropriate it was for General Mark Milley, the current chairman of the Joint Chiefs of Staff, to not only be tailing behind Donald Trump during this publicized stunt but to be wearing his battle fatigues, something extremely inappropriate for the setting.\n",
            "If voter fraud goes unpunished, America will never again have a free and fair election\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXpmfka-8Rv1"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjAqOS82LMrA",
        "outputId": "1803a688-4d63-479b-b76b-4bb16a2388fb"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 57.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B8AOAvmLMrA"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "from typing import Dict, List, Optional, Union\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, BertConfig, TFBertForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
        "from transformers import ElectraTokenizer, TFElectraForSequenceClassification\n",
        "from transformers import XLNetTokenizer, TFXLNetForSequenceClassification\n",
        "from transformers import LongformerTokenizer, TFLongformerForSequenceClassification\n",
        "from transformers import T5Tokenizer, T5Model\n",
        "from transformers import GPT2Tokenizer, GPT2Model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20GCnLmjLMrB"
      },
      "source": [
        "# set seed, TF uses python ramdom and numpy library, so these must also be fixed\n",
        "tf.random.set_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "os.environ['PYTHONHASHSEED']=str(0)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '0'"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt3JqjmhLMrB",
        "outputId": "9d804a88-c173-4acb-a5f8-816fafb04a87"
      },
      "source": [
        "# see if hardware accelerator available\n",
        "tf.config.experimental.list_physical_devices() "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IdraPgyYLMrC",
        "outputId": "a765a160-1f23-434a-cf80-aca1353e1703"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ_HlDUaLMrD"
      },
      "source": [
        "# Stratified k-Fold instance\n",
        "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6LoyAK6LMrD"
      },
      "source": [
        "# helper functions called in skfold loop\n",
        "\n",
        "def pd_to_tf(df):\n",
        "    \"\"\"convert a pandas dataframe into a tensorflow dataset\"\"\"\n",
        "    target = df.pop('Label')\n",
        "    headline = df.pop('Headline')\n",
        "    return tf.data.Dataset.from_tensor_slices((headline.values, target.values))\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history['val_'+metric], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, 'val_'+metric])\n",
        "    plt.show()\n",
        "\n",
        "def tokenize(df):\n",
        "    \"\"\"convert a pandas dataframe into a tensorflow dataset and run hugging face's tokenizer on data\"\"\"\n",
        "    target = df.pop('Label')\n",
        "    headline = df.pop('Headline')\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "                        headline.tolist(),                      \n",
        "                        add_special_tokens = True, # add [CLS], [SEP]\n",
        "                        truncation = True, # cut off at max length of the text that can go to BERT\n",
        "                        padding = True, # add [PAD] tokens\n",
        "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "              )\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (dict(train_encodings), \n",
        "         target.tolist()))\n",
        "    return dataset"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxFfoAFHLMrE"
      },
      "source": [
        "def run_model_5fold(df_train, model_name, freeze_encoder=True, pretrained=False, plot=False):\n",
        "    \"\"\"\"freeze flags whether encoder layer should be frozen to not destroy transfer learning. Only set to false when enough data is provided\"\"\"\n",
        "\n",
        "    # these variables will be needed for skfold to select indices\n",
        "    Y = df_train['Label']\n",
        "    X = df_train['Headline']\n",
        "\n",
        "    # hyperparams\n",
        "    BUFFER_SIZE = 10000\n",
        "    BATCH_SIZE = 32\n",
        "    k = 1\n",
        "\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "    val_prec = []\n",
        "    val_rec = []\n",
        "    val_f1 = []\n",
        "    val_f1_micro = []\n",
        "    val_f1_wmacro = []\n",
        "\n",
        "    for train_index, val_index in skfold.split(X,Y):\n",
        "        print('### Start fold {}'.format(k))\n",
        "\n",
        "        # split into train and validation set\n",
        "        train_dataset = df_train.iloc[train_index]\n",
        "        val_dataset = df_train.iloc[val_index]\n",
        "\n",
        "        # prepare data for transformer\n",
        "        train_dataset = tokenize(train_dataset)\n",
        "        val_dataset = tokenize(val_dataset)\n",
        "\n",
        "        # mini-batch it\n",
        "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "        # create new model\n",
        "        if model_name == 'bert':\n",
        "            model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "        if model_name == 'distilbert':\n",
        "            model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "        elif model_name == 'roberta':\n",
        "            model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "        elif model_name == 'electra':\n",
        "            model = TFElectraForSequenceClassification.from_pretrained('google/electra-small-discriminator')\n",
        "        elif model_name == 'xlnet':\n",
        "            model = TFXLNetForSequenceClassification.from_pretrained('xlnet-base-cased')\n",
        "        elif model_name == 't5':\n",
        "            model = T5Model.from_pretrained('t5-base')\n",
        "        elif model_name == 'gpt':\n",
        "            model = GPT2ForSequenceClassification.from_pretrained('microsoft/DialogRPT-updown')\n",
        "            \n",
        "\n",
        "\n",
        "        if freeze_encoder == True:\n",
        "            for w in model.get_layer(index=0).weights:\n",
        "                w._trainable = False\n",
        "\n",
        "        # compile it\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
        "        model.compile(optimizer=optimizer, loss=model.compute_loss) \n",
        "\n",
        "        # transfer learning\n",
        "        if pretrained == True:\n",
        "            model.get_layer(index=0).set_weights(trained_model_layer) # load bias-specific weights\n",
        "            #model.load_weights('./checkpoints/')\n",
        "\n",
        "        # after 2 epochs without improvement, stop training\n",
        "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "\n",
        "        # fit it\n",
        "        history = model.fit(train_dataset, epochs=10, validation_data = val_dataset, callbacks=[callback])\n",
        "\n",
        "        # plot history\n",
        "        if plot:\n",
        "            plot_graphs(history,'loss')\n",
        "\n",
        "        # evaluate\n",
        "        loss = model.evaluate(val_dataset)\n",
        "\n",
        "        if model_name == 'xlnet':\n",
        "            yhats = []\n",
        "            for row in df_train.iloc[val_index]['Headline']:\n",
        "                input = tokenizer(row, return_tensors=\"tf\")\n",
        "                output = model(input)\n",
        "                logits = output.logits.numpy()[0]\n",
        "                candidates = logits.tolist()\n",
        "                decision = candidates.index(max(candidates))\n",
        "                yhats.append(decision)\n",
        "        else:\n",
        "            logits = model.predict(val_dataset)  \n",
        "            yhats = []\n",
        "            for i in logits[0]:\n",
        "                # assign class label according to highest logit\n",
        "                candidates = i.tolist()\n",
        "                decision = candidates.index(max(candidates))\n",
        "                yhats.append(decision)\n",
        "\n",
        "        y = []\n",
        "        for text, label in val_dataset.unbatch():   \n",
        "              y.append(label.numpy())\n",
        "\n",
        "        val_loss.append(loss)\n",
        "        val_acc.append(accuracy_score(y, yhats))\n",
        "        val_prec.append(precision_score(y, yhats))\n",
        "        val_rec.append(recall_score(y, yhats))\n",
        "        val_f1.append(f1_score(y, yhats))\n",
        "        val_f1_micro.append(f1_score(y, yhats, average='micro'))\n",
        "        val_f1_wmacro.append(f1_score(y, yhats, average='weighted'))\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "        k += 1\n",
        "\n",
        "        \n",
        "\n",
        "    return val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PCUonVksn8V"
      },
      "source": [
        "### AllSides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-pr9pcTsn8X"
      },
      "source": [
        "#### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1bc89b464a0a42759ca999736cce5745",
            "1bb7460ee5d14921911f6895d0abf799",
            "fbdb47a9b5de43ce82924bdc176f8b0a",
            "5ed3cf7808d846dcb8c8f87925eac741",
            "fea4196dd9704b6b9582efc11b2ced6f",
            "ad309faa3194495fa540eac683c664aa",
            "5268b99020834444901940c85f56a701",
            "6f6410968aa743a1b9835eca1447709d",
            "77c57fff48db4ead895a5f04a279c672",
            "082b360b26cb457a86747711abf433d1",
            "d492daa58ad249acbca93bfa3c2add3d",
            "260ab876c4fc40f5b9bf4ce88d0a4a99",
            "e003c1f9d14f461f93e97c042481b891",
            "d005865cab1d496eb2323f9732bb4297",
            "e66618f9598442f7a8aa95c330b92abb",
            "e81316254fc640ddb73f43b58d155c49",
            "9ad5f6f227d84865ab33c8a57c53022e",
            "98c415837e464ace83dc9559a08b6c3f",
            "14cf1973ce5e42929197465d2d2d5b54",
            "ea38dc91ea4746148e158de8d5dba740",
            "c79ca9b9a1dc496fbdad101be28cf930",
            "c00878d5dbe84a79b564644b0d14bf2f",
            "f21ffd6fbccd45c4ac3bebadf26c9c1b",
            "8e424aa8f25c431b9960acd311375ffb",
            "2f545cc418264e2f9c89652e65556475",
            "efbcfb67a3704338b5a3c089ac62a8a1",
            "b500e1f989524b66a9824fae4da26945",
            "534e664e4e08457da9312b3842d49e3a",
            "7e3018247a4d4d1f89d3f63dbc966d4a",
            "497b9d0822ff4dc3b420aed716bdd0d8",
            "ace35eda31334712af73130343ec53b9",
            "a334ae97058c4ec098cc04601867889c",
            "8c6fceecb9924d5ba10a3c1165a8d219",
            "2b2e233303d5496f8c4ec6866538ec45",
            "0eb53e9b435a4e9e91ed1efbd41190f0",
            "2618e46223b74b728b63e3c6c1787372",
            "c77fd1cf89694874ab605d855ad95504",
            "3f56f6c2344a4c30b65b8488757d4b83",
            "7ae4ddc80db54481b560987c6bbd2484",
            "89d62af10d344502871879f422ca3700",
            "eb9f444f3041488da32ef6fa74c1c907",
            "58cb57bf54504b2abd20b4736863e185",
            "1d9ab1933a8c4ef3acf5b5c130d7f0c7",
            "e1069d1b71034c7eb55802980f177fd8",
            "1dbb2a9fa0ff4b7d813e698f9484e52a",
            "759c6e1287bf4d2d9182cc23492c0828",
            "3c604953ba5a44ef9c4be7e84f08e8a7",
            "3b30df9695854793b6f1bf88473d20ed",
            "63f23eae3c8446d98c03d88a8fb1af12",
            "acbcb06dfe884bab855a2d254a7cd2ac",
            "ea11adcda1c14ebb8b2c7a2c50ea857e",
            "d3ba7e28885e42f39edaa94c2f120ef8",
            "bb90dcd73cec457fbbeaf90d5818d2bb",
            "96370a3f0e5648ed937db6b23c53401c",
            "98693b494739426ab057ae8d2b815071"
          ]
        },
        "id": "646GnPkELMrE",
        "outputId": "a529c249-a86f-4266-c1eb-649491ea9ccd"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df, model_name='bert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bc89b464a0a42759ca999736cce5745",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "260ab876c4fc40f5b9bf4ce88d0a4a99",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f21ffd6fbccd45c4ac3bebadf26c9c1b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b2e233303d5496f8c4ec6866538ec45",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dbb2a9fa0ff4b7d813e698f9484e52a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fdd1b5b3ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fdd1b5b3ec0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fdd36768950> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7fdd36768950> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6684WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 98s 232ms/step - loss: 0.6684 - val_loss: 0.6412\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 47s 220ms/step - loss: 0.5835 - val_loss: 0.6704\n",
            "54/54 [==============================] - 4s 76ms/step - loss: 0.6412\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6733WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 62s 227ms/step - loss: 0.6733 - val_loss: 0.6672\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 46s 216ms/step - loss: 0.5845 - val_loss: 0.6518\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 46s 216ms/step - loss: 0.3999 - val_loss: 0.8603\n",
            "54/54 [==============================] - 4s 78ms/step - loss: 0.6518\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6801WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 65s 233ms/step - loss: 0.6801 - val_loss: 0.6608\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 47s 221ms/step - loss: 0.6121 - val_loss: 0.6297\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 47s 221ms/step - loss: 0.4523 - val_loss: 0.8120\n",
            "54/54 [==============================] - 4s 76ms/step - loss: 0.6297\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6763WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 64s 231ms/step - loss: 0.6763 - val_loss: 0.6688\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 47s 220ms/step - loss: 0.5940 - val_loss: 0.6888\n",
            "54/54 [==============================] - 4s 70ms/step - loss: 0.6688\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6730WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 64s 230ms/step - loss: 0.6730 - val_loss: 0.6359\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 47s 219ms/step - loss: 0.5800 - val_loss: 0.6484\n",
            "54/54 [==============================] - 4s 67ms/step - loss: 0.6359\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.645477831363678\n",
            "5-Fold CV Accuracy: 0.6251167200146792\n",
            "5-Fold CV Precision: 0.6491809066027606\n",
            "5-Fold CV Recall: 0.6215791088273641\n",
            "5-Fold CV F1 Score: 0.6214284571335817\n",
            "5-Fold CV Micro F1 Score: 0.6251167200146792\n",
            "5-Fold CV Weighted Macro F1 Score: 0.6166829298058032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nAynrbCsn8a"
      },
      "source": [
        "#### DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3ad291db492a4d1b8aa69a7208d1394f",
            "4312b80f2182456a94cfd9d36975564f",
            "a468e89e13934168a2718b4327c13db6",
            "c63e676cbbd8422ca7af91db59bc6659",
            "55a32112bdcd4ec9a1060ca4f7be57f5",
            "5f5d2da65c614d5091df023aacb279bd",
            "f18c736bd9134439b023e46dbfcbc8dc",
            "8edc65ca8c97465897bf545dee0a9246",
            "a5d1249c48ac46c88d9cd847f1d0413f",
            "99edb36129be4fa1b06876bf06420acb",
            "44f09c14f77146d28145427268002611",
            "2ecd2dd344494defb31a795ed18baaaf",
            "592194713aa044218f962a97cea167f1",
            "b7a5030d4aee4a37b307465683254992",
            "55885d99d16945c68c9df8849097cd2d",
            "f97736ea510f47209f63276570e24463",
            "ba4292efb4d049e088e606942154a728",
            "7a68d3a33e12460fa05003b9f7619f83",
            "6bc3fc3211014660b0e5e6025786c341",
            "f9bec7b473f44c92b6595a8e48996c6d",
            "14e74e2c72b9463da5e153e1981e9f49",
            "697767841a7044d0abc8943f16988349",
            "82595a1be51b47b2bc5dfcfdccff4e46",
            "d345115f7cc341faa93cfceb39e672a7",
            "274a103ba08e4602a21da3ae72d0c7e0",
            "5b39929f19244a8cad6c23eeb8b116d6",
            "818042d59c6b4f7cb12c7bf00229abcb",
            "085286364f0540a09c3ea34de9089d76",
            "a0fae6398dd94b8583bd25b74c24eb75",
            "5781b57987734d1e9012c4b9f641d968",
            "1e192cb1873044af9db3706bf0f920a6",
            "8f8915dd82504e218eddcf2fae80c55e",
            "b9794830910147e6a98e76b2c3eecd7f",
            "57bde018bd3e4383a7e06478a8af3b35",
            "80aa79a7eaa744f2bd6d775ca4593b1e",
            "4fd241c138784f379c77418683bb049b",
            "0fa1c49da19043c98d4e0fde5f7d45e1",
            "4a4311ab37f7460c88735416a4c6e4a1",
            "f30f705ecfdc4540812fc6cd7bb06f43",
            "e648a095a0634e5ea6569ebefe064b50",
            "645f895abc814602bcb33c7c9c0d149b",
            "01999dc18a3d4f2693bf9ddb46e96ff8",
            "29706cc4091c4d528d2033858910339c",
            "1f092d0088b44288bec29ff4eabffeec",
            "7865560c421a43b98e56c12e9970b151",
            "fac9ed17e7d1488dbc823535e05d46ab",
            "9ad7e89f941d432db99e43c02ea7acf2",
            "8a76912634fd403db6ca570566a39a4d",
            "fafdb0d5dbe4471b9a816b35531fd924",
            "e0b0d190a52a4d26a6e8f73e61046863",
            "0a2d0b7a896f499fb72dbffa4a530d5f",
            "d7c703b1d8094a30bb74a9518f48e474",
            "5f9e6a15c87a44e3bd6c97d6420b7107",
            "02e594755f7645418cfc60d571177e71",
            "2f034ccf421d4c42aeb1494781e43e8f"
          ]
        },
        "id": "phb3rT_oLMrF",
        "outputId": "4c22a754-04d2-4247-e7d0-d50573f7280f"
      },
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df, model_name='distilbert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ad291db492a4d1b8aa69a7208d1394f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ecd2dd344494defb31a795ed18baaaf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82595a1be51b47b2bc5dfcfdccff4e46",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57bde018bd3e4383a7e06478a8af3b35",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7865560c421a43b98e56c12e9970b151",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/363M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6616WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 35s 122ms/step - loss: 0.6616 - val_loss: 0.6348\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 25s 115ms/step - loss: 0.5658 - val_loss: 0.6789\n",
            "54/54 [==============================] - 2s 39ms/step - loss: 0.6348\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6534WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 32s 118ms/step - loss: 0.6534 - val_loss: 0.6542\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 24s 112ms/step - loss: 0.5588 - val_loss: 0.6659\n",
            "54/54 [==============================] - 2s 40ms/step - loss: 0.6542\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6667WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 33s 121ms/step - loss: 0.6667 - val_loss: 0.6484\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 25s 114ms/step - loss: 0.5738 - val_loss: 0.6365\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 25s 115ms/step - loss: 0.3727 - val_loss: 0.8782\n",
            "54/54 [==============================] - 2s 38ms/step - loss: 0.6365\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6636WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 33s 120ms/step - loss: 0.6636 - val_loss: 0.6460\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 24s 113ms/step - loss: 0.5658 - val_loss: 0.6628\n",
            "54/54 [==============================] - 2s 35ms/step - loss: 0.6460\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6648WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 33s 120ms/step - loss: 0.6648 - val_loss: 0.6262\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 24s 113ms/step - loss: 0.5588 - val_loss: 0.6271\n",
            "54/54 [==============================] - 2s 34ms/step - loss: 0.6262\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.6395406246185302\n",
            "5-Fold CV Accuracy: 0.6364266345898999\n",
            "5-Fold CV Precision: 0.6418816978212224\n",
            "5-Fold CV Recall: 0.6945632333767928\n",
            "5-Fold CV F1 Score: 0.6623625628989562\n",
            "5-Fold CV Micro F1 Score: 0.6364266345898999\n",
            "5-Fold CV Weighted Macro F1 Score: 0.6314755851399975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXN3opBJsn8c"
      },
      "source": [
        "#### RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "047085fb30f74f0dae77bc30e54f66ab",
            "5284a31f3247468b87927b046fad298e",
            "c731b8f0118142dbac11a77588906e60",
            "e99c20b9d98546ffa98c73c6cc6382d3",
            "59e37d56dff241e68ab0fa4642b0a285",
            "016f2dbf643b4c3d810b5ff11b71181b",
            "39fa781f4d6645c09369927a2e48967e",
            "b5e54f3e430c464c918e988a72ef1dd9",
            "6d04edde83c24ec7aaa2886942750f34",
            "fb6b6e3b1c4c4195b84d6ea42f718261",
            "1cb5c2119a954fc1bb4c892ebc97a11f",
            "8b4574d194e8407299b93048c8477e41",
            "0b1cca1ffdec4bd0940b8cb8574f6472",
            "9326be5c7c0043bdb91057064c5b13e5",
            "41c76385d40a48f2b971a985f6c97292",
            "df4524ce84d8427d91aa30b9d63aa109",
            "1e475390e58b4ffc8d8486fed42c3198",
            "a6a85cbcaed64f8eb90dbbaa28d52f0c",
            "b8d0b7e6f2fd406cb63e9c61ac2058a8",
            "593fa558a2264ac18ee91ce3de1eb90e",
            "6e4f3f7aa2a744908c358e0c6446bb29",
            "f6450a2e1e824ac387c0bcad34edca90",
            "a3ada041d0404875ae888a78def96270",
            "34babe09f7564360bf52a130a0eb8796",
            "dc13ed3875334ee7a291e083dc0e6e56",
            "e14730124df2449aad8565757ce9fa6b",
            "81a98778e70b49759155c2c68a296ece",
            "44fa5fd2c7f5408bb14b71925a9c0fec",
            "f8ae93a4e63a4e078837a226e7ad585b",
            "006a76227aec4057a3705393e7bfef23",
            "38e49c308792438e9d216c05c170d9c3",
            "d76ae1a3d43348aba518d739c5c30157",
            "8f6dfd0590c24232bfda21a1a7d9980d",
            "e8df76fb5f63473ebdd0228b46f5c735",
            "ef84f418d42040878415b7f5df3fc278",
            "35f81e05ac0b4dbbbb18055054ef7401",
            "5bdc17d3574b479d84f0ea9ce8c0d1f9",
            "3fd05eb8422546ef8923b16e63e6edfb",
            "6f326629c81f4fc2a77d5a83336547e8",
            "f58bbf25779a40788577c8907bcf9adc",
            "897a87741d01423ab9e2d1494ee4e8b0",
            "b30de39e82e14b6aa334d68076b53d82",
            "9c2fe6335bdd4ed698bf201ffcbe33b1",
            "2bd8c97d2eb94333a86aaebb7cf2cc62",
            "7b6164c23a594fb4b39f693d6b67144a",
            "df5deae47e8649d0ad7b232e448e8a5c",
            "6331b2a99377403298d9b6a4a4edd09f",
            "c24532f120b64d61a4552bb0abc52c22",
            "fafb2632ebf44651b4981a2075258cbc",
            "2b7adb1b5b734c21aa3cd13c1241bf7d",
            "8bbf13678eea4fb9b4f7784764bbbbcb",
            "2d15ea9cdf164672bf30bfdd6d35bec9",
            "8df8082e3708414eb3e187da572b2c70",
            "6261c905d668418cb56b60410247896c",
            "18a66a5342e74cb2ad6a84f7d6c8f4ba"
          ]
        },
        "id": "cIf4nbYif6Kh",
        "outputId": "203d3350-ba5d-4f01-ee33-6b40ebf99146"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df, model_name='roberta', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "047085fb30f74f0dae77bc30e54f66ab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b4574d194e8407299b93048c8477e41",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3ada041d0404875ae888a78def96270",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8df76fb5f63473ebdd0228b46f5c735",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b6164c23a594fb4b39f693d6b67144a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/657M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6670WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 77s 283ms/step - loss: 0.6670 - val_loss: 0.6415\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 58s 271ms/step - loss: 0.6242 - val_loss: 0.6573\n",
            "54/54 [==============================] - 4s 79ms/step - loss: 0.6415\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6646WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 74s 280ms/step - loss: 0.6646 - val_loss: 0.6417\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 58s 270ms/step - loss: 0.6136 - val_loss: 0.6381\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 58s 269ms/step - loss: 0.5492 - val_loss: 0.6449\n",
            "54/54 [==============================] - 4s 71ms/step - loss: 0.6381\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6609WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 67s 248ms/step - loss: 0.6609 - val_loss: 0.6161\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 51s 236ms/step - loss: 0.6061 - val_loss: 0.6489\n",
            "54/54 [==============================] - 5s 92ms/step - loss: 0.6161\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6627WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 75s 281ms/step - loss: 0.6627 - val_loss: 0.6334\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 58s 270ms/step - loss: 0.6150 - val_loss: 0.6005\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 58s 270ms/step - loss: 0.5572 - val_loss: 0.6320\n",
            "54/54 [==============================] - 4s 73ms/step - loss: 0.6005\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6689WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 75s 282ms/step - loss: 0.6689 - val_loss: 0.6244\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 58s 271ms/step - loss: 0.6079 - val_loss: 0.5871\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 58s 271ms/step - loss: 0.5150 - val_loss: 0.7217\n",
            "54/54 [==============================] - 4s 77ms/step - loss: 0.5871\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.6166400074958801\n",
            "5-Fold CV Accuracy: 0.6646483448524265\n",
            "5-Fold CV Precision: 0.6598610045496089\n",
            "5-Fold CV Recall: 0.7318710535061482\n",
            "5-Fold CV F1 Score: 0.6919540267604727\n",
            "5-Fold CV Micro F1 Score: 0.6646483448524265\n",
            "5-Fold CV Weighted Macro F1 Score: 0.661505333997616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiDklsr3sn8e"
      },
      "source": [
        "#### Electra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "83cae10301884d0db9ef998fb680b8c5",
            "4f0d51e5f22f451e9d368e645b82c91d",
            "fe7e8e62ce7840dfa35d2c6282131a01",
            "4858d767d45048f1a09004d54fc3783e",
            "396033aca22d494981b3114f69592e9f",
            "33b7b7162cc94071818d0552949131d7",
            "45148355ce174a1d8303a22a672ee995",
            "1fdf1ff6836448448ddea70ea242e3ec",
            "c641ab1e85f048bdae17e4445c481f70",
            "4e97f304624940b6b59b2c7aa1f10123",
            "fd849249e74348fcba3d2396811c0df3",
            "85ecefb004044903acec77ea9fcbe562",
            "2a7ab256553e41f1a86f3d51e3dc421f",
            "18d0215066ec450da2851df40dd00f00",
            "0eb73ddd04b848f5aa86a3d1883870d7",
            "0d3ee564c82144acada2397a15e0765c",
            "bb3c7d3b5a0a46eb877e560e83eeffb3",
            "4b13e4b5671e42679ac57bc230a101b5",
            "c259509efc1e4484b04e87c8880f4bd8",
            "a0a3ce617ec34fe39e76d2aea4c70c8d",
            "b23d8d8685844441a190adba05c25282",
            "767c2044d20041e5b65ac6bd5ea3fcd7",
            "cb23a70eaccf41989d3e94b2936a48f3",
            "2a89d3c97a9c41099b7e0176c9bd80d2",
            "aac84d5a09b54fa487c0be34209162c2",
            "335a780478424cc7ac96938c9ccaad4b",
            "cda670f8490a49c1ae59d922258a8ec1",
            "787e6dd55a11426c9a7faa7c33da1993",
            "1969313b0d244c62a0549fd2e446b59b",
            "50ae3b4e1a8e4f8db096153326c56065",
            "84293a7365ef435e9bb18156afc25a6d",
            "b2acc80f35374b1cb259fabaa018efac",
            "7c15616a49b84e9cad517916fcec16b4",
            "0a45fbb288134488879b641114ed0bff",
            "ef1b7b8f807c46b8a4706608c4e2a213",
            "a4f1c3b50850406aa07bc0a011b1c3af",
            "38e2107faba048ac92bb856ef03dccb8",
            "2b64310279ac467db553e3bc0b201442",
            "4d52e8addb0a4713815d58a80e220d10",
            "c2976880e4f54a8dba19778e485138bc",
            "98be053bcfeb49378a78be32b9e5b793",
            "1369281495784322a5cfd8dc7248e6be",
            "6f7a661522d94a4bbccf3ceca04176d6",
            "09daa6ed88bb41fb9b66665ca7953790",
            "706db006b0734feebcfec2adcbe41128",
            "33db4f70411a4e9289668203d42ad167",
            "c7d61b0ad9104502b7ceca452cfe92a6",
            "c313b7fda794418d85c78e96243e51f9",
            "1cf966ecc2464682b0beb1204720d0ad",
            "4355dfc619254294926bd4c02f800923",
            "725c6fe8b52f4830937e36c5452a64fe",
            "6b481346deda4432897b6bb96f72066a",
            "11c404741e75477cbc68f68a0c1e798f",
            "5280e616d09a49d89ed8da16c70b5b24",
            "6f40d122bc0e4dbfa8cccd2d0c7ded4b"
          ]
        },
        "id": "oUIHX7_xlMy6",
        "outputId": "cf2bcc3d-98fb-4483-a36c-84480f81b936"
      },
      "source": [
        "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df, model_name='electra', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83cae10301884d0db9ef998fb680b8c5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85ecefb004044903acec77ea9fcbe562",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb23a70eaccf41989d3e94b2936a48f3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a45fbb288134488879b641114ed0bff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "706db006b0734feebcfec2adcbe41128",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/54.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6711WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 34s 79ms/step - loss: 0.6711 - val_loss: 0.6491\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 15s 68ms/step - loss: 0.6200 - val_loss: 0.6444\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 15s 69ms/step - loss: 0.5456 - val_loss: 0.6713\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6444\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6739WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 31s 81ms/step - loss: 0.6737 - val_loss: 0.6624\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 14s 67ms/step - loss: 0.6240 - val_loss: 0.6566\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 15s 68ms/step - loss: 0.5631 - val_loss: 0.7003\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.6566\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6727WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 32s 81ms/step - loss: 0.6729 - val_loss: 0.6438\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 15s 68ms/step - loss: 0.6207 - val_loss: 0.6340\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 15s 68ms/step - loss: 0.5531 - val_loss: 0.6779\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6340\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6707WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 31s 79ms/step - loss: 0.6706 - val_loss: 0.6590\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 15s 68ms/step - loss: 0.6262 - val_loss: 0.6444\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 15s 68ms/step - loss: 0.5710 - val_loss: 0.6505\n",
            "54/54 [==============================] - 1s 28ms/step - loss: 0.6444\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6732WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "215/215 [==============================] - 32s 79ms/step - loss: 0.6734 - val_loss: 0.6566\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 15s 68ms/step - loss: 0.6229 - val_loss: 0.6470\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 15s 68ms/step - loss: 0.5544 - val_loss: 0.6648\n",
            "54/54 [==============================] - 1s 27ms/step - loss: 0.6470\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.6452997326850891\n",
            "5-Fold CV Accuracy: 0.6349113471562451\n",
            "5-Fold CV Precision: 0.6342297316336576\n",
            "5-Fold CV Recall: 0.7012971342383107\n",
            "5-Fold CV F1 Score: 0.6621588943869681\n",
            "5-Fold CV Micro F1 Score: 0.6349113471562451\n",
            "5-Fold CV Weighted Macro F1 Score: 0.6300258000409714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjXihwd8sn8f"
      },
      "source": [
        "#### XLNET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "18fb4bc6dd48459dbb3ab95586b3876c",
            "7c619a2e87ec47e2938baf7782feb785",
            "895be44dfa604b9483ee95cedc1b649a",
            "5671b45347384167bb07d1abadfdb0ce",
            "c5b45b35855d456db62c6e50a5c924f5",
            "f8055d772eca49cca9d4a9d590d9a366",
            "6d284bc664b446c584c44ed2f9dd519a",
            "6ce40f2ce4e5442fbcfc641e44ebf67a",
            "93aae4bafa6446b2ac3b24114aeac89b",
            "7954b30193884e6fa093140ef9d57359",
            "a3affd358fba4d61b07f7148befc6f6e",
            "e0a9cd21c63d4717b2dd06f7e6bb154b",
            "0f290214dc1144cd8acc84360342a33e",
            "8ecaaee415ae4f6f92724fd77087308e",
            "2573b86f82f545f88c016aefa01500e4",
            "10344061cb604a3da9b7ac93a88c491e",
            "bf92ff70ac4e47d48f00c8d4a4e4acf9",
            "fc9e538bfe4f4172a86f54d42f341e7c",
            "b6095f9287d84b099edd7c5ff8205585",
            "e82054308b8245959a2d820a22fc5214",
            "4be78969bd0547bc81c792ed7e30389d",
            "f2c6937fe0e54e95996d65b0697e4708",
            "c0323cab25574abab5a3e3d2e0fe6ac0",
            "27884f2787974c44b8d3d66ac066dec5",
            "622f1b4133c24a0d98d74e23c8e9b499",
            "df0c11ad643047db9c56026bdc2d516c",
            "e5d213d611e549909bf5dfb0d091b2c3",
            "1dd9f0eb48a34b42972b1a26cf603df0",
            "0962ecdb5611408a8d6b5f930f56d688",
            "aa07eace19504d14b331086f229634e0",
            "05964f643aae4750aeb0137f3a383273",
            "60da7387ca954436942c7cfd23093d82",
            "192c5f7c59994803872cbfe54723fec8",
            "2b8244ab9cb7479f8fe0718d1b40bf7a",
            "af58a7ed94c54a0fabd0ae2bf6139745",
            "3929ac3d82144c1f9ce765b333209080",
            "fddee7d4ebe54b9aaa04b9ad235a0078",
            "f156aae8a7f1456b8c14f1f8d8634f30",
            "8a37b006e74f476ea4d27d5cdddee73b",
            "1f15e9b95cd646c8b56ce09610c58604",
            "85059a7cdcee46178b00f580aa5bbb95",
            "7ae5517f2a0e4b2d98bfca1093c06aa0",
            "b3d01e7bb644483192ad62daa1f36dbe",
            "5847e3ce41034c2c896910ecf4cf1b48"
          ]
        },
        "id": "Beja8P2flSo5",
        "outputId": "097d4f3a-fdbc-4b66-e5c1-1ef8c896d1ef"
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df, model_name='xlnet', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18fb4bc6dd48459dbb3ab95586b3876c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0a9cd21c63d4717b2dd06f7e6bb154b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0323cab25574abab5a3e3d2e0fe6ac0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b8244ab9cb7479f8fe0718d1b40bf7a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/565M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6859WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "215/215 [==============================] - 82s 299ms/step - loss: 0.6859 - val_loss: 0.6472\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 62s 289ms/step - loss: 0.6430 - val_loss: 0.6434\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 62s 289ms/step - loss: 0.5657 - val_loss: 0.6689\n",
            "54/54 [==============================] - 5s 93ms/step - loss: 0.6434\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6804WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "215/215 [==============================] - 77s 298ms/step - loss: 0.6804 - val_loss: 0.6554\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 62s 288ms/step - loss: 0.6613 - val_loss: 0.6730\n",
            "54/54 [==============================] - 5s 91ms/step - loss: 0.6554\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6713WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "215/215 [==============================] - 77s 296ms/step - loss: 0.6713 - val_loss: 0.6675\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 61s 286ms/step - loss: 0.6264 - val_loss: 0.6208\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 61s 286ms/step - loss: 0.5512 - val_loss: 0.6611\n",
            "54/54 [==============================] - 4s 81ms/step - loss: 0.6208\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.7016WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "215/215 [==============================] - 78s 299ms/step - loss: 0.7016 - val_loss: 0.6945\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 62s 287ms/step - loss: 0.6958 - val_loss: 0.6712\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 62s 287ms/step - loss: 0.6627 - val_loss: 0.6477\n",
            "Epoch 4/10\n",
            "215/215 [==============================] - 62s 287ms/step - loss: 0.6161 - val_loss: 0.6201\n",
            "Epoch 5/10\n",
            "215/215 [==============================] - 62s 287ms/step - loss: 0.5370 - val_loss: 0.7013\n",
            "54/54 [==============================] - 5s 93ms/step - loss: 0.6201\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6781WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "215/215 [==============================] - 78s 299ms/step - loss: 0.6781 - val_loss: 0.6460\n",
            "Epoch 2/10\n",
            "215/215 [==============================] - 62s 288ms/step - loss: 0.6245 - val_loss: 0.6011\n",
            "Epoch 3/10\n",
            "215/215 [==============================] - 62s 288ms/step - loss: 0.5479 - val_loss: 0.6057\n",
            "54/54 [==============================] - 5s 93ms/step - loss: 0.6011\n",
            "5-Fold CV Loss: 0.6281654596328735\n",
            "5-Fold CV Accuracy: 0.6584648005056167\n",
            "5-Fold CV Precision: 0.6593260262719272\n",
            "5-Fold CV Recall: 0.7232983101976123\n",
            "5-Fold CV F1 Score: 0.683841833499995\n",
            "5-Fold CV Micro F1 Score: 0.6584648005056167\n",
            "5-Fold CV Weighted Macro F1 Score: 0.6529466145517556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac3R3mA7SsB5"
      },
      "source": [
        "### LDA Topic Modelling for Dataset Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGUH8R6f_1tx"
      },
      "source": [
        ""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "UCJytUY05bUA",
        "outputId": "1695cd7a-5b85-42c7-b67e-616cfa227040"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        " \n",
        " \n",
        "stop_words = set(stopwords.words('english'))\n",
        " \n",
        "df[\"word_tokens\"] = df.Headline.apply(word_tokenize)\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "filtered_list = []\n",
        " \n",
        "for i in df[\"word_tokens\"]:\n",
        "  filtered_sentence = []\n",
        "  for w in i:\n",
        "      if w not in stop_words:\n",
        "          w = lemmatizer.lemmatize(w)\n",
        "          filtered_sentence.append(w)\n",
        "  filtered_list.append(filtered_sentence)\n",
        "df[\"lda_ready\"] = filtered_list\n",
        "df"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Source</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Text</th>\n",
              "      <th>Bias</th>\n",
              "      <th>Subject_Tag</th>\n",
              "      <th>Date</th>\n",
              "      <th>Label</th>\n",
              "      <th>word_tokens</th>\n",
              "      <th>lda_ready</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Wall Street Journal (News)</td>\n",
              "      <td>Vaccines Appear to Be Slowing Spread of Covid-...</td>\n",
              "      <td>Vaccines appear to be starting to curb new Cov...</td>\n",
              "      <td>AllSides Media Bias Rating: Center</td>\n",
              "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
              "      <td>May 3rd, 2021</td>\n",
              "      <td>0</td>\n",
              "      <td>[Vaccines, Appear, to, Be, Slowing, Spread, of...</td>\n",
              "      <td>[Vaccines, Appear, Be, Slowing, Spread, Covid-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>The Hill</td>\n",
              "      <td>New York Times, WaPo, NBC retract reports abou...</td>\n",
              "      <td>The New York Times, The Washington Post and NB...</td>\n",
              "      <td>AllSides Media Bias Rating: Center</td>\n",
              "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
              "      <td>May 2nd, 2021</td>\n",
              "      <td>0</td>\n",
              "      <td>[New, York, Times, ,, WaPo, ,, NBC, retract, r...</td>\n",
              "      <td>[New, York, Times, ,, WaPo, ,, NBC, retract, r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>CNN (Online News)</td>\n",
              "      <td>Washington Post, New York Times, and NBC News ...</td>\n",
              "      <td>The Washington Post, The New York Times, and N...</td>\n",
              "      <td>AllSides Media Bias Rating: Left</td>\n",
              "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
              "      <td>May 2nd, 2021</td>\n",
              "      <td>1</td>\n",
              "      <td>[Washington, Post, ,, New, York, Times, ,, and...</td>\n",
              "      <td>[Washington, Post, ,, New, York, Times, ,, NBC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>The Hill</td>\n",
              "      <td>Biden cancels military-funded border wall proj...</td>\n",
              "      <td>President Biden is canceling projects to build...</td>\n",
              "      <td>AllSides Media Bias Rating: Center</td>\n",
              "      <td>Immigration, Border Wall, Pentagon, US Militar...</td>\n",
              "      <td>May 1st, 2021</td>\n",
              "      <td>0</td>\n",
              "      <td>[Biden, cancels, military-funded, border, wall...</td>\n",
              "      <td>[Biden, cancel, military-funded, border, wall,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>Ezra Klein</td>\n",
              "      <td>100 Days of Big, Bold, Partisan Change</td>\n",
              "      <td>OPINION\\r\\nI intended to duly fulfill my duty ...</td>\n",
              "      <td>AllSides Media Bias Rating: Left</td>\n",
              "      <td>Bridging Divides, Joe Biden, Polarization, Whi...</td>\n",
              "      <td>April 30th, 2021</td>\n",
              "      <td>1</td>\n",
              "      <td>[100, Days, of, Big, ,, Bold, ,, Partisan, Cha...</td>\n",
              "      <td>[100, Days, Big, ,, Bold, ,, Partisan, Change]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8571</th>\n",
              "      <td>16575</td>\n",
              "      <td>Townhall</td>\n",
              "      <td>CNN Takes Almost 3 Hours to Report Family Rese...</td>\n",
              "      <td>Mary Katharine Ham provides the details:\\r\\nCo...</td>\n",
              "      <td>AllSides Media Bias Rating: Right</td>\n",
              "      <td>Violence In America</td>\n",
              "      <td>August 16th, 2012</td>\n",
              "      <td>1</td>\n",
              "      <td>[CNN, Takes, Almost, 3, Hours, to, Report, Fam...</td>\n",
              "      <td>[CNN, Takes, Almost, 3, Hours, Report, Family,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8572</th>\n",
              "      <td>16576</td>\n",
              "      <td>CNN (Online News)</td>\n",
              "      <td>DC shooting suspect: \"I don't like your politi...</td>\n",
              "      <td>The suspect in the shooting at the Family Rese...</td>\n",
              "      <td>AllSides Media Bias Rating: Left</td>\n",
              "      <td>Violence In America</td>\n",
              "      <td>August 16th, 2012</td>\n",
              "      <td>1</td>\n",
              "      <td>[DC, shooting, suspect, :, ``, I, do, n't, lik...</td>\n",
              "      <td>[DC, shooting, suspect, :, ``, I, n't, like, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8573</th>\n",
              "      <td>16578</td>\n",
              "      <td>CNN (Online News)</td>\n",
              "      <td>Program providing protection for young immigra...</td>\n",
              "      <td>(CNN) -- Hundreds of thousands of people who e...</td>\n",
              "      <td>AllSides Media Bias Rating: Left</td>\n",
              "      <td>Presidential Elections, Elections</td>\n",
              "      <td>August 15th, 2012</td>\n",
              "      <td>1</td>\n",
              "      <td>[Program, providing, protection, for, young, i...</td>\n",
              "      <td>[Program, providing, protection, young, immigr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8574</th>\n",
              "      <td>16582</td>\n",
              "      <td>CNN (Online News)</td>\n",
              "      <td>Romney campaign tries to take Medicare away fr...</td>\n",
              "      <td>Washington (CNN) -- The fight to define Paul R...</td>\n",
              "      <td>AllSides Media Bias Rating: Left</td>\n",
              "      <td>Healthcare</td>\n",
              "      <td>August 15th, 2012</td>\n",
              "      <td>1</td>\n",
              "      <td>[Romney, campaign, tries, to, take, Medicare, ...</td>\n",
              "      <td>[Romney, campaign, try, take, Medicare, away, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8575</th>\n",
              "      <td>16583</td>\n",
              "      <td>Townhall</td>\n",
              "      <td>Smoking Gun: Obama Admits He Cut Billions from...</td>\n",
              "      <td>We've been having some fun with a clip of Davi...</td>\n",
              "      <td>AllSides Media Bias Rating: Right</td>\n",
              "      <td>Healthcare</td>\n",
              "      <td>August 15th, 2012</td>\n",
              "      <td>1</td>\n",
              "      <td>[Smoking, Gun, :, Obama, Admits, He, Cut, Bill...</td>\n",
              "      <td>[Smoking, Gun, :, Obama, Admits, He, Cut, Bill...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8576 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  ...                                          lda_ready\n",
              "0         1  ...  [Vaccines, Appear, Be, Slowing, Spread, Covid-...\n",
              "1         4  ...  [New, York, Times, ,, WaPo, ,, NBC, retract, r...\n",
              "2         5  ...  [Washington, Post, ,, New, York, Times, ,, NBC...\n",
              "3         8  ...  [Biden, cancel, military-funded, border, wall,...\n",
              "4        10  ...     [100, Days, Big, ,, Bold, ,, Partisan, Change]\n",
              "...     ...  ...                                                ...\n",
              "8571  16575  ...  [CNN, Takes, Almost, 3, Hours, Report, Family,...\n",
              "8572  16576  ...  [DC, shooting, suspect, :, ``, I, n't, like, p...\n",
              "8573  16578  ...  [Program, providing, protection, young, immigr...\n",
              "8574  16582  ...  [Romney, campaign, try, take, Medicare, away, ...\n",
              "8575  16583  ...  [Smoking, Gun, :, Obama, Admits, He, Cut, Bill...\n",
              "\n",
              "[8576 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl2aQg-XTFcW",
        "outputId": "e5ced201-6bde-430e-df97-0b3d431279d0"
      },
      "source": [
        "doc_sample = df[[\"Headline\"]].iloc[202].values[0]\n",
        "print('original document: ')\n",
        "words = []\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "print(words)\n",
        "print('\\n\\n tokenized and lemmatized document: ')\n",
        "print(df[[\"lda_ready\"]].iloc[202].values[0])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original document: \n",
            "['AstraZeneca:', 'US', 'data', 'shows', 'vaccine', 'effective', 'for', 'all', 'adults']\n",
            "\n",
            "\n",
            " tokenized and lemmatized document: \n",
            "['AstraZeneca', ':', 'US', 'data', 'show', 'vaccine', 'effective', 'adult']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGQSb9wKTFiU",
        "outputId": "e9783eb7-4741-414a-e1d5-3f79d54389da"
      },
      "source": [
        "processed_docs = df[\"lda_ready\"]\n",
        "processed_docs[:10]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [Vaccines, Appear, Be, Slowing, Spread, Covid-...\n",
              "1    [New, York, Times, ,, WaPo, ,, NBC, retract, r...\n",
              "2    [Washington, Post, ,, New, York, Times, ,, NBC...\n",
              "3    [Biden, cancel, military-funded, border, wall,...\n",
              "4       [100, Days, Big, ,, Bold, ,, Partisan, Change]\n",
              "5                              [Bidens, Con, America]\n",
              "6    [The, global, chip, shortage, going, bad, wors...\n",
              "7             [Why, care, semiconductor, chip, crisis]\n",
              "8    [Mark, Kelly, :, I, n't, hear, plan, border, B...\n",
              "9    [Biden, 's, 1st, 100, Days, :, A, Look, By, Th...\n",
              "Name: lda_ready, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rroz_sj9AL1_"
      },
      "source": [
        ""
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugjvAk4jzxv2"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0owbY41LTFnC",
        "outputId": "43825bc1-fa49-4634-bcba-28d9e4303c49"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Appear\n",
            "1 Be\n",
            "2 Covid-19\n",
            "3 Infections\n",
            "4 Slowing\n",
            "5 Spread\n",
            "6 Vaccines\n",
            "7 's\n",
            "8 ,\n",
            "9 FBI\n",
            "10 Giuliani\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F7lT-akTWil"
      },
      "source": [
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6Jib4qGTWvS",
        "outputId": "a0049cf4-d9cb-444d-f108-8e68a147d61d"
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "bow_corpus[4310]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(113, 1), (196, 1), (323, 1), (385, 1), (788, 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc2ojX75TWyG",
        "outputId": "f48abcda-04c6-47c9-dde3-a36050ac8624"
      },
      "source": [
        "bow_doc_4310 = bow_corpus[4310]\n",
        "for i in range(len(bow_doc_4310)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
        "                                               dictionary[bow_doc_4310[i][0]], \n",
        "bow_doc_4310[i][1]))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 113 (\"Trump\") appears 1 time.\n",
            "Word 196 (\"President\") appears 1 time.\n",
            "Word 323 (\"fight\") appears 1 time.\n",
            "Word 385 (\"election\") appears 1 time.\n",
            "Word 788 (\"abortion\") appears 1 time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek0OzCLsTW00"
      },
      "source": [
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyHuQRjwTW4w",
        "outputId": "67e3d89b-54cc-494d-9c03-610899567c69"
      },
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.080*\"'s\" + 0.040*\"Obama\" + 0.040*\":\" + 0.025*\"House\" + 0.020*\"?\" + 0.015*\"Romney\" + 0.014*\"'\" + 0.013*\"White\" + 0.013*\"Trump\" + 0.013*\"GOP\"\n",
            "Topic: 1 \n",
            "Words: 0.091*\":\" + 0.040*\"'s\" + 0.026*\",\" + 0.025*\"Trump\" + 0.024*\"'\" + 0.018*\"Obama\" + 0.016*\"Romney\" + 0.016*\"In\" + 0.016*\"House\" + 0.013*\"Court\"\n",
            "Topic: 2 \n",
            "Words: 0.064*\"'\" + 0.038*\":\" + 0.027*\"Trump\" + 0.023*\"Obama\" + 0.023*\"To\" + 0.018*\"say\" + 0.016*\",\" + 0.013*\"campaign\" + 0.013*\"Republicans\" + 0.012*\"'s\"\n",
            "Topic: 3 \n",
            "Words: 0.026*\":\" + 0.026*\"Obama\" + 0.024*\"Korea\" + 0.024*\"?\" + 0.024*\"To\" + 0.023*\",\" + 0.022*\"'\" + 0.021*\"North\" + 0.019*\"'s\" + 0.019*\"GOP\"\n",
            "Topic: 4 \n",
            "Words: 0.063*\":\" + 0.043*\",\" + 0.030*\"The\" + 0.029*\"'\" + 0.021*\"Obama\" + 0.020*\"On\" + 0.017*\"To\" + 0.016*\"Senate\" + 0.014*\"?\" + 0.013*\"Is\"\n",
            "Topic: 5 \n",
            "Words: 0.057*\",\" + 0.039*\":\" + 0.038*\"Trump\" + 0.036*\"Romney\" + 0.031*\"Obama\" + 0.028*\"'s\" + 0.020*\"For\" + 0.017*\"The\" + 0.016*\"To\" + 0.012*\"Of\"\n",
            "Topic: 6 \n",
            "Words: 0.064*\"Obama\" + 0.059*\",\" + 0.023*\"'\" + 0.023*\"House\" + 0.022*\"In\" + 0.021*\":\" + 0.021*\"Debate\" + 0.021*\"Romney\" + 0.019*\"U.S.\" + 0.015*\"On\"\n",
            "Topic: 7 \n",
            "Words: 0.109*\",\" + 0.041*\"Trump\" + 0.017*\"'\" + 0.016*\":\" + 0.016*\"GOP\" + 0.015*\"cliff\" + 0.013*\"Obama\" + 0.013*\"'s\" + 0.012*\"fiscal\" + 0.012*\"Senate\"\n",
            "Topic: 8 \n",
            "Words: 0.052*\":\" + 0.046*\",\" + 0.021*\"'\" + 0.019*\"GOP\" + 0.018*\"In\" + 0.015*\"'s\" + 0.013*\"To\" + 0.012*\"Obama\" + 0.011*\"Trump\" + 0.011*\"A\"\n",
            "Topic: 9 \n",
            "Words: 0.033*\",\" + 0.032*\"Trump\" + 0.027*\"'s\" + 0.025*\"Obama\" + 0.021*\"Senate\" + 0.019*\":\" + 0.017*\".\" + 0.013*\"'\" + 0.012*\"U.S.\" + 0.012*\"Congress\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "CuhnGEsiTW7B",
        "outputId": "b238ed4b-d3ef-4abf-ab6c-962796e56f8f"
      },
      "source": [
        "babe.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>news_link</th>\n",
              "      <th>outlet</th>\n",
              "      <th>topic</th>\n",
              "      <th>type</th>\n",
              "      <th>label_bias</th>\n",
              "      <th>label_opinion</th>\n",
              "      <th>biased_words</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
              "      <td>https://www.foxnews.com/entertainment/australi...</td>\n",
              "      <td>Fox News</td>\n",
              "      <td>environment</td>\n",
              "      <td>right</td>\n",
              "      <td>Non-biased</td>\n",
              "      <td>Entirely factual</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
              "      <td>https://www.alternet.org/2020/06/law-and-order...</td>\n",
              "      <td>Alternet</td>\n",
              "      <td>gun control</td>\n",
              "      <td>left</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Somewhat factual but also opinionated</td>\n",
              "      <td>['bizarre', 'characteristically']</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
              "      <td>https://www.nbcnews.com/news/latino/after-step...</td>\n",
              "      <td>MSNBC</td>\n",
              "      <td>white-nationalism</td>\n",
              "      <td>left</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Expresses writer’s opinion</td>\n",
              "      <td>['criminals', 'fringe', 'extreme']</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>...we sounded the alarm in the early months of...</td>\n",
              "      <td>https://www.alternet.org/2019/07/fox-news-has-...</td>\n",
              "      <td>Alternet</td>\n",
              "      <td>white-nationalism</td>\n",
              "      <td>left</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Somewhat factual but also opinionated</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Black Lives Matter] is essentially a non-fals...</td>\n",
              "      <td>http://feedproxy.google.com/~r/breitbart/~3/-v...</td>\n",
              "      <td>Breitbart</td>\n",
              "      <td>marriage-equality</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Expresses writer’s opinion</td>\n",
              "      <td>['cult']</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ... Label\n",
              "0  \"Orange Is the New Black\" star Yael Stone is r...  ...     0\n",
              "1  \"We have one beautiful law,\" Trump recently sa...  ...     1\n",
              "2  ...immigrants as criminals and eugenics, all o...  ...     1\n",
              "3  ...we sounded the alarm in the early months of...  ...     1\n",
              "4  [Black Lives Matter] is essentially a non-fals...  ...     1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "G1GB8RIwTW5C",
        "outputId": "0ac30afc-49ea-4862-936b-407b5e57c554"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        " \n",
        "babe[\"word_tokens\"] = babe.text.apply(word_tokenize)\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "filtered_list = []\n",
        " \n",
        "for i in babe[\"word_tokens\"]:\n",
        "  filtered_sentence = []\n",
        "  for w in i:\n",
        "      if w not in stop_words:\n",
        "          w = lemmatizer.lemmatize(w)\n",
        "          filtered_sentence.append(w)\n",
        "  filtered_list.append(filtered_sentence)\n",
        "babe[\"lda_ready\"] = filtered_list\n",
        "babe"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>news_link</th>\n",
              "      <th>outlet</th>\n",
              "      <th>topic</th>\n",
              "      <th>type</th>\n",
              "      <th>label_bias</th>\n",
              "      <th>label_opinion</th>\n",
              "      <th>biased_words</th>\n",
              "      <th>Label</th>\n",
              "      <th>word_tokens</th>\n",
              "      <th>lda_ready</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
              "      <td>https://www.foxnews.com/entertainment/australi...</td>\n",
              "      <td>Fox News</td>\n",
              "      <td>environment</td>\n",
              "      <td>right</td>\n",
              "      <td>Non-biased</td>\n",
              "      <td>Entirely factual</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>[``, Orange, Is, the, New, Black, '', star, Ya...</td>\n",
              "      <td>[``, Orange, Is, New, Black, '', star, Yael, S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
              "      <td>https://www.alternet.org/2020/06/law-and-order...</td>\n",
              "      <td>Alternet</td>\n",
              "      <td>gun control</td>\n",
              "      <td>left</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Somewhat factual but also opinionated</td>\n",
              "      <td>['bizarre', 'characteristically']</td>\n",
              "      <td>1</td>\n",
              "      <td>[``, We, have, one, beautiful, law, ,, '', Tru...</td>\n",
              "      <td>[``, We, one, beautiful, law, ,, '', Trump, re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
              "      <td>https://www.nbcnews.com/news/latino/after-step...</td>\n",
              "      <td>MSNBC</td>\n",
              "      <td>white-nationalism</td>\n",
              "      <td>left</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Expresses writer’s opinion</td>\n",
              "      <td>['criminals', 'fringe', 'extreme']</td>\n",
              "      <td>1</td>\n",
              "      <td>[..., immigrants, as, criminals, and, eugenics...</td>\n",
              "      <td>[..., immigrant, criminal, eugenics, ,, consid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>...we sounded the alarm in the early months of...</td>\n",
              "      <td>https://www.alternet.org/2019/07/fox-news-has-...</td>\n",
              "      <td>Alternet</td>\n",
              "      <td>white-nationalism</td>\n",
              "      <td>left</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Somewhat factual but also opinionated</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "      <td>[..., we, sounded, the, alarm, in, the, early,...</td>\n",
              "      <td>[..., sounded, alarm, early, month, Trump, ’, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Black Lives Matter] is essentially a non-fals...</td>\n",
              "      <td>http://feedproxy.google.com/~r/breitbart/~3/-v...</td>\n",
              "      <td>Breitbart</td>\n",
              "      <td>marriage-equality</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Expresses writer’s opinion</td>\n",
              "      <td>['cult']</td>\n",
              "      <td>1</td>\n",
              "      <td>[[, Black, Lives, Matter, ], is, essentially, ...</td>\n",
              "      <td>[[, Black, Lives, Matter, ], essentially, non-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3669</th>\n",
              "      <td>You’ve heard of Jim Crow and Southern Segregat...</td>\n",
              "      <td>http://feedproxy.google.com/~r/breitbart/~3/ei...</td>\n",
              "      <td>Breitbart</td>\n",
              "      <td>marriage-equality</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Expresses writer’s opinion</td>\n",
              "      <td>['ALL']</td>\n",
              "      <td>1</td>\n",
              "      <td>[You, ’, ve, heard, of, Jim, Crow, and, Southe...</td>\n",
              "      <td>[You, ’, heard, Jim, Crow, Southern, Segregati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3670</th>\n",
              "      <td>Young female athletes’ dreams and accomplishme...</td>\n",
              "      <td>http://feedproxy.google.com/~r/breitbart/~3/eW...</td>\n",
              "      <td>Breitbart</td>\n",
              "      <td>marriage-equality</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Expresses writer’s opinion</td>\n",
              "      <td>['dashed', '\"identify\"']</td>\n",
              "      <td>1</td>\n",
              "      <td>[Young, female, athletes, ’, dreams, and, acco...</td>\n",
              "      <td>[Young, female, athlete, ’, dream, accomplishm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3671</th>\n",
              "      <td>Young white men, reacting to social and educat...</td>\n",
              "      <td>https://thefederalist.com/2016/05/23/how-anti-...</td>\n",
              "      <td>Federalist</td>\n",
              "      <td>white-nationalism</td>\n",
              "      <td>right</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Expresses writer’s opinion</td>\n",
              "      <td>['evil', 'white']</td>\n",
              "      <td>1</td>\n",
              "      <td>[Young, white, men, ,, reacting, to, social, a...</td>\n",
              "      <td>[Young, white, men, ,, reacting, social, educa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3672</th>\n",
              "      <td>Young women taking part in high school and col...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Breitbart</td>\n",
              "      <td>sport</td>\n",
              "      <td>right</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Somewhat factual but also opinionated</td>\n",
              "      <td>['dashed', '\"identify\"']</td>\n",
              "      <td>1</td>\n",
              "      <td>[Young, women, taking, part, in, high, school,...</td>\n",
              "      <td>[Young, woman, taking, part, high, school, col...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3673</th>\n",
              "      <td>YouTube is making clear there will be no “birt...</td>\n",
              "      <td>https://eu.usatoday.com/story/tech/2020/02/03/...</td>\n",
              "      <td>USA Today</td>\n",
              "      <td>elections-2020</td>\n",
              "      <td>center</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Entirely factual</td>\n",
              "      <td>['birtherism']</td>\n",
              "      <td>1</td>\n",
              "      <td>[YouTube, is, making, clear, there, will, be, ...</td>\n",
              "      <td>[YouTube, making, clear, “, birtherism, ”, pla...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3674 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ...                                          lda_ready\n",
              "0     \"Orange Is the New Black\" star Yael Stone is r...  ...  [``, Orange, Is, New, Black, '', star, Yael, S...\n",
              "1     \"We have one beautiful law,\" Trump recently sa...  ...  [``, We, one, beautiful, law, ,, '', Trump, re...\n",
              "2     ...immigrants as criminals and eugenics, all o...  ...  [..., immigrant, criminal, eugenics, ,, consid...\n",
              "3     ...we sounded the alarm in the early months of...  ...  [..., sounded, alarm, early, month, Trump, ’, ...\n",
              "4     [Black Lives Matter] is essentially a non-fals...  ...  [[, Black, Lives, Matter, ], essentially, non-...\n",
              "...                                                 ...  ...                                                ...\n",
              "3669  You’ve heard of Jim Crow and Southern Segregat...  ...  [You, ’, heard, Jim, Crow, Southern, Segregati...\n",
              "3670  Young female athletes’ dreams and accomplishme...  ...  [Young, female, athlete, ’, dream, accomplishm...\n",
              "3671  Young white men, reacting to social and educat...  ...  [Young, white, men, ,, reacting, social, educa...\n",
              "3672  Young women taking part in high school and col...  ...  [Young, woman, taking, part, high, school, col...\n",
              "3673  YouTube is making clear there will be no “birt...  ...  [YouTube, making, clear, “, birtherism, ”, pla...\n",
              "\n",
              "[3674 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_8NJ74aTW5C",
        "outputId": "55c05510-c390-4297-fbd6-daa76804e042"
      },
      "source": [
        "doc_sample = babe[[\"text\"]].iloc[202].values[0]\n",
        "print('original document: ')\n",
        "words = []\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "print(words)\n",
        "print('\\n\\n tokenized and lemmatized document: ')\n",
        "print(babe[[\"lda_ready\"]].iloc[202].values[0])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original document: \n",
            "['After', 'the', 'first', 'Black', 'president,', 'Barack', 'Obama,', 'was', 'elected', 'in', '2008,', 'the', 'number', 'of', 'hate', 'groups', '“ballooned,”', 'Miller-Idriss', 'said,', 'just', 'as', 'Ku', 'Klux', 'Klan', 'activity', 'grew', 'again', 'after', 'the', '1954', 'Brown', 'v.', 'Board', 'of', 'Ed.', 'decision', 'desegregating', 'schools,', 'and', 'during', 'the', '1960s', 'civil', 'rights', 'movement.']\n",
            "\n",
            "\n",
            " tokenized and lemmatized document: \n",
            "['After', 'first', 'Black', 'president', ',', 'Barack', 'Obama', ',', 'elected', '2008', ',', 'number', 'hate', 'group', '“', 'ballooned', ',', '”', 'Miller-Idriss', 'said', ',', 'Ku', 'Klux', 'Klan', 'activity', 'grew', '1954', 'Brown', 'v.', 'Board', 'Ed', '.', 'decision', 'desegregating', 'school', ',', '1960s', 'civil', 'right', 'movement', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfjDciC8TW5D",
        "outputId": "4ae9f382-4cff-46a2-8212-130f8a1fb404"
      },
      "source": [
        "processed_docs = babe[\"lda_ready\"]\n",
        "processed_docs[:10]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [``, Orange, Is, New, Black, '', star, Yael, S...\n",
              "1    [``, We, one, beautiful, law, ,, '', Trump, re...\n",
              "2    [..., immigrant, criminal, eugenics, ,, consid...\n",
              "3    [..., sounded, alarm, early, month, Trump, ’, ...\n",
              "4    [[, Black, Lives, Matter, ], essentially, non-...\n",
              "5    [[, Democrats, employ, ], full, arsenal, deleg...\n",
              "6    [[, Newsoms, 's, ], obsession, mask, created, ...\n",
              "7    [[, Newsoms, 's, ], onslaught, propaganda, ign...\n",
              "8    [[, The, police, ], prefer, think, like, glori...\n",
              "9    [‘, A, new, low, ’, :, Washington, Post, mediu...\n",
              "Name: lda_ready, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlvjweApTW5D",
        "outputId": "1538b513-86ed-4287-ae1a-b28e6fddadf9"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 ''\n",
            "1 .\n",
            "2 Australia\n",
            "3 Black\n",
            "4 Is\n",
            "5 New\n",
            "6 Orange\n",
            "7 Stone\n",
            "8 U.S.\n",
            "9 Yael\n",
            "10 ``\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdecepouTW5E"
      },
      "source": [
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "docP_dmvTW5E",
        "outputId": "14d0d2c8-61bd-4d7c-888f-0f13a2720288"
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "bow_corpus[202]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1),\n",
              " (16, 1),\n",
              " (41, 1),\n",
              " (42, 1),\n",
              " (92, 1),\n",
              " (97, 1),\n",
              " (172, 1),\n",
              " (182, 1),\n",
              " (255, 1),\n",
              " (260, 1),\n",
              " (275, 1),\n",
              " (485, 1),\n",
              " (486, 1),\n",
              " (519, 1),\n",
              " (574, 1),\n",
              " (668, 1),\n",
              " (674, 1),\n",
              " (716, 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh6TcAH2TW5E",
        "outputId": "6045cc5b-2a94-4824-9c22-44cd53a8f609"
      },
      "source": [
        "bow_doc_202 = bow_corpus[202]\n",
        "for i in range(len(bow_doc_202)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_202[i][0], \n",
        "                                               dictionary[bow_doc_202[i][0]], \n",
        "bow_doc_202[i][1]))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 1 (\"Black\") appears 1 time.\n",
            "Word 16 (\"said\") appears 1 time.\n",
            "Word 41 (\"“\") appears 1 time.\n",
            "Word 42 (\"”\") appears 1 time.\n",
            "Word 92 (\"president\") appears 1 time.\n",
            "Word 97 (\"decision\") appears 1 time.\n",
            "Word 172 (\"civil\") appears 1 time.\n",
            "Word 182 (\"group\") appears 1 time.\n",
            "Word 255 (\"right\") appears 1 time.\n",
            "Word 260 (\"first\") appears 1 time.\n",
            "Word 275 (\"movement\") appears 1 time.\n",
            "Word 485 (\"Barack\") appears 1 time.\n",
            "Word 486 (\"Obama\") appears 1 time.\n",
            "Word 519 (\"school\") appears 1 time.\n",
            "Word 574 (\"number\") appears 1 time.\n",
            "Word 668 (\"hate\") appears 1 time.\n",
            "Word 674 (\"After\") appears 1 time.\n",
            "Word 716 (\"elected\") appears 1 time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJo4ZPtWTW5F"
      },
      "source": [
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na0TUKvQTW5F",
        "outputId": "b27e438c-f693-46d5-be5c-915106eaaafe"
      },
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.029*\"Trump\" + 0.025*\"’\" + 0.013*\"—\" + 0.011*\"police\" + 0.011*\"The\" + 0.010*\"''\" + 0.009*\"``\" + 0.008*\"protester\" + 0.008*\"'s\" + 0.008*\"would\"\n",
            "Topic: 1 \n",
            "Words: 0.029*\"’\" + 0.021*\"Trump\" + 0.019*\"The\" + 0.012*\"student\" + 0.011*\"U.S.\" + 0.011*\"'s\" + 0.010*\"—\" + 0.009*\"”\" + 0.009*\"“\" + 0.008*\"year\"\n",
            "Topic: 2 \n",
            "Words: 0.029*\"“\" + 0.029*\"”\" + 0.025*\"’\" + 0.020*\"Trump\" + 0.013*\"The\" + 0.013*\"police\" + 0.012*\"Floyd\" + 0.012*\"people\" + 0.009*\"George\" + 0.009*\"Donald\"\n",
            "Topic: 3 \n",
            "Words: 0.025*\"Trump\" + 0.019*\"’\" + 0.018*\"The\" + 0.015*\"coronavirus\" + 0.010*\"would\" + 0.009*\"woman\" + 0.009*\"vaccine\" + 0.008*\"gun\" + 0.008*\"state\" + 0.008*\"year\"\n",
            "Topic: 4 \n",
            "Words: 0.040*\"’\" + 0.028*\"Trump\" + 0.015*\"The\" + 0.014*\"President\" + 0.013*\"Democrats\" + 0.012*\"abortion\" + 0.012*\"“\" + 0.012*\"Donald\" + 0.012*\"”\" + 0.011*\"House\"\n",
            "Topic: 5 \n",
            "Words: 0.015*\"people\" + 0.013*\"U.S.\" + 0.013*\"“\" + 0.013*\"”\" + 0.012*\"’\" + 0.011*\"Trump\" + 0.010*\"coronavirus\" + 0.009*\"(\" + 0.009*\")\" + 0.008*\"year\"\n",
            "Topic: 6 \n",
            "Words: 0.023*\"woman\" + 0.018*\"Trump\" + 0.017*\"The\" + 0.014*\"’\" + 0.010*\"people\" + 0.009*\"police\" + 0.009*\"would\" + 0.009*\"President\" + 0.008*\"said\" + 0.007*\"death\"\n",
            "Topic: 7 \n",
            "Words: 0.072*\"’\" + 0.032*\"Trump\" + 0.013*\"The\" + 0.012*\"President\" + 0.010*\"Donald\" + 0.009*\"would\" + 0.008*\"Biden\" + 0.007*\"people\" + 0.007*\":\" + 0.006*\"law\"\n",
            "Topic: 8 \n",
            "Words: 0.031*\"”\" + 0.030*\"“\" + 0.019*\"’\" + 0.010*\"police\" + 0.010*\"said\" + 0.010*\"U.S.\" + 0.010*\"white\" + 0.007*\"American\" + 0.007*\"country\" + 0.007*\"illegal\"\n",
            "Topic: 9 \n",
            "Words: 0.022*\"Trump\" + 0.022*\"The\" + 0.017*\"’\" + 0.010*\"”\" + 0.010*\"“\" + 0.009*\")\" + 0.009*\"made\" + 0.009*\"(\" + 0.008*\"Americans\" + 0.007*\"coronavirus\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD-cZWJbsn8i"
      },
      "source": [
        "### Right vs. Left Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzoi-wIIav1x",
        "outputId": "794d3d7a-a551-40a4-9194-0e067a0012ad"
      },
      "source": [
        "df_rl = df\n",
        "df_rl = df_rl[df_rl[\"Bias\"] != \"AllSides Media Bias Rating: Center\"]\n",
        "df_rl[\"Label\"] = df_rl.Bias.apply(lambda x: 1 if x == \"AllSides Media Bias Rating: Lean Right\" or x == \"AllSides Media Bias Rating: Right\" else 0)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpFaSasYkqaK"
      },
      "source": [
        "df_rl1 = df\n",
        "df_rl1 = df_rl1[df_rl1[\"Bias\"] != \"AllSides Media Bias Rating: Center\"]\n",
        "df_rl1 = df_rl1[df_rl1[\"Bias\"] != \"AllSides Media Bias Rating: Lean Right\"]\n",
        "df_rl1 = df_rl1[df_rl1[\"Bias\"] != \"AllSides Media Bias Rating: Lean Left\"]\n",
        "df_rl1[\"Label\"] = df_rl1.Bias.apply(lambda x: 1 if x == \"AllSides Media Bias Rating: Right\" else 0)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY7Ud4rxxSeF",
        "outputId": "facb2678-8242-4ffd-95b2-fa884722b791"
      },
      "source": [
        "df_rl[\"Label\"].value_counts()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2816\n",
              "1    1607\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07C1dE3WawCJ",
        "outputId": "172f45c7-45c2-4f03-86e5-910b691110db"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_rl, model_name='bert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - ETA: 0s - loss: 0.6493WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - 41s 244ms/step - loss: 0.6493 - val_loss: 0.6305\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 24s 218ms/step - loss: 0.5771 - val_loss: 0.6417\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.6305\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - ETA: 0s - loss: 0.6567WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - 41s 240ms/step - loss: 0.6567 - val_loss: 0.6339\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 24s 219ms/step - loss: 0.6039 - val_loss: 0.6228\n",
            "Epoch 3/10\n",
            "111/111 [==============================] - 24s 218ms/step - loss: 0.4365 - val_loss: 0.7116\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.6228\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - ETA: 0s - loss: 0.6558WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - 41s 248ms/step - loss: 0.6558 - val_loss: 0.6371\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 24s 218ms/step - loss: 0.6214 - val_loss: 0.6201\n",
            "Epoch 3/10\n",
            "111/111 [==============================] - 24s 218ms/step - loss: 0.4650 - val_loss: 0.6694\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.6201\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - ETA: 0s - loss: 0.6615WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - 38s 216ms/step - loss: 0.6615 - val_loss: 0.6386\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 22s 194ms/step - loss: 0.6551 - val_loss: 0.6409\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.6386\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - ETA: 0s - loss: 0.6534WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - 41s 244ms/step - loss: 0.6534 - val_loss: 0.6356\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 24s 217ms/step - loss: 0.5923 - val_loss: 0.6212\n",
            "Epoch 3/10\n",
            "111/111 [==============================] - 24s 219ms/step - loss: 0.4040 - val_loss: 0.7358\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.6212\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.6266334652900696\n",
            "5-Fold CV Accuracy: 0.6660597694097194\n",
            "5-Fold CV Precision: 0.6268547652968749\n",
            "5-Fold CV Recall: 0.24952884038621542\n",
            "5-Fold CV F1 Score: 0.3310671839040946\n",
            "5-Fold CV Micro F1 Score: 0.6660597694097194\n",
            "5-Fold CV Weighted Macro F1 Score: 0.6133847128083066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viKfiR3ouSKX",
        "outputId": "0c55aa34-40d8-4036-ecd7-45a034839b6c"
      },
      "source": [
        "df_rl1[\"Label\"].value_counts()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2816\n",
              "1    1607\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR1Ga_S6lQbj",
        "outputId": "db26c4af-a3a4-4f61-8783-46ccfccafafb"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_rl1, model_name='bert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - ETA: 0s - loss: 0.6579WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - 41s 241ms/step - loss: 0.6579 - val_loss: 0.6360\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 24s 218ms/step - loss: 0.6177 - val_loss: 0.6467\n",
            "28/28 [==============================] - 2s 67ms/step - loss: 0.6360\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - ETA: 0s - loss: 0.6636WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - 41s 244ms/step - loss: 0.6636 - val_loss: 0.6580\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 24s 219ms/step - loss: 0.6440 - val_loss: 0.6389\n",
            "Epoch 3/10\n",
            "111/111 [==============================] - 24s 218ms/step - loss: 0.5589 - val_loss: 0.6103\n",
            "Epoch 4/10\n",
            "111/111 [==============================] - 24s 219ms/step - loss: 0.3654 - val_loss: 0.7281\n",
            "28/28 [==============================] - 2s 68ms/step - loss: 0.6103\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - ETA: 0s - loss: 0.6528WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - 40s 240ms/step - loss: 0.6528 - val_loss: 0.6478\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 24s 218ms/step - loss: 0.5720 - val_loss: 0.6084\n",
            "Epoch 3/10\n",
            "111/111 [==============================] - 24s 218ms/step - loss: 0.3577 - val_loss: 0.7761\n",
            "28/28 [==============================] - 2s 65ms/step - loss: 0.6084\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - ETA: 0s - loss: 0.6608WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - 38s 220ms/step - loss: 0.6608 - val_loss: 0.6594\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 21s 194ms/step - loss: 0.6339 - val_loss: 0.6255\n",
            "Epoch 3/10\n",
            "111/111 [==============================] - 21s 194ms/step - loss: 0.4943 - val_loss: 0.6309\n",
            "28/28 [==============================] - 2s 77ms/step - loss: 0.6255\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - ETA: 0s - loss: 0.6582WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "111/111 [==============================] - 40s 240ms/step - loss: 0.6582 - val_loss: 0.6380\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 24s 218ms/step - loss: 0.6002 - val_loss: 0.6121\n",
            "Epoch 3/10\n",
            "111/111 [==============================] - 24s 218ms/step - loss: 0.4104 - val_loss: 0.7029\n",
            "28/28 [==============================] - 2s 66ms/step - loss: 0.6121\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.6184610843658447\n",
            "5-Fold CV Accuracy: 0.6656108597285068\n",
            "5-Fold CV Precision: 0.4665108931163977\n",
            "5-Fold CV Recall: 0.2905284340473288\n",
            "5-Fold CV F1 Score: 0.35627166525997567\n",
            "5-Fold CV Micro F1 Score: 0.6656108597285068\n",
            "5-Fold CV Weighted Macro F1 Score: 0.6195202013045962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9c9Wp2p0Wr3"
      },
      "source": [
        "### Mainstream\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inxtkl770bGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e1c696-037c-4106-a4c8-f45d55ced24f"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(new_df, model_name='roberta', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.6957WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "209/209 [==============================] - 63s 236ms/step - loss: 0.6957 - val_loss: 0.6942\n",
            "Epoch 2/10\n",
            "209/209 [==============================] - 47s 224ms/step - loss: 0.6954 - val_loss: 0.6961\n",
            "53/53 [==============================] - 4s 67ms/step - loss: 0.6942\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.6966WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "209/209 [==============================] - 64s 237ms/step - loss: 0.6966 - val_loss: 0.6944\n",
            "Epoch 2/10\n",
            "209/209 [==============================] - 47s 225ms/step - loss: 0.6950 - val_loss: 0.6929\n",
            "Epoch 3/10\n",
            "209/209 [==============================] - 47s 225ms/step - loss: 0.6956 - val_loss: 0.6954\n",
            "53/53 [==============================] - 4s 69ms/step - loss: 0.6929\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.6958WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "209/209 [==============================] - 65s 238ms/step - loss: 0.6958 - val_loss: 0.6937\n",
            "Epoch 2/10\n",
            "209/209 [==============================] - 47s 225ms/step - loss: 0.6953 - val_loss: 0.6936\n",
            "Epoch 3/10\n",
            "209/209 [==============================] - 47s 226ms/step - loss: 0.6945 - val_loss: 0.6946\n",
            "53/53 [==============================] - 4s 74ms/step - loss: 0.6936\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.6953WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "209/209 [==============================] - 62s 225ms/step - loss: 0.6953 - val_loss: 0.6932\n",
            "Epoch 2/10\n",
            "209/209 [==============================] - 45s 214ms/step - loss: 0.6946 - val_loss: 0.6933\n",
            "53/53 [==============================] - 4s 77ms/step - loss: 0.6932\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.6910WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "209/209 [==============================] - 64s 236ms/step - loss: 0.6910 - val_loss: 0.6929\n",
            "Epoch 2/10\n",
            "209/209 [==============================] - 47s 224ms/step - loss: 0.6932 - val_loss: 0.6896\n",
            "Epoch 3/10\n",
            "209/209 [==============================] - 47s 224ms/step - loss: 0.6893 - val_loss: 0.6933\n",
            "53/53 [==============================] - 4s 67ms/step - loss: 0.6896\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.6927016973495483\n",
            "5-Fold CV Accuracy: 0.5148770245950811\n",
            "5-Fold CV Precision: 0.31210444175782415\n",
            "5-Fold CV Recall: 0.516929937442603\n",
            "5-Fold CV F1 Score: 0.3824815803731133\n",
            "5-Fold CV Micro F1 Score: 0.5148770245950811\n",
            "5-Fold CV Weighted Macro F1 Score: 0.3879183313460962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eSNyMp_sn8j"
      },
      "source": [
        "## AdFontes Article Specific Headlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "vsnaC4IvawJy",
        "outputId": "a0f85e61-df2e-42fc-a06b-6c7ca78728f4"
      },
      "source": [
        "plt.scatter(af_headlines[\"Bias\"], af_headlines[\"Reliability\"])\n",
        "plt.ylabel('Reliability')\n",
        "plt.xlabel('Bias')\n",
        "plt.title('Headlines by bias and reliability')\n",
        "plt.show()\n",
        "\n",
        "          "
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAxyYgNQawNY",
        "outputId": "0e65af5a-0194-4f81-e3f3-3453680f7d8b"
      },
      "source": [
        "print(\"Number of headlines that are extreme: \", af_headlines[\"Headline\"][(af_headlines[\"Bias\"] > 10) | (af_headlines[\"Bias\"] < -10)].count())\n",
        "print(\"Number of headlines that are central: \", af_headlines[\"Headline\"][(af_headlines[\"Bias\"] < 5) & (af_headlines[\"Bias\"] > -5)].count())\n",
        "print(\"Total number of headlines: \", af_headlines[\"Headline\"].count())"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of headlines that are extreme:  1887\n",
            "Number of headlines that are central:  2497\n",
            "Total number of headlines:  4384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrdWpqF9sn8p"
      },
      "source": [
        "#### AdFontes BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7r1_pvBYWx1",
        "outputId": "bee5bfbc-fca1-4075-ecc9-00545c430b8c"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "val_loss1, val_acc1, val_prec1, val_rec1, val_f11, val_f1_micro1, val_f1_wmacro1 = run_model_5fold(af_headlines, model_name='bert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss1)\n",
        "acc_cv = np.mean(val_acc1)\n",
        "prec_cv = np.mean(val_prec1)\n",
        "rec_cv = np.mean(val_rec1)\n",
        "f1_cv = np.mean(val_f11)\n",
        "f1_micro_cv = np.mean(val_f1_micro1)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro1)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5700WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 49s 304ms/step - loss: 0.5700 - val_loss: 0.5321\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 282ms/step - loss: 0.3722 - val_loss: 0.4931\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 31s 282ms/step - loss: 0.1825 - val_loss: 0.6459\n",
            "28/28 [==============================] - 2s 80ms/step - loss: 0.4931\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5828WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 47s 306ms/step - loss: 0.5828 - val_loss: 0.4935\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 284ms/step - loss: 0.3602 - val_loss: 0.4733\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 31s 284ms/step - loss: 0.1358 - val_loss: 0.6823\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.4733\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5772WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 48s 307ms/step - loss: 0.5772 - val_loss: 0.4922\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 285ms/step - loss: 0.3989 - val_loss: 0.5101\n",
            "28/28 [==============================] - 3s 90ms/step - loss: 0.4922\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5609WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 47s 305ms/step - loss: 0.5609 - val_loss: 0.4818\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 282ms/step - loss: 0.3545 - val_loss: 0.5001\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.4818\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5802WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 48s 313ms/step - loss: 0.5802 - val_loss: 0.5321\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 286ms/step - loss: 0.3809 - val_loss: 0.6358\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.5321\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.49450960755348206\n",
            "5-Fold CV Accuracy: 0.7613988639144447\n",
            "5-Fold CV Precision: 0.7128969603065951\n",
            "5-Fold CV Recall: 0.7498519360588326\n",
            "5-Fold CV F1 Score: 0.7300325607983335\n",
            "5-Fold CV Micro F1 Score: 0.7613988639144447\n",
            "5-Fold CV Weighted Macro F1 Score: 0.761843227515876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibEcQpQ3cba8"
      },
      "source": [
        "### AdFontes DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5OGoOVJ79i5",
        "outputId": "8cf94372-7d3e-49a6-f442-3de4201ef8ee"
      },
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(af_headlines, model_name='distilbert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5743WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 24s 158ms/step - loss: 0.5743 - val_loss: 0.5102\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 16s 146ms/step - loss: 0.3679 - val_loss: 0.5328\n",
            "28/28 [==============================] - 1s 40ms/step - loss: 0.5102\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5634WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 26s 171ms/step - loss: 0.5634 - val_loss: 0.4875\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 16s 146ms/step - loss: 0.3619 - val_loss: 0.5111\n",
            "28/28 [==============================] - 1s 43ms/step - loss: 0.4875\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5695WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 25s 160ms/step - loss: 0.5695 - val_loss: 0.4919\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 16s 147ms/step - loss: 0.3543 - val_loss: 0.5062\n",
            "28/28 [==============================] - 1s 46ms/step - loss: 0.4919\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5594WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 25s 158ms/step - loss: 0.5594 - val_loss: 0.4978\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 16s 145ms/step - loss: 0.3680 - val_loss: 0.5669\n",
            "28/28 [==============================] - 1s 49ms/step - loss: 0.4978\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5484WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 24s 160ms/step - loss: 0.5484 - val_loss: 0.5443\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 16s 148ms/step - loss: 0.3450 - val_loss: 0.5430\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 16s 148ms/step - loss: 0.1338 - val_loss: 0.8039\n",
            "28/28 [==============================] - 1s 49ms/step - loss: 0.5430\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.5060978829860687\n",
            "5-Fold CV Accuracy: 0.7563859254515445\n",
            "5-Fold CV Precision: 0.7458631900580611\n",
            "5-Fold CV Recall: 0.6603118465187431\n",
            "5-Fold CV F1 Score: 0.6992617401766958\n",
            "5-Fold CV Micro F1 Score: 0.7563859254515447\n",
            "5-Fold CV Weighted Macro F1 Score: 0.753744090617929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4ZJ3vZPcfg7"
      },
      "source": [
        "### AdFontes RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CavzAZCZ70e7",
        "outputId": "90935298-3d52-4a7f-c7d5-abf82fae7f91"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(af_headlines, model_name='roberta', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5168WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 49s 322ms/step - loss: 0.5168 - val_loss: 0.4083\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 33s 297ms/step - loss: 0.3968 - val_loss: 0.3862\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 33s 297ms/step - loss: 0.2878 - val_loss: 0.5483\n",
            "28/28 [==============================] - 2s 87ms/step - loss: 0.3862\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5275WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 49s 319ms/step - loss: 0.5275 - val_loss: 0.4457\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 33s 297ms/step - loss: 0.3910 - val_loss: 0.4378\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 33s 297ms/step - loss: 0.2725 - val_loss: 0.4201\n",
            "Epoch 4/10\n",
            "110/110 [==============================] - 33s 297ms/step - loss: 0.2197 - val_loss: 0.5919\n",
            "28/28 [==============================] - 2s 89ms/step - loss: 0.4201\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5129WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 50s 329ms/step - loss: 0.5129 - val_loss: 0.4388\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 33s 299ms/step - loss: 0.3808 - val_loss: 0.4172\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 33s 299ms/step - loss: 0.2657 - val_loss: 0.5501\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.4172\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5256WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 48s 311ms/step - loss: 0.5256 - val_loss: 0.4366\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 32s 289ms/step - loss: 0.3685 - val_loss: 0.4656\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.4366\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5076WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 49s 320ms/step - loss: 0.5076 - val_loss: 0.4787\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 33s 298ms/step - loss: 0.3924 - val_loss: 0.5021\n",
            "28/28 [==============================] - 3s 90ms/step - loss: 0.4787\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.4277645587921143\n",
            "5-Fold CV Accuracy: 0.8093013750696386\n",
            "5-Fold CV Precision: 0.7473454044598011\n",
            "5-Fold CV Recall: 0.8420782282851247\n",
            "5-Fold CV F1 Score: 0.7917720593808089\n",
            "5-Fold CV Micro F1 Score: 0.8093013750696386\n",
            "5-Fold CV Weighted Macro F1 Score: 0.8101656992850058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_l9dGfpq-q-",
        "outputId": "77339d1b-8aa3-40dc-abe7-ac0f3d69fe64"
      },
      "source": [
        "val_loss"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.38623112440109253,\n",
              " 0.4201417863368988,\n",
              " 0.4171997308731079,\n",
              " 0.4365672767162323,\n",
              " 0.47868287563323975]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp-gA3CD8Iww",
        "outputId": "2c368b05-0437-43e2-9c03-bf2e661686cd"
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(af_headlines, model_name='xlnet', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5516WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "110/110 [==============================] - 56s 386ms/step - loss: 0.5516 - val_loss: 0.5051\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 40s 367ms/step - loss: 0.4279 - val_loss: 0.4820\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 40s 367ms/step - loss: 0.3267 - val_loss: 0.5313\n",
            "28/28 [==============================] - 3s 103ms/step - loss: 0.4820\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5825WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "110/110 [==============================] - 56s 389ms/step - loss: 0.5825 - val_loss: 0.5163\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 40s 367ms/step - loss: 0.4693 - val_loss: 0.4845\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 40s 367ms/step - loss: 0.3771 - val_loss: 0.5213\n",
            "28/28 [==============================] - 3s 104ms/step - loss: 0.4845\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5749WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "110/110 [==============================] - 55s 388ms/step - loss: 0.5749 - val_loss: 0.4802\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 41s 370ms/step - loss: 0.4612 - val_loss: 0.4835\n",
            "28/28 [==============================] - 3s 116ms/step - loss: 0.4802\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5677WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "110/110 [==============================] - 56s 381ms/step - loss: 0.5677 - val_loss: 0.4846\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 40s 363ms/step - loss: 0.4528 - val_loss: 0.4671\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 40s 362ms/step - loss: 0.3544 - val_loss: 0.4827\n",
            "28/28 [==============================] - 3s 119ms/step - loss: 0.4671\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification/transformer/mask_emb:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5530WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "110/110 [==============================] - 56s 388ms/step - loss: 0.5530 - val_loss: 0.5659\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 41s 369ms/step - loss: 0.4573 - val_loss: 0.5247\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 41s 369ms/step - loss: 0.3213 - val_loss: 0.6122\n",
            "28/28 [==============================] - 3s 115ms/step - loss: 0.5247\n",
            "5-Fold CV Loss: 0.48769731521606446\n",
            "5-Fold CV Accuracy: 0.7709824380541801\n",
            "5-Fold CV Precision: 0.7278204512444623\n",
            "5-Fold CV Recall: 0.7582964927792514\n",
            "5-Fold CV F1 Score: 0.7386476039148221\n",
            "5-Fold CV Micro F1 Score: 0.7709824380541801\n",
            "5-Fold CV Weighted Macro F1 Score: 0.7705696412592824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZHELkMKsn8q"
      },
      "source": [
        "#### Include Pre-Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0ZybYMx9EPB"
      },
      "source": [
        "def preprocess(df):\n",
        "    \"\"\"convert a pandas dataframe into a tensorflow dataset\"\"\"\n",
        "    target = df.pop('Label')\n",
        "    sentence = df.pop('Headline')\n",
        "\n",
        "    #tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') #uncased\n",
        "    #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "                        sentence.tolist(),                      \n",
        "                        add_special_tokens = True, # add [CLS], [SEP]\n",
        "                        truncation = True, # cut off at max length of the text that can go to BERT\n",
        "                        padding = True, # add [PAD] tokens\n",
        "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "              )\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (dict(train_encodings), \n",
        "         target.tolist()))\n",
        "    return dataset"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eSD6vHs9EWT"
      },
      "source": [
        "# pandas -> tensorflow\n",
        "# train-test split\n",
        "df_train, df_test = train_test_split(df, test_size = 0.10, random_state = 42)\n",
        "\n",
        "\n",
        "train_distant_dataset = preprocess(df_train)\n",
        "test_distant_dataset = preprocess(df_test)\n",
        "\n",
        "# batch and randomize\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_distant_dataset = train_distant_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_distant_dataset = test_distant_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFlLqfIC9EZS"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I8-Phan9Ecf",
        "outputId": "e11a8990-2f42-466a-9772-cef90c939594"
      },
      "source": [
        "# train entire model with distant signals\n",
        "#bert = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\") #DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output\n",
        "#bert = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "roberta = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True) # after 3 epochs without improvement, stop training\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "roberta.compile(optimizer=optimizer, loss=roberta.compute_loss)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G6_Cq0f9EfO",
        "outputId": "301871cc-e811-45a8-959a-8ec036f51c48"
      },
      "source": [
        "history_bert = roberta.fit(train_distant_dataset, epochs=1, validation_data = test_distant_dataset, callbacks=[callback])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "242/242 [==============================] - ETA: 0s - loss: 0.6564WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "242/242 [==============================] - 80s 268ms/step - loss: 0.6564 - val_loss: 0.6325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS79QmhT9Eij"
      },
      "source": [
        "trained_layer = roberta.get_layer(index=0).get_weights()\n",
        "\n",
        "roberta.save_weights('./checkpoints/bert_final_checkpoint_news_headlines_USA')\n",
        "\n",
        "#bert.load_weights('./checkpoints/final_checkpoint_distant_learning')"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEcnPTF48N2F",
        "outputId": "7dee6ab8-e856-40ae-927a-ebdf25606d20"
      },
      "source": [
        "transfer_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "transfer_model.compile(optimizer=optimizer, loss=transfer_model.compute_loss) \n",
        "\n",
        "transfer_model.load_weights('./checkpoints/bert_final_checkpoint_news_headlines_USA')\n",
        "trained_model_layer = transfer_model.get_layer(index=0).get_weights()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwYpQcBFsn8s"
      },
      "source": [
        "#### AdFontes Pre-Trained on AllSides"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVwqil7U-JbJ",
        "outputId": "1f9d9295-6caf-4e37-cd8b-e401b660af37"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(af_headlines, model_name='roberta', \n",
        "                                                                                            freeze_encoder=False, pretrained=True)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5196WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 54s 326ms/step - loss: 0.5196 - val_loss: 0.4608\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 33s 297ms/step - loss: 0.3944 - val_loss: 0.4374\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 33s 297ms/step - loss: 0.2684 - val_loss: 0.4805\n",
            "28/28 [==============================] - 2s 88ms/step - loss: 0.4374\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5217WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 49s 319ms/step - loss: 0.5217 - val_loss: 0.5442\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 33s 297ms/step - loss: 0.4187 - val_loss: 0.4293\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 33s 297ms/step - loss: 0.3010 - val_loss: 0.5378\n",
            "28/28 [==============================] - 3s 90ms/step - loss: 0.4293\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5411WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 50s 322ms/step - loss: 0.5411 - val_loss: 0.4937\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 33s 299ms/step - loss: 0.4021 - val_loss: 0.4271\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 33s 299ms/step - loss: 0.3270 - val_loss: 0.4680\n",
            "28/28 [==============================] - 3s 97ms/step - loss: 0.4271\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5507WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 49s 312ms/step - loss: 0.5507 - val_loss: 0.5052\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 32s 289ms/step - loss: 0.4435 - val_loss: 0.5096\n",
            "28/28 [==============================] - 3s 98ms/step - loss: 0.5052\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5043WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 50s 321ms/step - loss: 0.5043 - val_loss: 0.4771\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 33s 298ms/step - loss: 0.3978 - val_loss: 0.5527\n",
            "28/28 [==============================] - 3s 90ms/step - loss: 0.4771\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.45520569682121276\n",
            "5-Fold CV Accuracy: 0.7876324435211364\n",
            "5-Fold CV Precision: 0.7409357355498519\n",
            "5-Fold CV Recall: 0.7817565576186267\n",
            "5-Fold CV F1 Score: 0.7573110264794962\n",
            "5-Fold CV Micro F1 Score: 0.7876324435211364\n",
            "5-Fold CV Weighted Macro F1 Score: 0.7872267482355559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlSMPbnF4jpz"
      },
      "source": [
        ""
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5td9RHPX4j-k"
      },
      "source": [
        "#### Include Pre-Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YtboA5C4j-m"
      },
      "source": [
        "def preprocess(df):\n",
        "    \"\"\"convert a pandas dataframe into a tensorflow dataset\"\"\"\n",
        "    target = df.pop('Label')\n",
        "    sentence = df.pop('Headline')\n",
        "\n",
        "    #tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') #uncased\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    #tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "                        sentence.tolist(),                      \n",
        "                        add_special_tokens = True, # add [CLS], [SEP]\n",
        "                        truncation = True, # cut off at max length of the text that can go to BERT\n",
        "                        padding = True, # add [PAD] tokens\n",
        "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "              )\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (dict(train_encodings), \n",
        "         target.tolist()))\n",
        "    return dataset"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5yLgX0h4j-n"
      },
      "source": [
        "# pandas -> tensorflow\n",
        "# train-test split\n",
        "df_train, df_test = train_test_split(df, test_size = 0.10, random_state = 42)\n",
        "\n",
        "\n",
        "train_distant_dataset = preprocess(df_train)\n",
        "test_distant_dataset = preprocess(df_test)\n",
        "\n",
        "# batch and randomize\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_distant_dataset = train_distant_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_distant_dataset = test_distant_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry4rn32r4j-p"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkBUsQEQ4j-q",
        "outputId": "bd859918-1c66-4110-d4b0-c034a6a14cb9"
      },
      "source": [
        "# train entire model with distant signals\n",
        "#bert = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\") #DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output\n",
        "bert = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "##roberta = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True) # after 3 epochs without improvement, stop training\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "bert.compile(optimizer=optimizer, loss=bert.compute_loss)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIxTyx1X4j-u",
        "outputId": "d57a9062-315b-4938-89b4-ac6b0e4db5bd"
      },
      "source": [
        "history_bert = bert.fit(train_distant_dataset, epochs=1, validation_data = test_distant_dataset, callbacks=[callback])"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "242/242 [==============================] - ETA: 0s - loss: 0.6993WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "242/242 [==============================] - 69s 219ms/step - loss: 0.6993 - val_loss: 0.6933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoL2WRWm4j-v"
      },
      "source": [
        "trained_layer = bert.get_layer(index=0).get_weights()\n",
        "\n",
        "bert.save_weights('./checkpoints/bert_final_checkpoint_news_headlines_USA')\n",
        "\n",
        "#bert.load_weights('./checkpoints/final_checkpoint_distant_learning')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q1bw_3t4j-w",
        "outputId": "bada73c7-76d6-42a4-d09d-4e8bb69fe9ec"
      },
      "source": [
        "transfer_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "transfer_model.compile(optimizer=optimizer, loss=transfer_model.compute_loss) \n",
        "\n",
        "transfer_model.load_weights('./checkpoints/bert_final_checkpoint_news_headlines_USA')\n",
        "trained_model_layer = transfer_model.get_layer(index=0).get_weights()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtZfjGSY4j-y"
      },
      "source": [
        "#### AdFontes Pre-Trained on AllSides"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z48Zkn54j-0",
        "outputId": "af82454c-1610-4aba-98e3-0945e80fd153"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(af_headlines, model_name='bert', \n",
        "                                                                                            freeze_encoder=False, pretrained=True)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6809WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "110/110 [==============================] - 53s 311ms/step - loss: 0.6809 - val_loss: 0.5901\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 282ms/step - loss: 0.5801 - val_loss: 0.5632\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 31s 282ms/step - loss: 0.4650 - val_loss: 0.5016\n",
            "Epoch 4/10\n",
            "110/110 [==============================] - 31s 285ms/step - loss: 0.3374 - val_loss: 0.5320\n",
            "28/28 [==============================] - 2s 84ms/step - loss: 0.5016\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6795WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 48s 305ms/step - loss: 0.6795 - val_loss: 0.6481\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 284ms/step - loss: 0.5676 - val_loss: 0.5189\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 31s 283ms/step - loss: 0.4061 - val_loss: 0.4970\n",
            "Epoch 4/10\n",
            "110/110 [==============================] - 31s 284ms/step - loss: 0.2063 - val_loss: 0.6171\n",
            "28/28 [==============================] - 2s 89ms/step - loss: 0.4970\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6877WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 51s 308ms/step - loss: 0.6877 - val_loss: 0.6874\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 285ms/step - loss: 0.6847 - val_loss: 0.6847\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 31s 285ms/step - loss: 0.6856 - val_loss: 0.6841\n",
            "Epoch 4/10\n",
            "110/110 [==============================] - 31s 285ms/step - loss: 0.6870 - val_loss: 0.6837\n",
            "Epoch 5/10\n",
            "110/110 [==============================] - 32s 290ms/step - loss: 0.6847 - val_loss: 0.6844\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.6837\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6794WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 49s 305ms/step - loss: 0.6794 - val_loss: 0.6845\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 283ms/step - loss: 0.6382 - val_loss: 0.5540\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 31s 283ms/step - loss: 0.5087 - val_loss: 0.5604\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.5540\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6865WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 50s 310ms/step - loss: 0.6865 - val_loss: 0.6856\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 286ms/step - loss: 0.6854 - val_loss: 0.6860\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.6856\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.584377509355545\n",
            "5-Fold CV Accuracy: 0.67926331464155\n",
            "5-Fold CV Precision: 0.4274562211981567\n",
            "5-Fold CV Recall: 0.437779461917393\n",
            "5-Fold CV F1 Score: 0.43078191676289135\n",
            "5-Fold CV Micro F1 Score: 0.67926331464155\n",
            "5-Fold CV Weighted Macro F1 Score: 0.6165497410885905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn-MhrHFsn8t"
      },
      "source": [
        "#### AdFontes Freeze Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0xrrs2o2Z6Y",
        "outputId": "b94be1af-bc27-44b4-8d27-a901844af862"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(af_headlines, model_name='roberta', \n",
        "                                                                                            freeze_encoder=True, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6812WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 22s 153ms/step - loss: 0.6812 - val_loss: 0.6725\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 14s 130ms/step - loss: 0.6702 - val_loss: 0.6625\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 14s 130ms/step - loss: 0.6531 - val_loss: 0.6463\n",
            "Epoch 4/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.6412 - val_loss: 0.6319\n",
            "Epoch 5/10\n",
            "110/110 [==============================] - 15s 135ms/step - loss: 0.6246 - val_loss: 0.6157\n",
            "Epoch 6/10\n",
            "110/110 [==============================] - 15s 134ms/step - loss: 0.6115 - val_loss: 0.5991\n",
            "Epoch 7/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.5954 - val_loss: 0.5817\n",
            "Epoch 8/10\n",
            "110/110 [==============================] - 14s 130ms/step - loss: 0.5817 - val_loss: 0.5686\n",
            "Epoch 9/10\n",
            "110/110 [==============================] - 14s 130ms/step - loss: 0.5724 - val_loss: 0.5530\n",
            "Epoch 10/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.5578 - val_loss: 0.5490\n",
            "28/28 [==============================] - 3s 92ms/step - loss: 0.5490\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6810WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 22s 154ms/step - loss: 0.6810 - val_loss: 0.6726\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.6677 - val_loss: 0.6600\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 14s 132ms/step - loss: 0.6528 - val_loss: 0.6464\n",
            "Epoch 4/10\n",
            "110/110 [==============================] - 15s 135ms/step - loss: 0.6410 - val_loss: 0.6333\n",
            "Epoch 5/10\n",
            "110/110 [==============================] - 15s 136ms/step - loss: 0.6212 - val_loss: 0.6177\n",
            "Epoch 6/10\n",
            "110/110 [==============================] - 14s 132ms/step - loss: 0.6085 - val_loss: 0.6015\n",
            "Epoch 7/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.5925 - val_loss: 0.5834\n",
            "Epoch 8/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.5807 - val_loss: 0.5736\n",
            "Epoch 9/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.5676 - val_loss: 0.5580\n",
            "Epoch 10/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.5588 - val_loss: 0.5457\n",
            "28/28 [==============================] - 3s 93ms/step - loss: 0.5457\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6793WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 23s 156ms/step - loss: 0.6793 - val_loss: 0.6723\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.6661 - val_loss: 0.6598\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 15s 138ms/step - loss: 0.6500 - val_loss: 0.6464\n",
            "Epoch 4/10\n",
            "110/110 [==============================] - 15s 136ms/step - loss: 0.6385 - val_loss: 0.6330\n",
            "Epoch 5/10\n",
            "110/110 [==============================] - 15s 134ms/step - loss: 0.6227 - val_loss: 0.6193\n",
            "Epoch 6/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.6103 - val_loss: 0.6029\n",
            "Epoch 7/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.5968 - val_loss: 0.5887\n",
            "Epoch 8/10\n",
            "110/110 [==============================] - 15s 134ms/step - loss: 0.5821 - val_loss: 0.5766\n",
            "Epoch 9/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.5699 - val_loss: 0.5697\n",
            "Epoch 10/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.5617 - val_loss: 0.5564\n",
            "28/28 [==============================] - 3s 100ms/step - loss: 0.5564\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6803WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 22s 154ms/step - loss: 0.6803 - val_loss: 0.6715\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 15s 136ms/step - loss: 0.6641 - val_loss: 0.6582\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 15s 135ms/step - loss: 0.6505 - val_loss: 0.6435\n",
            "Epoch 4/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.6374 - val_loss: 0.6284\n",
            "Epoch 5/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.6221 - val_loss: 0.6123\n",
            "Epoch 6/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.6058 - val_loss: 0.5962\n",
            "Epoch 7/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.5908 - val_loss: 0.5803\n",
            "Epoch 8/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.5741 - val_loss: 0.5660\n",
            "Epoch 9/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.5655 - val_loss: 0.5561\n",
            "Epoch 10/10\n",
            "110/110 [==============================] - 14s 131ms/step - loss: 0.5618 - val_loss: 0.5434\n",
            "28/28 [==============================] - 3s 104ms/step - loss: 0.5434\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0', 'tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6800WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 25s 165ms/step - loss: 0.6800 - val_loss: 0.6731\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 15s 136ms/step - loss: 0.6664 - val_loss: 0.6620\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.6515 - val_loss: 0.6499\n",
            "Epoch 4/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.6375 - val_loss: 0.6398\n",
            "Epoch 5/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.6201 - val_loss: 0.6213\n",
            "Epoch 6/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.6073 - val_loss: 0.6074\n",
            "Epoch 7/10\n",
            "110/110 [==============================] - 15s 132ms/step - loss: 0.5872 - val_loss: 0.5935\n",
            "Epoch 8/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.5825 - val_loss: 0.5844\n",
            "Epoch 9/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.5664 - val_loss: 0.5699\n",
            "Epoch 10/10\n",
            "110/110 [==============================] - 15s 133ms/step - loss: 0.5548 - val_loss: 0.5594\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.5594\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.5507811188697815\n",
            "5-Fold CV Accuracy: 0.7224017640045194\n",
            "5-Fold CV Precision: 0.7797751743279795\n",
            "5-Fold CV Recall: 0.5024013024013023\n",
            "5-Fold CV F1 Score: 0.6043342546958874\n",
            "5-Fold CV Micro F1 Score: 0.7224017640045194\n",
            "5-Fold CV Weighted Macro F1 Score: 0.7071929935302611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp6G3oatsn8u"
      },
      "source": [
        "#### AdFontes Freeze Encoder + Pre-Trained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS5FDfaM2chQ",
        "outputId": "2f91c60f-ec38-492a-b201-b10644a95e6d"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(af_headlines, model_name='bert', \n",
        "                                                                                            freeze_encoder=True, pretrained=True)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_sequence_classification/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_sequence_classification/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6868WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 23s 154ms/step - loss: 0.6868 - val_loss: 0.6833\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 14s 125ms/step - loss: 0.6858 - val_loss: 0.6848\n",
            "28/28 [==============================] - 2s 83ms/step - loss: 0.6833\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_sequence_classification/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_sequence_classification/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6887WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 22s 159ms/step - loss: 0.6887 - val_loss: 0.6833\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 14s 126ms/step - loss: 0.6877 - val_loss: 0.6833\n",
            "28/28 [==============================] - 2s 88ms/step - loss: 0.6833\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_sequence_classification/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_sequence_classification/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6853WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 22s 151ms/step - loss: 0.6853 - val_loss: 0.6837\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 14s 128ms/step - loss: 0.6852 - val_loss: 0.6858\n",
            "28/28 [==============================] - 3s 95ms/step - loss: 0.6837\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_sequence_classification/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_sequence_classification/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6883WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 22s 157ms/step - loss: 0.6883 - val_loss: 0.6845\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 15s 132ms/step - loss: 0.6845 - val_loss: 0.6836\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 14s 129ms/step - loss: 0.6856 - val_loss: 0.6841\n",
            "28/28 [==============================] - 3s 99ms/step - loss: 0.6836\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_sequence_classification/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/embeddings/word_embeddings/weight:0', 'tf_bert_for_sequence_classification/bert/embeddings/token_type_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/position_embeddings/embeddings:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/embeddings/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._1/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._2/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._3/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._4/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._5/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._6/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._7/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._8/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._9/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._10/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/query/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/key/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/self/value/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/intermediate/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/dense/bias:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'tf_bert_for_sequence_classification/bert/encoder/layer_._11/output/LayerNorm/beta:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6862WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 22s 152ms/step - loss: 0.6862 - val_loss: 0.6842\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 14s 129ms/step - loss: 0.6861 - val_loss: 0.6834\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 14s 130ms/step - loss: 0.6864 - val_loss: 0.6835\n",
            "28/28 [==============================] - 3s 99ms/step - loss: 0.6834\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.683461356163025\n",
            "5-Fold CV Accuracy: 0.5695711823724507\n",
            "5-Fold CV Precision: 0.0\n",
            "5-Fold CV Recall: 0.0\n",
            "5-Fold CV F1 Score: 0.0\n",
            "5-Fold CV Micro F1 Score: 0.5695711823724507\n",
            "5-Fold CV Weighted Macro F1 Score: 0.41337588431512867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4GUPXkhsn8u"
      },
      "source": [
        "#### AdFontes Electra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdhudES0nM2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cbfd41e-5536-4b19-e03d-901b9b4f7ec7"
      },
      "source": [
        "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(af_headlines, model_name='electra', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6445WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 27s 106ms/step - loss: 0.6445 - val_loss: 0.6248\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.5225 - val_loss: 0.5264\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 10s 89ms/step - loss: 0.4184 - val_loss: 0.5539\n",
            "28/28 [==============================] - 1s 33ms/step - loss: 0.5264\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6359WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 27s 112ms/step - loss: 0.6359 - val_loss: 0.5520\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 9s 83ms/step - loss: 0.5213 - val_loss: 0.5266\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 9s 83ms/step - loss: 0.4152 - val_loss: 0.5237\n",
            "Epoch 4/10\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.3114 - val_loss: 0.5792\n",
            "28/28 [==============================] - 1s 33ms/step - loss: 0.5237\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6368WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 25s 106ms/step - loss: 0.6368 - val_loss: 0.5637\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 9s 83ms/step - loss: 0.5104 - val_loss: 0.5133\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 9s 83ms/step - loss: 0.4208 - val_loss: 0.5103\n",
            "Epoch 4/10\n",
            "110/110 [==============================] - 9s 83ms/step - loss: 0.3244 - val_loss: 0.5872\n",
            "28/28 [==============================] - 1s 33ms/step - loss: 0.5103\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6489WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 28s 113ms/step - loss: 0.6489 - val_loss: 0.5631\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 10s 89ms/step - loss: 0.5201 - val_loss: 0.5499\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 10s 88ms/step - loss: 0.4084 - val_loss: 0.5947\n",
            "28/28 [==============================] - 1s 37ms/step - loss: 0.5499\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.6284WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 27s 106ms/step - loss: 0.6284 - val_loss: 0.5914\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 9s 84ms/step - loss: 0.5114 - val_loss: 0.6199\n",
            "28/28 [==============================] - 1s 34ms/step - loss: 0.5914\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.5403617143630981\n",
            "5-Fold CV Accuracy: 0.7310582985791121\n",
            "5-Fold CV Precision: 0.6996443065618715\n",
            "5-Fold CV Recall: 0.6773118324842463\n",
            "5-Fold CV F1 Score: 0.6749822078981118\n",
            "5-Fold CV Micro F1 Score: 0.7310582985791121\n",
            "5-Fold CV Weighted Macro F1 Score: 0.7263453766381932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUSn5MqQsn8y"
      },
      "source": [
        "### Ablation Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xJcgqOmsn8y"
      },
      "source": [
        "Run our best classifier on first a quarter, then half the AllSides data. See how dataset size affects classifier performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpDQ9x0i3l1_"
      },
      "source": [
        ""
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtS-_Zlysn8y"
      },
      "source": [
        "df_quart = df.sample(n = round(len(df)*.25), random_state = 1)\n",
        "df_half = df.sample(n = round(len(df)*.5), random_state = 1)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZYTg5Vtsn8y"
      },
      "source": [
        "#### Quarter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_zVrg0usn8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f25dda2-68d9-4b0b-c4b4-6ea55ae59b3b"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_quart, model_name='roberta', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "54/54 [==============================] - ETA: 0s - loss: 0.6846WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "54/54 [==============================] - 28s 259ms/step - loss: 0.6846 - val_loss: 0.6536\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 11s 213ms/step - loss: 0.6546 - val_loss: 0.6607\n",
            "14/14 [==============================] - 1s 73ms/step - loss: 0.6536\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "54/54 [==============================] - ETA: 0s - loss: 0.6891WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "54/54 [==============================] - 30s 262ms/step - loss: 0.6891 - val_loss: 0.6520\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 12s 220ms/step - loss: 0.6696 - val_loss: 0.6427\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 12s 220ms/step - loss: 0.6352 - val_loss: 0.6281\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 12s 215ms/step - loss: 0.6087 - val_loss: 0.6435\n",
            "14/14 [==============================] - 1s 67ms/step - loss: 0.6281\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "54/54 [==============================] - ETA: 0s - loss: 0.6785WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "54/54 [==============================] - 29s 283ms/step - loss: 0.6785 - val_loss: 0.6734\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 12s 216ms/step - loss: 0.6096 - val_loss: 0.6875\n",
            "14/14 [==============================] - 1s 72ms/step - loss: 0.6734\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "54/54 [==============================] - ETA: 0s - loss: 0.6794WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "54/54 [==============================] - 28s 264ms/step - loss: 0.6794 - val_loss: 0.6666\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 12s 213ms/step - loss: 0.6132 - val_loss: 0.7150\n",
            "14/14 [==============================] - 1s 65ms/step - loss: 0.6666\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "54/54 [==============================] - ETA: 0s - loss: 0.6838WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "54/54 [==============================] - 31s 272ms/step - loss: 0.6838 - val_loss: 0.6773\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 12s 221ms/step - loss: 0.6565 - val_loss: 0.6848\n",
            "14/14 [==============================] - 1s 70ms/step - loss: 0.6773\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.6597838640213013\n",
            "5-Fold CV Accuracy: 0.5988715334509727\n",
            "5-Fold CV Precision: 0.6420553997644973\n",
            "5-Fold CV Recall: 0.6073614653373521\n",
            "5-Fold CV F1 Score: 0.6005250209112576\n",
            "5-Fold CV Micro F1 Score: 0.5988715334509727\n",
            "5-Fold CV Weighted Macro F1 Score: 0.584707694510526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Jo9qoJsn8y"
      },
      "source": [
        "#### Half"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ImY4sAQsn8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154348fa-7139-43e0-a0f8-4f1757c066c6"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_half, model_name='roberta', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "108/108 [==============================] - ETA: 0s - loss: 0.6746WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "108/108 [==============================] - 45s 291ms/step - loss: 0.6746 - val_loss: 0.6335\n",
            "Epoch 2/10\n",
            "108/108 [==============================] - 29s 269ms/step - loss: 0.6191 - val_loss: 0.6041\n",
            "Epoch 3/10\n",
            "108/108 [==============================] - 29s 268ms/step - loss: 0.5363 - val_loss: 0.6383\n",
            "27/27 [==============================] - 2s 75ms/step - loss: 0.6041\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "108/108 [==============================] - ETA: 0s - loss: 0.6892WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "108/108 [==============================] - 47s 299ms/step - loss: 0.6892 - val_loss: 0.6967\n",
            "Epoch 2/10\n",
            "108/108 [==============================] - 29s 267ms/step - loss: 0.6765 - val_loss: 0.7117\n",
            "27/27 [==============================] - 2s 74ms/step - loss: 0.6967\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "108/108 [==============================] - ETA: 0s - loss: 0.6755WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "108/108 [==============================] - 47s 291ms/step - loss: 0.6755 - val_loss: 0.6550\n",
            "Epoch 2/10\n",
            "108/108 [==============================] - 29s 268ms/step - loss: 0.6182 - val_loss: 0.6682\n",
            "27/27 [==============================] - 2s 73ms/step - loss: 0.6550\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "108/108 [==============================] - ETA: 0s - loss: 0.6716WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "108/108 [==============================] - 44s 257ms/step - loss: 0.6716 - val_loss: 0.6343\n",
            "Epoch 2/10\n",
            "108/108 [==============================] - 24s 219ms/step - loss: 0.6236 - val_loss: 0.6427\n",
            "27/27 [==============================] - 2s 92ms/step - loss: 0.6343\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "108/108 [==============================] - ETA: 0s - loss: 0.6915WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "108/108 [==============================] - 45s 289ms/step - loss: 0.6915 - val_loss: 0.6901\n",
            "Epoch 2/10\n",
            "108/108 [==============================] - 29s 266ms/step - loss: 0.6823 - val_loss: 0.6578\n",
            "Epoch 3/10\n",
            "108/108 [==============================] - 29s 269ms/step - loss: 0.6706 - val_loss: 0.7050\n",
            "27/27 [==============================] - 2s 67ms/step - loss: 0.6578\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.6495730161666871\n",
            "5-Fold CV Accuracy: 0.5979570410142172\n",
            "5-Fold CV Precision: 0.7252492885126044\n",
            "5-Fold CV Recall: 0.4884249417957284\n",
            "5-Fold CV F1 Score: 0.5012193796715526\n",
            "5-Fold CV Micro F1 Score: 0.5979570410142172\n",
            "5-Fold CV Weighted Macro F1 Score: 0.5608214421281478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zsyi_oT1NHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713764a7-0592-4589-da7e-c5ad00fa2955"
      },
      "source": [
        "df_half['Label'].value_counts()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    2224\n",
              "0    2064\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VRBHDkO1QBv"
      },
      "source": [
        "### Pre-training on BABE, Running on AdFontes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UudRRNnW1Edv"
      },
      "source": [
        "def preprocess(df):\n",
        "    \"\"\"convert a pandas dataframe into a tensorflow dataset\"\"\"\n",
        "    target = df.pop('Label')\n",
        "    sentence = df.pop('text')\n",
        "\n",
        "    #tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') #uncased\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    #tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "                        sentence.tolist(),                      \n",
        "                        add_special_tokens = True, # add [CLS], [SEP]\n",
        "                        truncation = True, # cut off at max length of the text that can go to BERT\n",
        "                        padding = True, # add [PAD] tokens\n",
        "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "              )\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (dict(train_encodings), \n",
        "         target.tolist()))\n",
        "    return dataset"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSxRj8J21Ed2"
      },
      "source": [
        "# pandas -> tensorflow\n",
        "# train-test split\n",
        "df_train, df_test = train_test_split(babe, test_size = 0.10, random_state = 42)\n",
        "\n",
        "\n",
        "train_distant_dataset = preprocess(df_train)\n",
        "test_distant_dataset = preprocess(df_test)\n",
        "\n",
        "# batch and randomize\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_distant_dataset = train_distant_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_distant_dataset = test_distant_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBlT0k161Ed3"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK3Abs5U1Ed4",
        "outputId": "4c1effff-e11e-497c-d600-00589eb3827c"
      },
      "source": [
        "# train entire model with distant signals\n",
        "#bert = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\") #DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output\n",
        "bert = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "##roberta = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True) # after 3 epochs without improvement, stop training\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "bert.compile(optimizer=optimizer, loss=bert.compute_loss)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQVMDo6J1Ed5",
        "outputId": "a493b31a-d5af-40ce-fea3-849cf9ca8cc9"
      },
      "source": [
        "history_bert = bert.fit(train_distant_dataset, epochs=1, validation_data = test_distant_dataset, callbacks=[callback])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5300WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "104/104 [==============================] - 66s 485ms/step - loss: 0.5300 - val_loss: 0.4389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co97RhZs1Ed5"
      },
      "source": [
        "trained_layer = bert.get_layer(index=0).get_weights()\n",
        "\n",
        "bert.save_weights('./checkpoints/bert_final_checkpoint_news_headlines_USA')\n",
        "\n",
        "#bert.load_weights('./checkpoints/final_checkpoint_distant_learning')"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WhFARsg1Ed6",
        "outputId": "2ea2c538-e051-4874-810b-396fc2c0ec7a"
      },
      "source": [
        "transfer_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "transfer_model.compile(optimizer=optimizer, loss=transfer_model.compute_loss) \n",
        "\n",
        "transfer_model.load_weights('./checkpoints/bert_final_checkpoint_news_headlines_USA')\n",
        "trained_model_layer = transfer_model.get_layer(index=0).get_weights()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ56bOGZ1Ed6"
      },
      "source": [
        "#### AdFontes Pre-Trained on AllSides"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i-WnFH-1Ed7",
        "outputId": "82a7178b-c45a-4de5-dd93-1cbc8ad67ae4"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(af_headlines, model_name='bert', \n",
        "                                                                                            freeze_encoder=False, pretrained=True)\n",
        "\n",
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Start fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5463WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 51s 306ms/step - loss: 0.5463 - val_loss: 0.5006\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 283ms/step - loss: 0.3389 - val_loss: 0.5246\n",
            "28/28 [==============================] - 2s 81ms/step - loss: 0.5006\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5725WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 48s 312ms/step - loss: 0.5725 - val_loss: 0.4927\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 32s 290ms/step - loss: 0.3642 - val_loss: 0.4905\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 31s 284ms/step - loss: 0.1605 - val_loss: 0.6699\n",
            "28/28 [==============================] - 2s 87ms/step - loss: 0.4905\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5741WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 47s 308ms/step - loss: 0.5741 - val_loss: 0.4934\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 286ms/step - loss: 0.3564 - val_loss: 0.5730\n",
            "28/28 [==============================] - 3s 91ms/step - loss: 0.4934\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5777WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 52s 308ms/step - loss: 0.5777 - val_loss: 0.4967\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 31s 283ms/step - loss: 0.3736 - val_loss: 0.4878\n",
            "Epoch 3/10\n",
            "110/110 [==============================] - 31s 283ms/step - loss: 0.1775 - val_loss: 0.7149\n",
            "28/28 [==============================] - 3s 96ms/step - loss: 0.4878\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "### Start fold 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.5454WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "110/110 [==============================] - 49s 311ms/step - loss: 0.5454 - val_loss: 0.5161\n",
            "Epoch 2/10\n",
            "110/110 [==============================] - 32s 291ms/step - loss: 0.3113 - val_loss: 0.5580\n",
            "28/28 [==============================] - 3s 96ms/step - loss: 0.5161\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "5-Fold CV Loss: 0.49767595529556274\n",
            "5-Fold CV Accuracy: 0.76299625643669\n",
            "5-Fold CV Precision: 0.7352403614430771\n",
            "5-Fold CV Recall: 0.7058594024111267\n",
            "5-Fold CV F1 Score: 0.7190110425146969\n",
            "5-Fold CV Micro F1 Score: 0.76299625643669\n",
            "5-Fold CV Weighted Macro F1 Score: 0.7620981537110633\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}