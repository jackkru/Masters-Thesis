{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp_spacy_core_web_lg = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Jack\\\\Documents_\\\\Thesis_2\\\\Features'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Jack\\\\Documents_\\\\Thesis_2\\\\Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DatasetFull.csv\", encoding =\"latin-1\") #my full allsides data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdt = pd.read_csv(\"final_labels_SG2.xlsx - Sheet1.csv\") #manu, david, timo BABE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv(\"AllSides AdFontes Merged.csv\", encoding = \"latin-1\") # AllSides data with AdFontes ratings included\n",
    "df_merged_extreme = pd.read_csv(\"AllAd Merged Extreme.csv\", encoding = \"latin-1\") # AllSides data with AdFontes rating included, prerated based on extreme labels (AdFont_Bias = 1 if AdFontes rating is <-15 or >15, 0 if AdFontes rating is between -8 and 8)\n",
    "df_ad_art = pd.read_csv(\"AdFontes Article Classifications.csv\", encoding = \"latin-1\") # AdFontes specific article ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Subject_Tag</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wall Street Journal (News)</td>\n",
       "      <td>Vaccines Appear to Be Slowing Spread of Covid-...</td>\n",
       "      <td>Vaccines appear to be starting to curb new Cov...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington Examiner</td>\n",
       "      <td>Pandemic retreat signals vaccines are working</td>\n",
       "      <td>COVID-19 cases and hospitalizations in the Uni...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Epoch Times</td>\n",
       "      <td>NYT, Washington Post, NBC Retract Incorrect Re...</td>\n",
       "      <td>The New York Times, The Washington Post, and N...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>New York Times, WaPo, NBC retract reports abou...</td>\n",
       "      <td>The New York Times, The Washington Post and NB...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Source  \\\n",
       "0    U.S. News & World Report   \n",
       "1  Wall Street Journal (News)   \n",
       "2         Washington Examiner   \n",
       "3             The Epoch Times   \n",
       "4                    The Hill   \n",
       "\n",
       "                                            Headline  \\\n",
       "0           COVID Deaths Continue to Decline in U.S.   \n",
       "1  Vaccines Appear to Be Slowing Spread of Covid-...   \n",
       "2      Pandemic retreat signals vaccines are working   \n",
       "3  NYT, Washington Post, NBC Retract Incorrect Re...   \n",
       "4  New York Times, WaPo, NBC retract reports abou...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  In a sign that the coronavirus pandemic is beg...   \n",
       "1  Vaccines appear to be starting to curb new Cov...   \n",
       "2  COVID-19 cases and hospitalizations in the Uni...   \n",
       "3  The New York Times, The Washington Post, and N...   \n",
       "4  The New York Times, The Washington Post and NB...   \n",
       "\n",
       "                                     Bias  \\\n",
       "0   AllSides Media Bias Rating: Lean Left   \n",
       "1      AllSides Media Bias Rating: Center   \n",
       "2  AllSides Media Bias Rating: Lean Right   \n",
       "3  AllSides Media Bias Rating: Lean Right   \n",
       "4      AllSides Media Bias Rating: Center   \n",
       "\n",
       "                                         Subject_Tag           Date  \n",
       "0  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021  \n",
       "1  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021  \n",
       "2  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021  \n",
       "3  Media Industry, Media Bias, New York Times, Wa...  May 2nd, 2021  \n",
       "4  Media Industry, Media Bias, New York Times, Wa...  May 2nd, 2021  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>news_link</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>label_bias</th>\n",
       "      <th>label_opinion</th>\n",
       "      <th>biased_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Orange Is the New Black\" star Yael Stone is r...</td>\n",
       "      <td>https://www.foxnews.com/entertainment/australi...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>environment</td>\n",
       "      <td>right</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>Entirely factual</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"We have one beautiful law,\" Trump recently sa...</td>\n",
       "      <td>https://www.alternet.org/2020/06/law-and-order...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>gun control</td>\n",
       "      <td>left</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>['bizarre', 'characteristically']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...immigrants as criminals and eugenics, all o...</td>\n",
       "      <td>https://www.nbcnews.com/news/latino/after-step...</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>left</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Expresses writer’s opinion</td>\n",
       "      <td>['criminals', 'fringe', 'extreme']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...we sounded the alarm in the early months of...</td>\n",
       "      <td>https://www.alternet.org/2019/07/fox-news-has-...</td>\n",
       "      <td>Alternet</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>left</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Black Lives Matter] is essentially a non-fals...</td>\n",
       "      <td>http://feedproxy.google.com/~r/breitbart/~3/-v...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>marriage-equality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Expresses writer’s opinion</td>\n",
       "      <td>['cult']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  \"Orange Is the New Black\" star Yael Stone is r...   \n",
       "1  \"We have one beautiful law,\" Trump recently sa...   \n",
       "2  ...immigrants as criminals and eugenics, all o...   \n",
       "3  ...we sounded the alarm in the early months of...   \n",
       "4  [Black Lives Matter] is essentially a non-fals...   \n",
       "\n",
       "                                           news_link     outlet  \\\n",
       "0  https://www.foxnews.com/entertainment/australi...   Fox News   \n",
       "1  https://www.alternet.org/2020/06/law-and-order...   Alternet   \n",
       "2  https://www.nbcnews.com/news/latino/after-step...      MSNBC   \n",
       "3  https://www.alternet.org/2019/07/fox-news-has-...   Alternet   \n",
       "4  http://feedproxy.google.com/~r/breitbart/~3/-v...  Breitbart   \n",
       "\n",
       "               topic   type  label_bias  \\\n",
       "0        environment  right  Non-biased   \n",
       "1        gun control   left      Biased   \n",
       "2  white-nationalism   left      Biased   \n",
       "3  white-nationalism   left      Biased   \n",
       "4  marriage-equality    NaN      Biased   \n",
       "\n",
       "                           label_opinion                        biased_words  \n",
       "0                       Entirely factual                                  []  \n",
       "1  Somewhat factual but also opinionated   ['bizarre', 'characteristically']  \n",
       "2             Expresses writer’s opinion  ['criminals', 'fringe', 'extreme']  \n",
       "3  Somewhat factual but also opinionated                                  []  \n",
       "4             Expresses writer’s opinion                            ['cult']  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mdt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bias.x</th>\n",
       "      <th>Subject_Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>AdFont_Bias</th>\n",
       "      <th>AdFont_Reli</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washington Examiner</td>\n",
       "      <td>Pandemic retreat signals vaccines are working</td>\n",
       "      <td>COVID-19 cases and hospitalizations in the Uni...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>15.24</td>\n",
       "      <td>32.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>New York Times, WaPo, NBC retract reports abou...</td>\n",
       "      <td>The New York Times, The Washington Post and NB...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>45.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Trumps border wall hits a wall as Pentagon ca...</td>\n",
       "      <td>The US Department of Defense said on Friday it...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Immigration, Border Wall, Pentagon, US Militar...</td>\n",
       "      <td>May 1st, 2021</td>\n",
       "      <td>-9.91</td>\n",
       "      <td>44.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>Biden cancels military-funded border wall proj...</td>\n",
       "      <td>President Biden is canceling projects to build...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Immigration, Border Wall, Pentagon, US Militar...</td>\n",
       "      <td>May 1st, 2021</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>45.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>National Review</td>\n",
       "      <td>Bidens Con against America</td>\n",
       "      <td>OPINION\\r\\nOne-hundred days in, Bidens radica...</td>\n",
       "      <td>AllSides Media Bias Rating: Right</td>\n",
       "      <td>Bridging Divides, Joe Biden, Polarization, Whi...</td>\n",
       "      <td>April 30th, 2021</td>\n",
       "      <td>17.04</td>\n",
       "      <td>31.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Source                                           Headline  \\\n",
       "0  Washington Examiner      Pandemic retreat signals vaccines are working   \n",
       "1             The Hill  New York Times, WaPo, NBC retract reports abou...   \n",
       "2         The Guardian  Trumps border wall hits a wall as Pentagon ca...   \n",
       "3             The Hill  Biden cancels military-funded border wall proj...   \n",
       "4      National Review                        Bidens Con against America   \n",
       "\n",
       "                                                Text  \\\n",
       "0  COVID-19 cases and hospitalizations in the Uni...   \n",
       "1  The New York Times, The Washington Post and NB...   \n",
       "2  The US Department of Defense said on Friday it...   \n",
       "3  President Biden is canceling projects to build...   \n",
       "4  OPINION\\r\\nOne-hundred days in, Bidens radica...   \n",
       "\n",
       "                                   Bias.x  \\\n",
       "0  AllSides Media Bias Rating: Lean Right   \n",
       "1      AllSides Media Bias Rating: Center   \n",
       "2   AllSides Media Bias Rating: Lean Left   \n",
       "3      AllSides Media Bias Rating: Center   \n",
       "4       AllSides Media Bias Rating: Right   \n",
       "\n",
       "                                         Subject_Tag              Date  \\\n",
       "0  Coronavirus, Coronavirus Vaccine, Coronavirus ...     May 3rd, 2021   \n",
       "1  Media Industry, Media Bias, New York Times, Wa...     May 2nd, 2021   \n",
       "2  Immigration, Border Wall, Pentagon, US Militar...     May 1st, 2021   \n",
       "3  Immigration, Border Wall, Pentagon, US Militar...     May 1st, 2021   \n",
       "4  Bridging Divides, Joe Biden, Polarization, Whi...  April 30th, 2021   \n",
       "\n",
       "   AdFont_Bias  AdFont_Reli  \n",
       "0        15.24        32.01  \n",
       "1        -0.26        45.10  \n",
       "2        -9.91        44.54  \n",
       "3        -0.26        45.10  \n",
       "4        17.04        31.86  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bias.x</th>\n",
       "      <th>Subject_Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>AdFont_Bias</th>\n",
       "      <th>AdFont_Reli</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washington Examiner</td>\n",
       "      <td>Pandemic retreat signals vaccines are working</td>\n",
       "      <td>COVID-19 cases and hospitalizations in the Uni...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>1</td>\n",
       "      <td>32.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>New York Times, WaPo, NBC retract reports abou...</td>\n",
       "      <td>The New York Times, The Washington Post and NB...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>45.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>Biden cancels military-funded border wall proj...</td>\n",
       "      <td>President Biden is canceling projects to build...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Immigration, Border Wall, Pentagon, US Militar...</td>\n",
       "      <td>May 1st, 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>45.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>National Review</td>\n",
       "      <td>Bidens Con against America</td>\n",
       "      <td>OPINION\\r\\nOne-hundred days in, Bidens radica...</td>\n",
       "      <td>AllSides Media Bias Rating: Right</td>\n",
       "      <td>Bridging Divides, Joe Biden, Polarization, Whi...</td>\n",
       "      <td>April 30th, 2021</td>\n",
       "      <td>1</td>\n",
       "      <td>31.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>Mark Kelly: I didn't hear plan for border in B...</td>\n",
       "      <td>Democratic Sen. Mark Kelly (Ariz.) criticized ...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Immigration, Border Crisis, Joe Biden</td>\n",
       "      <td>April 30th, 2021</td>\n",
       "      <td>0</td>\n",
       "      <td>45.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Source                                           Headline  \\\n",
       "0  Washington Examiner      Pandemic retreat signals vaccines are working   \n",
       "1             The Hill  New York Times, WaPo, NBC retract reports abou...   \n",
       "2             The Hill  Biden cancels military-funded border wall proj...   \n",
       "3      National Review                        Bidens Con against America   \n",
       "4             The Hill  Mark Kelly: I didn't hear plan for border in B...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  COVID-19 cases and hospitalizations in the Uni...   \n",
       "1  The New York Times, The Washington Post and NB...   \n",
       "2  President Biden is canceling projects to build...   \n",
       "3  OPINION\\r\\nOne-hundred days in, Bidens radica...   \n",
       "4  Democratic Sen. Mark Kelly (Ariz.) criticized ...   \n",
       "\n",
       "                                   Bias.x  \\\n",
       "0  AllSides Media Bias Rating: Lean Right   \n",
       "1      AllSides Media Bias Rating: Center   \n",
       "2      AllSides Media Bias Rating: Center   \n",
       "3       AllSides Media Bias Rating: Right   \n",
       "4      AllSides Media Bias Rating: Center   \n",
       "\n",
       "                                         Subject_Tag              Date  \\\n",
       "0  Coronavirus, Coronavirus Vaccine, Coronavirus ...     May 3rd, 2021   \n",
       "1  Media Industry, Media Bias, New York Times, Wa...     May 2nd, 2021   \n",
       "2  Immigration, Border Wall, Pentagon, US Militar...     May 1st, 2021   \n",
       "3  Bridging Divides, Joe Biden, Polarization, Whi...  April 30th, 2021   \n",
       "4              Immigration, Border Crisis, Joe Biden  April 30th, 2021   \n",
       "\n",
       "   AdFont_Bias  AdFont_Reli  \n",
       "0            1        32.01  \n",
       "1            0        45.10  \n",
       "2            0        45.10  \n",
       "3            1        31.86  \n",
       "4            0        45.10  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_extreme.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Reliability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exclusive: Study finds pregnancy-related healt...</td>\n",
       "      <td>-6.33</td>\n",
       "      <td>45.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bidens first 100 days: How diverse is the adm...</td>\n",
       "      <td>-9.67</td>\n",
       "      <td>43.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How COVID-19 has endangered gender equity worl...</td>\n",
       "      <td>-7.00</td>\n",
       "      <td>44.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inside the Lincoln Projects toxic workplace</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>57.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pregnant health care workers could get Pfizer ...</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>44.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Bias  Reliability\n",
       "0  Exclusive: Study finds pregnancy-related healt... -6.33        45.67\n",
       "1  Bidens first 100 days: How diverse is the adm... -9.67        43.67\n",
       "2  How COVID-19 has endangered gender equity worl... -7.00        44.33\n",
       "3     Inside the Lincoln Projects toxic workplace -0.33        57.00\n",
       "4  Pregnant health care workers could get Pfizer ... -0.67        44.67"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ad_art.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"spacy_lg\"] = df[\"Headline\"].apply(lambda x: nlp_spacy_core_web_lg(x)) # applying spacy web tokenizer to headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"spacy_lg_dict\"] = None # to be filled in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Subject_Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>spacy_lg</th>\n",
       "      <th>spacy_lg_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wall Street Journal (News)</td>\n",
       "      <td>Vaccines Appear to Be Slowing Spread of Covid-...</td>\n",
       "      <td>Vaccines appear to be starting to curb new Cov...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(Vaccines, Appear, to, Be, Slowing, Spread, of...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington Examiner</td>\n",
       "      <td>Pandemic retreat signals vaccines are working</td>\n",
       "      <td>COVID-19 cases and hospitalizations in the Uni...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(Pandemic, retreat, signals, vaccines, are, wo...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Epoch Times</td>\n",
       "      <td>NYT, Washington Post, NBC Retract Incorrect Re...</td>\n",
       "      <td>The New York Times, The Washington Post, and N...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "      <td>(NYT, ,, Washington, Post, ,, NBC, Retract, In...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>New York Times, WaPo, NBC retract reports abou...</td>\n",
       "      <td>The New York Times, The Washington Post and NB...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "      <td>(New, York, Times, ,, WaPo, ,, NBC, retract, r...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Source  \\\n",
       "0    U.S. News & World Report   \n",
       "1  Wall Street Journal (News)   \n",
       "2         Washington Examiner   \n",
       "3             The Epoch Times   \n",
       "4                    The Hill   \n",
       "\n",
       "                                            Headline  \\\n",
       "0           COVID Deaths Continue to Decline in U.S.   \n",
       "1  Vaccines Appear to Be Slowing Spread of Covid-...   \n",
       "2      Pandemic retreat signals vaccines are working   \n",
       "3  NYT, Washington Post, NBC Retract Incorrect Re...   \n",
       "4  New York Times, WaPo, NBC retract reports abou...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  In a sign that the coronavirus pandemic is beg...   \n",
       "1  Vaccines appear to be starting to curb new Cov...   \n",
       "2  COVID-19 cases and hospitalizations in the Uni...   \n",
       "3  The New York Times, The Washington Post, and N...   \n",
       "4  The New York Times, The Washington Post and NB...   \n",
       "\n",
       "                                     Bias  \\\n",
       "0   AllSides Media Bias Rating: Lean Left   \n",
       "1      AllSides Media Bias Rating: Center   \n",
       "2  AllSides Media Bias Rating: Lean Right   \n",
       "3  AllSides Media Bias Rating: Lean Right   \n",
       "4      AllSides Media Bias Rating: Center   \n",
       "\n",
       "                                         Subject_Tag           Date  \\\n",
       "0  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "1  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "2  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "3  Media Industry, Media Bias, New York Times, Wa...  May 2nd, 2021   \n",
       "4  Media Industry, Media Bias, New York Times, Wa...  May 2nd, 2021   \n",
       "\n",
       "                                            spacy_lg spacy_lg_dict  \n",
       "0   (COVID, Deaths, Continue, to, Decline, in, U.S.)          None  \n",
       "1  (Vaccines, Appear, to, Be, Slowing, Spread, of...          None  \n",
       "2  (Pandemic, retreat, signals, vaccines, are, wo...          None  \n",
       "3  (NYT, ,, Washington, Post, ,, NBC, Retract, In...          None  \n",
       "4  (New, York, Times, ,, WaPo, ,, NBC, retract, r...          None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Covid-19'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"spacy_lg\"][1][-2].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"spacy_lg\"][1][1].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vaccines'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"spacy_lg\"][1][0].lemma_.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Jack\\\\Documents_\\\\Thesis_2\\\\Datasets\")\n",
    "assertives = list(pd.read_excel(\"Assertives.xlsx\", header = None)[0]) # assertives data, courtesy of ___\n",
    "factives = list(pd.read_excel(\"Factives.xlsx\", header = None)[0])\n",
    "hedges = list(pd.read_excel(\"Hedges.xlsx\", header = None)[0])\n",
    "implicatives = list(pd.read_excel(\"Implicatives.xlsx\", header = None)[0])\n",
    "\n",
    "negative = list(pd.read_excel(\"Negative Opinion Lexicon.xlsx\", header = None)[0])\n",
    "positive = list(pd.read_excel(\"Positive Opinion Lexicon.xlsx\", header = None)[0])\n",
    "bias_lex_recasens = list(pd.read_excel(\"Recasens Bias Lexicon 2012.xlsx\", header = None)[0])\n",
    "bias_lex_hube = list(pd.read_excel(\"HubeFetahu Bias Lexicon 2018.xlsx\", header = None)[0])\n",
    "report_verbs = list(pd.read_excel(\"Recasens Report Verbs.xlsx\", header = None)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 COVID Deaths Continue to Decline in U.S.\n",
       "1        Vaccines Appear to Be Slowing Spread of Covid-...\n",
       "2            Pandemic retreat signals vaccines are working\n",
       "3        NYT, Washington Post, NBC Retract Incorrect Re...\n",
       "4        New York Times, WaPo, NBC retract reports abou...\n",
       "                               ...                        \n",
       "16579    Murdoch: 'I have no idea' why Romney isn't cha...\n",
       "16580    Illegal immigrants line up at consulates acros...\n",
       "16581                       Obama to talk Medicare in Iowa\n",
       "16582    Romney campaign tries to take Medicare away fr...\n",
       "16583    Smoking Gun: Obama Admits He Cut Billions from...\n",
       "Name: Headline, Length: 16584, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Headline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "vec = TfidfVectorizer(token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "vec.fit(df[\"Headline\"])\n",
    "\n",
    "tf_idf = pd.DataFrame(vec.transform(df[\"Headline\"]).toarray(), columns = sorted(vec.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc = pd.read_excel(\"LIWC 2015.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_codes = pd.read_excel(\"LIWC 2015.xlsx\", sheet_name = \"Sheet2\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30.0, 31.0, 120.0, 122.0, nan, nan, nan, nan, nan, nan]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(liwc['(:'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cut'] = None\n",
    "for index, row in df.iterrows():\n",
    "    length = 0\n",
    "    for token in df['spacy_lg'][index]:\n",
    "        if token.pos_ not in ['PUNCT', 'SPACE', 'SYM', 'NUM']:\n",
    "            length += 1\n",
    "    if length < 5:\n",
    "        df[\"Cut\"][index] = 0\n",
    "    else:\n",
    "        df[\"Cut\"][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Cut\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['text', 'text_low', 'pos', 'lemma', 'lemma_low', 'tag', 'dep', 'shape', 'is_alpha', 'is_stop', 'has_vec', 'glove_vec300', \n",
    "         'glove_vec300_norm', 'is_oov','morph', 'order', 'tf_idf', 'is_ne', 'ne_label'\n",
    "         , 'negative', 'positive', 'bias_lex_r', 'bias_lex_h', 'liwc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_C1 = [a + b for a, b in zip(feats, len(feats)*['_c1'])]\n",
    "feats_C2 = [a + b for a, b in zip(feats, len(feats)*['_c2'])]\n",
    "feats_C3 = [a + b for a, b in zip(feats, len(feats)*['_c3'])]\n",
    "feats_C4 = [a + b for a, b in zip(feats, len(feats)*['_c4'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sent_tokens = []\n",
    "    entities = [] # create list of named entities\n",
    "    for ent in df[\"spacy_lg\"][index].ents:\n",
    "        ent_dict = {'text': ent.text,\n",
    "                   'label': ent.label_}\n",
    "        entities.append(ent_dict)\n",
    "    \n",
    "    order = 0\n",
    "    for token in df['spacy_lg'][index]:\n",
    "        if token.pos_ not in ['PUNCT', 'SPACE', 'SYM', 'CCONJ', 'PART', 'NUM']:\n",
    "            order += 1    \n",
    "            if token.text.lower() in list(tf_idf):\n",
    "                tfidf_temp = tf_idf[token.text.lower()][index]\n",
    "            else:\n",
    "                tfidf_temp = None \n",
    "            token_dict = {'text':token.text,\n",
    "                    'text_low': token.text.lower(),\n",
    "                    'pos': token.pos_,\n",
    "                    'lemma': token.lemma_,\n",
    "                    'lemma_low': token.lemma_.lower(),\n",
    "                    'tag': token.tag_,\n",
    "                    'dep': token.dep_,\n",
    "                    'shape': token.shape_,\n",
    "                    'is_alpha': token.is_alpha,\n",
    "                    'is_stop': token.is_stop,\n",
    "                    'has_vec':token.has_vector,\n",
    "                    'glove_vec300':token.vector,\n",
    "                    'glove_vec300_norm':token.vector_norm,\n",
    "                    'is_oov':token.is_oov,\n",
    "                    'morph': token.morph,\n",
    "                    'order': (order - 1),\n",
    "                    'tf_idf': tfidf_temp}\n",
    "            for ent in entities:\n",
    "                if token.text in ent['text']:\n",
    "                    token_dict['is_ne']=True\n",
    "                    token_dict['ne_label']=ent['label']\n",
    "            if 'is_ne' not in token_dict:\n",
    "                token_dict['is_ne']=False\n",
    "                token_dict['ne_label']=None\n",
    "            \n",
    "            # LIWC2015 FEATURES\n",
    "            if token.text.lower() in list(liwc):\n",
    "                token_dict['liwc'] = list(liwc[token.text.lower()])\n",
    "            elif token.lemma_ in list(liwc):\n",
    "                token_dict['liwc'] = list(liwc[token.lemma_])\n",
    "            else:\n",
    "                token_dict['liwc'] = []\n",
    "            \n",
    "            # negative opinion\n",
    "            if token.text.lower() in negative:\n",
    "                token_dict['negative'] = 1\n",
    "            elif token.lemma_.lower() in negative:\n",
    "                token_dict['negative'] = 1\n",
    "            else:\n",
    "                token_dict['negative'] = 0\n",
    "                \n",
    "            # positive opinion\n",
    "            if token.text.lower() in positive:\n",
    "                token_dict['positive'] = 1\n",
    "            elif token.lemma_.lower() in positive:\n",
    "                token_dict['positive'] = 1\n",
    "            else:\n",
    "                token_dict['positive'] = 0\n",
    "            \n",
    "           # bias lexicon hube\n",
    "            if token.text.lower() in bias_lex_hube:\n",
    "                token_dict['bias_lex_h'] = 1\n",
    "            elif token.lemma_.lower() in bias_lex_hube:\n",
    "                token_dict['bias_lex_h'] = 1\n",
    "            else:\n",
    "                token_dict['bias_lex_h'] = 0\n",
    "                \n",
    "            # bias lexicon recasens\n",
    "            if token.text.lower() in bias_lex_recasens:\n",
    "                token_dict['bias_lex_r'] = 1\n",
    "            elif token.lemma_.lower() in bias_lex_recasens:\n",
    "                token_dict['bias_lex_r'] = 1\n",
    "            else:\n",
    "                token_dict['bias_lex_r'] = 0\n",
    "                \n",
    "            sent_tokens.append(token_dict)\n",
    "            \n",
    "    for token in sent_tokens:\n",
    "        # the 1st word in the sentence\n",
    "        if token['order'] == 0:\n",
    "            for feat in feats_C1: # context word -2\n",
    "                token[feat]=None\n",
    "            for feat in feats_C2: # context word -1\n",
    "                token[feat]=None\n",
    "            for i, feat in enumerate(feats_C3): # context word +1\n",
    "                token[feat]=sent_tokens[token['order']+1][feats[i]]\n",
    "            for i, feat in enumerate(feats_C4): # context word +2\n",
    "                token[feat]=sent_tokens[token['order']+2][feats[i]]\n",
    "        # the 2nd word in the sentence\n",
    "        elif token['order'] == 1:\n",
    "            for feat in feats_C1: # context word -2\n",
    "                token[feat]=None\n",
    "            for i, feat in enumerate(feats_C2): # context word -1\n",
    "                token[feat]=sent_tokens[token['order']-1][feats[i]]\n",
    "            for i, feat in enumerate(feats_C3): # context word +1\n",
    "                token[feat]=sent_tokens[token['order']+1][feats[i]]\n",
    "            for i, feat in enumerate(feats_C4): # context word +2\n",
    "                token[feat]=sent_tokens[token['order']+2][feats[i]]\n",
    "        # the pre-last word in the sentence\n",
    "        elif token['order'] == (len(sent_tokens)-2):\n",
    "            for i, feat in enumerate(feats_C1): # context word -2\n",
    "                token[feat]=sent_tokens[token['order']-2][feats[i]]\n",
    "            for i, feat in enumerate(feats_C2): # context word -1\n",
    "                token[feat]=sent_tokens[token['order']-1][feats[i]]\n",
    "            for i, feat in enumerate(feats_C3): # context word +1\n",
    "                token[feat]=sent_tokens[token['order']+1][feats[i]]\n",
    "            for i, feat in enumerate(feats_C4): # context word +2\n",
    "                token[feat]=None\n",
    "        # the last word in the sentence\n",
    "        elif token['order'] == (len(sent_tokens)-1):\n",
    "            for i, feat in enumerate(feats_C1): # context word -2\n",
    "                token[feat]=sent_tokens[token['order']-2][feats[i]]\n",
    "            for i, feat in enumerate(feats_C2): # context word -1\n",
    "                token[feat]=sent_tokens[token['order']-1][feats[i]]\n",
    "            for i, feat in enumerate(feats_C3): # context word +1\n",
    "                token[feat]=None\n",
    "            for i, feat in enumerate(feats_C4): # context word +2\n",
    "                token[feat]=None\n",
    "        # in other cases:\n",
    "        else:\n",
    "            for i, feat in enumerate(feats_C1): # context word -2\n",
    "                token[feat]=sent_tokens[token['order']-2][feats[i]]\n",
    "            for i, feat in enumerate(feats_C2): # context word -1\n",
    "                token[feat]=sent_tokens[token['order']-1][feats[i]]\n",
    "            for i, feat in enumerate(feats_C3): # context word +1\n",
    "                token[feat]=sent_tokens[token['order']+1][feats[i]]\n",
    "            for i, feat in enumerate(feats_C4): # context word +2\n",
    "                token[feat]=sent_tokens[token['order']+2][feats[i]]            \n",
    "        \n",
    "    \n",
    "    # update column spacy_lg_dict\n",
    "    df.at[index,'spacy_lg_dict'] = sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add windows of -4 and +4 words and lemmas\n",
    "for index, row in df.iterrows():\n",
    "   \n",
    "    if len(row.spacy_lg_dict) >= 8:\n",
    "        for i, token in enumerate(row.spacy_lg_dict):\n",
    "            \n",
    "            if i == 0:\n",
    "              \n",
    "                token['window_text'] = [None, None, None, None, token['text_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['text_low'],row.spacy_lg_dict[i+2]['text_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['text_low'],row.spacy_lg_dict[i+4]['text_low']]\n",
    "                token['window_lemma'] = [None, None, None, None, token['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['lemma_low'],row.spacy_lg_dict[i+2]['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['lemma_low'],row.spacy_lg_dict[i+4]['lemma_low']]\n",
    "            elif i == 1:\n",
    "                \n",
    "                token['window_text'] = [None, None, None, row.spacy_lg_dict[i-1]['text_low'], token['text_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['text_low'],row.spacy_lg_dict[i+2]['text_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['text_low'],row.spacy_lg_dict[i+4]['text_low']]\n",
    "                token['window_lemma'] = [None, None, None, row.spacy_lg_dict[i-1]['lemma_low'], token['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['lemma_low'],row.spacy_lg_dict[i+2]['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['lemma_low'],row.spacy_lg_dict[i+4]['lemma_low']]\n",
    "            elif i == 2:\n",
    "               \n",
    "                token['window_text'] = [None, None, row.spacy_lg_dict[i-2]['text_low'],\n",
    "                                        row.spacy_lg_dict[i-1]['text_low'],token['text_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['text_low'],row.spacy_lg_dict[i+2]['text_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['text_low'],row.spacy_lg_dict[i+4]['text_low']]\n",
    "                token['window_lemma'] = [None, None, row.spacy_lg_dict[i-2]['lemma_low'],\n",
    "                                         row.spacy_lg_dict[i-1]['lemma_low'],token['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['lemma_low'],row.spacy_lg_dict[i+2]['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['lemma_low'],row.spacy_lg_dict[i+4]['lemma_low']]\n",
    "            elif i == 3:\n",
    "              \n",
    "                token['window_text'] = [None, row.spacy_lg_dict[i-3]['text_low'], row.spacy_lg_dict[i-2]['text_low'],\n",
    "                                        row.spacy_lg_dict[i-1]['text_low'],token['text_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['text_low'],row.spacy_lg_dict[i+2]['text_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['text_low'],row.spacy_lg_dict[i+4]['text_low']]\n",
    "                token['window_lemma'] = [None, row.spacy_lg_dict[i-3]['lemma_low'], row.spacy_lg_dict[i-2]['lemma_low'],\n",
    "                                         row.spacy_lg_dict[i-1]['lemma_low'],token['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['lemma_low'],row.spacy_lg_dict[i+2]['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['lemma_low'],row.spacy_lg_dict[i+4]['lemma_low']]\n",
    "            elif i == (len(row.spacy_lg_dict)-4):\n",
    "               \n",
    "                token['window_text'] = [row.spacy_lg_dict[i-4]['text_low'], row.spacy_lg_dict[i-3]['text_low'],\n",
    "                                        row.spacy_lg_dict[i-2]['text_low'], row.spacy_lg_dict[i-1]['text_low'],\n",
    "                                        token['text_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['text_low'],row.spacy_lg_dict[i+2]['text_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['text_low'],None]\n",
    "                token['window_lemma'] = [row.spacy_lg_dict[i-4]['lemma_low'], row.spacy_lg_dict[i-3]['lemma_low'],\n",
    "                                         row.spacy_lg_dict[i-2]['lemma_low'], row.spacy_lg_dict[i-1]['lemma_low'],\n",
    "                                         token['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['lemma_low'],row.spacy_lg_dict[i+2]['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['lemma_low'],None]\n",
    "            elif i == (len(row.spacy_lg_dict)-3):\n",
    "                \n",
    "                token['window_text'] = [row.spacy_lg_dict[i-4]['text_low'], row.spacy_lg_dict[i-3]['text_low'],\n",
    "                                        row.spacy_lg_dict[i-2]['text_low'], row.spacy_lg_dict[i-1]['text_low'],\n",
    "                                        token['text_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['text_low'],row.spacy_lg_dict[i+2]['text_low'],\n",
    "                                        None,None]\n",
    "                token['window_lemma'] = [row.spacy_lg_dict[i-4]['lemma_low'], row.spacy_lg_dict[i-3]['lemma_low'],\n",
    "                                         row.spacy_lg_dict[i-2]['lemma_low'], row.spacy_lg_dict[i-1]['lemma_low'],\n",
    "                                         token['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['lemma_low'],row.spacy_lg_dict[i+2]['lemma_low'],\n",
    "                                         None,None]\n",
    "            elif i == (len(row.spacy_lg_dict)-2):\n",
    "                \n",
    "                token['window_text'] = [row.spacy_lg_dict[i-4]['text_low'], row.spacy_lg_dict[i-3]['text_low'],\n",
    "                                        row.spacy_lg_dict[i-2]['text_low'], row.spacy_lg_dict[i-1]['text_low'],\n",
    "                                        token['text_low'],row.spacy_lg_dict[i+1]['text_low'],None,None,None]\n",
    "                token['window_lemma'] = [row.spacy_lg_dict[i-4]['lemma_low'], row.spacy_lg_dict[i-3]['lemma_low'],\n",
    "                                         row.spacy_lg_dict[i-2]['lemma_low'], row.spacy_lg_dict[i-1]['lemma_low'],\n",
    "                                         token['lemma_low'],row.spacy_lg_dict[i+1]['lemma_low'],None,None,None]\n",
    "            elif i == (len(row.spacy_lg_dict)-1):\n",
    "                \n",
    "                token['window_text'] = [row.spacy_lg_dict[i-4]['text_low'], row.spacy_lg_dict[i-3]['text_low'],\n",
    "                                        row.spacy_lg_dict[i-2]['text_low'], row.spacy_lg_dict[i-1]['text_low'],\n",
    "                                        token['text_low'],None,None,None,None]\n",
    "                token['window_lemma'] = [row.spacy_lg_dict[i-4]['lemma_low'], row.spacy_lg_dict[i-3]['lemma_low'],\n",
    "                                         row.spacy_lg_dict[i-2]['lemma_low'], row.spacy_lg_dict[i-1]['lemma_low'],\n",
    "                                         token['lemma_low'],None,None,None,None]\n",
    "            else:\n",
    "                \n",
    "                token['window_text'] = [row.spacy_lg_dict[i-4]['text_low'], row.spacy_lg_dict[i-3]['text_low'],\n",
    "                                        row.spacy_lg_dict[i-2]['text_low'], row.spacy_lg_dict[i-1]['text_low'],\n",
    "                                        token['text_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['text_low'],row.spacy_lg_dict[i+2]['text_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['text_low'],row.spacy_lg_dict[i+4]['text_low']]\n",
    "                token['window_lemma'] = [row.spacy_lg_dict[i-4]['lemma_low'], row.spacy_lg_dict[i-3]['lemma_low'],\n",
    "                                         row.spacy_lg_dict[i-2]['lemma_low'], row.spacy_lg_dict[i-1]['lemma_low'],\n",
    "                                         token['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+1]['lemma_low'],row.spacy_lg_dict[i+2]['lemma_low'],\n",
    "                                        row.spacy_lg_dict[i+3]['lemma_low'],row.spacy_lg_dict[i+4]['lemma_low']]\n",
    "\n",
    "        \n",
    "    else:\n",
    "        for i, token in enumerate(row.spacy_lg_dict):\n",
    "            token['window_text'] = None\n",
    "            token['window_lemma'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token itself\n",
    "for index, row in df.iterrows():\n",
    "    for i, token in enumerate(row.spacy_lg_dict):\n",
    "        if token['window_text'] != None and token['window_lemma'] != None:\n",
    "            window_text = token['window_text']\n",
    "            window_lemma = token['window_lemma']\n",
    "\n",
    "            # words\n",
    "            c1, c2, c_m, c3, c4 = None, None, None, None, None\n",
    "            if window_text[2] != None and window_text[3] != None:\n",
    "                c1 = window_text[2] + ' ' + window_text[3] + ' ' + token['text_low']\n",
    "            if window_text[3] != None:\n",
    "                c2 = window_text[3] + ' ' + token['text_low']\n",
    "            if window_text[3] != None and window_text[5] != None:\n",
    "                c_m = window_text[3] + ' ' + token['text_low'] + ' ' + window_text[5]\n",
    "            if window_text[5] != None:\n",
    "                c3 = token['text_low'] + ' ' + window_text[5]\n",
    "            if window_text[5] != None and window_text[6] != None:\n",
    "                c4 = token['text_low'] + ' ' + window_text[5] + ' ' + window_text[6]\n",
    "\n",
    "            # lemmas\n",
    "            c1l, c2l, c_ml, c3l, c4l = None, None, None, None, None\n",
    "            if window_lemma[2] != None and window_lemma[3] != None:\n",
    "                c1l = window_lemma[2] + ' ' + window_lemma[3] + ' ' + token['lemma_low']\n",
    "            if window_lemma[3] != None:\n",
    "                c2l = window_lemma[3] + ' ' + token['lemma_low']\n",
    "            if window_lemma[3] != None and window_lemma[5] != None:\n",
    "                c_ml = window_lemma[3] + ' ' + token['lemma_low'] + ' ' + window_lemma[5]\n",
    "            if window_lemma[5] != None:\n",
    "                c3l = token['lemma_low'] + ' ' + window_lemma[5]\n",
    "            if window_lemma[5] != None and window_text[6] != None:\n",
    "                c4l = token['lemma_low'] + ' ' + window_lemma[5] + ' ' + window_lemma[6]\n",
    "\n",
    "            # assertive verbs\n",
    "            if token['text_low'] in assertives or c1 in assertives or c2 in assertives or \\\n",
    "            c_m in assertives or c3 in assertives or c4 in assertives or \\\n",
    "            token['lemma_low'] in assertives or c1l in assertives or c2l in assertives or \\\n",
    "            c_ml in assertives or c3l in assertives or c4l in assertives:\n",
    "                token['assertives'] = 1\n",
    "            else:\n",
    "                token['assertives'] = 0   \n",
    "\n",
    "            # factive verbs\n",
    "            if token['text_low'] in factives or c1 in factives or c2 in factives or \\\n",
    "            c_m in factives or c3 in factives or c4 in factives or \\\n",
    "            token['lemma_low'] in factives or c1l in factives or c2l in factives or \\\n",
    "            c_ml in factives or c3l in factives or c4l in factives:\n",
    "                token['factives'] = 1\n",
    "            else:\n",
    "                token['factives'] = 0   \n",
    "\n",
    "            # report verbs\n",
    "            if token['text_low'] in report_verbs or c1 in report_verbs or c2 in report_verbs or \\\n",
    "            c_m in report_verbs or c3 in report_verbs or c4 in report_verbs or \\\n",
    "            token['lemma_low'] in report_verbs or c1l in report_verbs or c2l in report_verbs or \\\n",
    "            c_ml in report_verbs or c3l in report_verbs or c4l in report_verbs:\n",
    "                token['report_verbs'] = 1\n",
    "            else:\n",
    "                token['report_verbs'] = 0\n",
    "\n",
    "            # hedges\n",
    "            if token['text_low'] in hedges or c1 in hedges or c2 in hedges or \\\n",
    "            c_m in hedges or c3 in hedges or c4 in hedges or \\\n",
    "            token['lemma_low'] in hedges or c1l in hedges or c2l in hedges or \\\n",
    "            c_ml in hedges or c3l in hedges or c4l in hedges:\n",
    "                token['hedges'] = 1\n",
    "            else:\n",
    "                token['hedges'] = 0\n",
    "\n",
    "            # boosters\n",
    "            #if token['text_low'] in boosters or c1 in boosters or c2 in boosters or \\\n",
    "            #c_m in boosters or c3 in boosters or c4 in boosters or \\\n",
    "            #token['lemma_low'] in boosters or c1l in boosters or c2l in boosters or \\\n",
    "            #c_ml in boosters or c3l in boosters or c4l in boosters:\n",
    "            #    token['boosters'] = 1\n",
    "            #else:\n",
    "            #    token['boosters'] = 0\n",
    "                \n",
    "            # implicative verbs\n",
    "            if token['text_low'] in implicatives or c1 in implicatives or c2 in implicatives or \\\n",
    "            c_m in implicatives or c3 in implicatives or c4 in implicatives or \\\n",
    "            token['lemma_low'] in implicatives or c1l in implicatives or c2l in implicatives or \\\n",
    "            c_ml in implicatives or c3l in implicatives or c4l in implicatives:\n",
    "                token['implicatives'] = 1\n",
    "            else:\n",
    "                token['implicatives'] = 0\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            # assertive verbs\n",
    "            if token['text_low'] in assertives or token['lemma_low'] in assertives:\n",
    "                token['assertives'] = 1\n",
    "            else:\n",
    "                token['assertives'] = 0 \n",
    "\n",
    "            # factive verbs\n",
    "            if token['text_low'] in factives or token['lemma_low'] in factives:\n",
    "                token['factives'] = 1\n",
    "            else:\n",
    "                token['factives'] = 0   \n",
    "\n",
    "            # report verbs\n",
    "            if token['text_low'] in report_verbs or token['lemma_low'] in report_verbs:\n",
    "                token['report_verbs'] = 1\n",
    "            else:\n",
    "                token['report_verbs'] = 0\n",
    "\n",
    "            # hedges\n",
    "            if token['text_low'] in hedges or token['lemma_low'] in hedges:\n",
    "                token['hedges'] = 1\n",
    "            else:\n",
    "                token['hedges'] = 0\n",
    "\n",
    "            # boosters\n",
    "            #if token['text_low'] in boosters or token['lemma_low'] in boosters:\n",
    "            #    token['boosters'] = 1\n",
    "            #else:\n",
    "            #    token['boosters'] = 0\n",
    "                \n",
    "            # implicative verbs\n",
    "            if token['text_low'] in implicatives or token['lemma_low'] in implicatives:\n",
    "                token['implicatives'] = 1\n",
    "            else:\n",
    "                token['implicatives'] = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c1\n",
    "for index, row in df.iterrows():\n",
    "    for i, token in enumerate(row.spacy_lg_dict):\n",
    "        if token['text_low_c1'] == None:\n",
    "            token['assertives_c1'] = 0\n",
    "            token['factives_c1'] = 0\n",
    "            token['report_verbs_c1'] = 0\n",
    "            token['hedges_c1'] = 0\n",
    "            #token['boosters_C1'] = 0\n",
    "            token['implicatives_c1'] = 0\n",
    "        else:\n",
    "            \n",
    "            if token['window_text'] != None and token['window_lemma'] != None:\n",
    "                window_text = token['window_text']\n",
    "                window_lemma = token['window_lemma']\n",
    "\n",
    "                # words\n",
    "                c1, c2, c_m, c3, c4 = None, None, None, None, None\n",
    "                if window_text[0] != None and window_text[1] != None:\n",
    "                    c1 = window_text[0] + ' ' + window_text[1] + ' ' + token['text_low_c1']\n",
    "                if window_text[1] != None:\n",
    "                    c2 = window_text[1] + ' ' + token['text_low_c1']\n",
    "                if window_text[1] != None and window_text[3] != None:\n",
    "                    c_m = window_text[1] + ' ' + token['text_low_c1'] + ' ' + window_text[3]\n",
    "                if window_text[3] != None:\n",
    "                    c3 = token['text_low_c1'] + ' ' + window_text[3]\n",
    "                if window_text[3] != None and window_text[4] != None:\n",
    "                    c4 = token['text_low_c1'] + ' ' + window_text[3] + ' ' + window_text[4]\n",
    "\n",
    "                # lemmas\n",
    "                c1l, c2l, c_ml, c3l, c4l = None, None, None, None, None\n",
    "                if window_lemma[0] != None and window_lemma[1] != None:\n",
    "                    c1l = window_lemma[0] + ' ' + window_lemma[1] + ' ' + token['lemma_low_c1']\n",
    "                if window_lemma[1] != None:\n",
    "                    c2l = window_lemma[1] + ' ' + token['lemma_low_c1']\n",
    "                if window_lemma[1] != None and window_lemma[3] != None:\n",
    "                    c_ml = window_lemma[1] + ' ' + token['lemma_low_c1'] + ' ' + window_lemma[3]\n",
    "                if window_lemma[3] != None:\n",
    "                    c3l = token['lemma_low_c1'] + ' ' + window_lemma[3]\n",
    "                if window_lemma[3] != None and window_text[4] != None:\n",
    "                    c4l = token['lemma_low_c1'] + ' ' + window_lemma[3] + ' ' + window_lemma[4]\n",
    "\n",
    "                # assertive verbs\n",
    "                if token['text_low_c1'] in assertives or c1 in assertives or c2 in assertives or \\\n",
    "                c_m in assertives or c3 in assertives or c4 in assertives or \\\n",
    "                token['lemma_low_c1'] in assertives or c1l in assertives or c2l in assertives or \\\n",
    "                c_ml in assertives or c3l in assertives or c4l in assertives:\n",
    "                    token['assertives_c1'] = 1\n",
    "                else:\n",
    "                    token['assertives_c1'] = 0   \n",
    "\n",
    "                # factive verbs\n",
    "                if token['text_low_c1'] in factives or c1 in factives or c2 in factives or \\\n",
    "                c_m in factives or c3 in factives or c4 in factives or \\\n",
    "                token['lemma_low_c1'] in factives or c1l in factives or c2l in factives or \\\n",
    "                c_ml in factives or c3l in factives or c4l in factives:\n",
    "                    token['factives_c1'] = 1\n",
    "                else:\n",
    "                    token['factives_c1'] = 0   \n",
    "\n",
    "                # report verbs\n",
    "                if token['text_low_c1'] in report_verbs or c1 in report_verbs or c2 in report_verbs or \\\n",
    "                c_m in report_verbs or c3 in report_verbs or c4 in report_verbs or \\\n",
    "                token['lemma_low_c1'] in report_verbs or c1l in report_verbs or c2l in report_verbs or \\\n",
    "                c_ml in report_verbs or c3l in report_verbs or c4l in report_verbs:\n",
    "                    token['report_verbs_c1'] = 1\n",
    "                else:\n",
    "                    token['report_verbs_c1'] = 0\n",
    "\n",
    "                # hedges\n",
    "                if token['text_low_c1'] in hedges or c1 in hedges or c2 in hedges or \\\n",
    "                c_m in hedges or c3 in hedges or c4 in hedges or \\\n",
    "                token['lemma_low_c1'] in hedges or c1l in hedges or c2l in hedges or \\\n",
    "                c_ml in hedges or c3l in hedges or c4l in hedges:\n",
    "                    token['hedges_c1'] = 1\n",
    "                else:\n",
    "                    token['hedges_c1'] = 0\n",
    "\n",
    "                # boosters\n",
    "                #if token['text_low'] in boosters or c1 in boosters or c2 in boosters or \\\n",
    "                #c_m in boosters or c3 in boosters or c4 in boosters or \\\n",
    "                #token['lemma_low'] in boosters or c1l in boosters or c2l in boosters or \\\n",
    "                #c_ml in boosters or c3l in boosters or c4l in boosters:\n",
    "                #    token['boosters'] = 1\n",
    "                #else:\n",
    "                #    token['boosters'] = 0\n",
    "\n",
    "                # implicative verbs\n",
    "                if token['text_low_c1'] in implicatives or c1 in implicatives or c2 in implicatives or \\\n",
    "                c_m in implicatives or c3 in implicatives or c4 in implicatives or \\\n",
    "                token['lemma_low_c1'] in implicatives or c1l in implicatives or c2l in implicatives or \\\n",
    "                c_ml in implicatives or c3l in implicatives or c4l in implicatives:\n",
    "                    token['implicatives_c1'] = 1\n",
    "                else:\n",
    "                    token['implicatives_c1'] = 0\n",
    "\n",
    "\n",
    "            else:\n",
    "                # assertive verbs\n",
    "                if token['text_low_c1'] in assertives or token['lemma_low_c1'] in assertives:\n",
    "                    token['assertives_c1'] = 1\n",
    "                else:\n",
    "                    token['assertives_c1'] = 0 \n",
    "\n",
    "                # factive verbs\n",
    "                if token['text_low_c1'] in factives or token['lemma_low_c1'] in factives:\n",
    "                    token['factives_c1'] = 1\n",
    "                else:\n",
    "                    token['factives_c1'] = 0   \n",
    "\n",
    "                # report verbs\n",
    "                if token['text_low'] in report_verbs or token['lemma_low'] in report_verbs:\n",
    "                    token['report_verbs_c1'] = 1\n",
    "                else:\n",
    "                    token['report_verbs_c1'] = 0\n",
    "\n",
    "                # hedges\n",
    "                if token['text_low_c1'] in hedges or token['lemma_low_c1'] in hedges:\n",
    "                    token['hedges_c1'] = 1\n",
    "                else:\n",
    "                    token['hedges_c1'] = 0\n",
    "\n",
    "                # boosters\n",
    "                #if token['text_low'] in boosters or token['lemma_low'] in boosters:\n",
    "                #    token['boosters'] = 1\n",
    "                #else:\n",
    "                #    token['boosters'] = 0\n",
    "\n",
    "                # implicative verbs\n",
    "                if token['text_low_c1'] in implicatives or token['lemma_low_c1'] in implicatives:\n",
    "                    token['implicatives_c1'] = 1\n",
    "                else:\n",
    "                    token['implicatives_c1'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c2\n",
    "for index, row in df.iterrows():\n",
    "    for i, token in enumerate(row.spacy_lg_dict):\n",
    "        if token['text_low_c2'] == None:\n",
    "            token['assertives_c2'] = 0\n",
    "            token['factives_c2'] = 0\n",
    "            token['report_verbs_c2'] = 0\n",
    "            token['hedges_c2'] = 0\n",
    "            #token['boosters_c2'] = 0\n",
    "            token['implicatives_c2'] = 0\n",
    "        else:\n",
    "            \n",
    "            if token['window_text'] != None and token['window_lemma'] != None:\n",
    "                window_text = token['window_text']\n",
    "                window_lemma = token['window_lemma']\n",
    "\n",
    "                # words\n",
    "                c1, c2, c_m, c3, c4 = None, None, None, None, None\n",
    "                if window_text[1] != None and window_text[2] != None:\n",
    "                    c1 = window_text[1] + ' ' + window_text[2] + ' ' + token['text_low_c2']\n",
    "                if window_text[2] != None:\n",
    "                    c2 = window_text[2] + ' ' + token['text_low_c2']\n",
    "                if window_text[2] != None and window_text[4] != None:\n",
    "                    c_m = window_text[2] + ' ' + token['text_low_c2'] + ' ' + window_text[4]\n",
    "                if window_text[4] != None:\n",
    "                    c3 = token['text_low_c2'] + ' ' + window_text[4]\n",
    "                if window_text[4] != None and window_text[5] != None:\n",
    "                    c4 = token['text_low_c2'] + ' ' + window_text[4] + ' ' + window_text[5]\n",
    "\n",
    "                # lemmas\n",
    "                c1l, c2l, c_ml, c3l, c4l = None, None, None, None, None\n",
    "                if window_lemma[1] != None and window_lemma[2] != None:\n",
    "                    c1l = window_lemma[1] + ' ' + window_lemma[2] + ' ' + token['lemma_low_c2']\n",
    "                if window_lemma[2] != None:\n",
    "                    c2l = window_lemma[2] + ' ' + token['lemma_low_c2']\n",
    "                if window_lemma[2] != None and window_lemma[4] != None:\n",
    "                    c_ml = window_lemma[2] + ' ' + token['lemma_low_c2'] + ' ' + window_lemma[4]\n",
    "                if window_lemma[4] != None:\n",
    "                    c3l = token['lemma_low_c2'] + ' ' + window_lemma[4]\n",
    "                if window_lemma[4] != None and window_text[5] != None:\n",
    "                    c4l = token['lemma_low_c2'] + ' ' + window_lemma[4] + ' ' + window_lemma[5]\n",
    "\n",
    "                # assertive verbs\n",
    "                if token['text_low_c2'] in assertives or c1 in assertives or c2 in assertives or \\\n",
    "                c_m in assertives or c3 in assertives or c4 in assertives or \\\n",
    "                token['lemma_low_c2'] in assertives or c1l in assertives or c2l in assertives or \\\n",
    "                c_ml in assertives or c3l in assertives or c4l in assertives:\n",
    "                    token['assertives_c2'] = 1\n",
    "                else:\n",
    "                    token['assertives_c2'] = 0   \n",
    "\n",
    "                # factive verbs\n",
    "                if token['text_low_c2'] in factives or c1 in factives or c2 in factives or \\\n",
    "                c_m in factives or c3 in factives or c4 in factives or \\\n",
    "                token['lemma_low_c2'] in factives or c1l in factives or c2l in factives or \\\n",
    "                c_ml in factives or c3l in factives or c4l in factives:\n",
    "                    token['factives_c2'] = 1\n",
    "                else:\n",
    "                    token['factives_c2'] = 0   \n",
    "\n",
    "                # report verbs\n",
    "                if token['text_low_c2'] in report_verbs or c1 in report_verbs or c2 in report_verbs or \\\n",
    "                c_m in report_verbs or c3 in report_verbs or c4 in report_verbs or \\\n",
    "                token['lemma_low_c2'] in report_verbs or c1l in report_verbs or c2l in report_verbs or \\\n",
    "                c_ml in report_verbs or c3l in report_verbs or c4l in report_verbs:\n",
    "                    token['report_verbs_c2'] = 1\n",
    "                else:\n",
    "                    token['report_verbs_c2'] = 0\n",
    "\n",
    "                # hedges\n",
    "                if token['text_low_c2'] in hedges or c1 in hedges or c2 in hedges or \\\n",
    "                c_m in hedges or c3 in hedges or c4 in hedges or \\\n",
    "                token['lemma_low_c2'] in hedges or c1l in hedges or c2l in hedges or \\\n",
    "                c_ml in hedges or c3l in hedges or c4l in hedges:\n",
    "                    token['hedges_c2'] = 1\n",
    "                else:\n",
    "                    token['hedges_c2'] = 0\n",
    "\n",
    "                # boosters\n",
    "                #if token['text_low'] in boosters or c1 in boosters or c2 in boosters or \\\n",
    "                #c_m in boosters or c3 in boosters or c4 in boosters or \\\n",
    "                #token['lemma_low'] in boosters or c1l in boosters or c2l in boosters or \\\n",
    "                #c_ml in boosters or c3l in boosters or c4l in boosters:\n",
    "                #    token['boosters'] = 1\n",
    "                #else:\n",
    "                #    token['boosters'] = 0\n",
    "\n",
    "                # implicative verbs\n",
    "                if token['text_low_c2'] in implicatives or c1 in implicatives or c2 in implicatives or \\\n",
    "                c_m in implicatives or c3 in implicatives or c4 in implicatives or \\\n",
    "                token['lemma_low_c2'] in implicatives or c1l in implicatives or c2l in implicatives or \\\n",
    "                c_ml in implicatives or c3l in implicatives or c4l in implicatives:\n",
    "                    token['implicatives_c2'] = 1\n",
    "                else:\n",
    "                    token['implicatives_c2'] = 0\n",
    "\n",
    "\n",
    "            else:\n",
    "                # assertive verbs\n",
    "                if token['text_low_c2'] in assertives or token['lemma_low_c2'] in assertives:\n",
    "                    token['assertives_c2'] = 1\n",
    "                else:\n",
    "                    token['assertives_c2'] = 0 \n",
    "\n",
    "                # factive verbs\n",
    "                if token['text_low_c2'] in factives or token['lemma_low_c2'] in factives:\n",
    "                    token['factives_c2'] = 1\n",
    "                else:\n",
    "                    token['factives_c2'] = 0   \n",
    "\n",
    "                # report verbs\n",
    "                if token['text_low_c2'] in report_verbs or token['lemma_low_c2'] in report_verbs:\n",
    "                    token['report_verbs_c2'] = 1\n",
    "                else:\n",
    "                    token['report_verbs_c2'] = 0\n",
    "\n",
    "                # hedges\n",
    "                if token['text_low_c2'] in hedges or token['lemma_low_c2'] in hedges:\n",
    "                    token['hedges_c2'] = 1\n",
    "                else:\n",
    "                    token['hedges_c2'] = 0\n",
    "\n",
    "                # boosters\n",
    "                #if token['text_low'] in boosters or token['lemma_low'] in boosters:\n",
    "                #    token['boosters'] = 1\n",
    "                #else:\n",
    "                #    token['boosters'] = 0\n",
    "\n",
    "                # implicative verbs\n",
    "                if token['text_low_c2'] in implicatives or token['lemma_low_c2'] in implicatives:\n",
    "                    token['implicatives_c2'] = 1\n",
    "                else:\n",
    "                    token['implicatives_c2'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c3\n",
    "for index, row in df.iterrows():\n",
    "    for i, token in enumerate(row.spacy_lg_dict):\n",
    "        if token['text_low_c3'] == None:\n",
    "            token['assertives_c3'] = 0\n",
    "            token['factives_c3'] = 0\n",
    "            token['report_verbs_c3'] = 0\n",
    "            token['hedges_c3'] = 0\n",
    "            #token['boosters_c2'] = 0\n",
    "            token['implicatives_c3'] = 0\n",
    "        else:\n",
    "            \n",
    "            if token['window_text'] != None and token['window_lemma'] != None:\n",
    "                window_text = token['window_text']\n",
    "                window_lemma = token['window_lemma']\n",
    "\n",
    "                # words\n",
    "                c1, c2, c_m, c3, c4 = None, None, None, None, None\n",
    "                if window_text[3] != None and window_text[4] != None:\n",
    "                    c1 = window_text[3] + ' ' + window_text[4] + ' ' + token['text_low_c3']\n",
    "                if window_text[4] != None:\n",
    "                    c2 = window_text[4] + ' ' + token['text_low_c3']\n",
    "                if window_text[4] != None and window_text[6] != None:\n",
    "                    c_m = window_text[4] + ' ' + token['text_low_c3'] + ' ' + window_text[6]\n",
    "                if window_text[6] != None:\n",
    "                    c3 = token['text_low_c3'] + ' ' + window_text[6]\n",
    "                if window_text[6] != None and window_text[7] != None:\n",
    "                    c4 = token['text_low_c3'] + ' ' + window_text[6] + ' ' + window_text[7]\n",
    "\n",
    "                # lemmas\n",
    "                c1l, c2l, c_ml, c3l, c4l = None, None, None, None, None\n",
    "                if window_lemma[3] != None and window_lemma[4] != None:\n",
    "                    c1l = window_lemma[3] + ' ' + window_lemma[4] + ' ' + token['lemma_low_c3']\n",
    "                if window_lemma[4] != None:\n",
    "                    c2l = window_lemma[4] + ' ' + token['lemma_low_c3']\n",
    "                if window_lemma[4] != None and window_lemma[6] != None:\n",
    "                    c_ml = window_lemma[4] + ' ' + token['lemma_low_c3'] + ' ' + window_lemma[6]\n",
    "                if window_lemma[6] != None:\n",
    "                    c3l = token['lemma_low_c3'] + ' ' + window_lemma[6]\n",
    "                if window_lemma[6] != None and window_text[7] != None:\n",
    "                    c4l = token['lemma_low_c3'] + ' ' + window_lemma[6] + ' ' + window_lemma[7]\n",
    "\n",
    "                # assertive verbs\n",
    "                if token['text_low_c3'] in assertives or c1 in assertives or c2 in assertives or \\\n",
    "                c_m in assertives or c3 in assertives or c4 in assertives or \\\n",
    "                token['lemma_low_c3'] in assertives or c1l in assertives or c2l in assertives or \\\n",
    "                c_ml in assertives or c3l in assertives or c4l in assertives:\n",
    "                    token['assertives_c3'] = 1\n",
    "                else:\n",
    "                    token['assertives_c3'] = 0   \n",
    "\n",
    "                # factive verbs\n",
    "                if token['text_low_c3'] in factives or c1 in factives or c2 in factives or \\\n",
    "                c_m in factives or c3 in factives or c4 in factives or \\\n",
    "                token['lemma_low_c3'] in factives or c1l in factives or c2l in factives or \\\n",
    "                c_ml in factives or c3l in factives or c4l in factives:\n",
    "                    token['factives_c3'] = 1\n",
    "                else:\n",
    "                    token['factives_c3'] = 0   \n",
    "\n",
    "                # report verbs\n",
    "                if token['text_low_c3'] in report_verbs or c1 in report_verbs or c2 in report_verbs or \\\n",
    "                c_m in report_verbs or c3 in report_verbs or c4 in report_verbs or \\\n",
    "                token['lemma_low_c3'] in report_verbs or c1l in report_verbs or c2l in report_verbs or \\\n",
    "                c_ml in report_verbs or c3l in report_verbs or c4l in report_verbs:\n",
    "                    token['report_verbs_c3'] = 1\n",
    "                else:\n",
    "                    token['report_verbs_c3'] = 0\n",
    "\n",
    "                # hedges\n",
    "                if token['text_low_c3'] in hedges or c1 in hedges or c2 in hedges or \\\n",
    "                c_m in hedges or c3 in hedges or c4 in hedges or \\\n",
    "                token['lemma_low_c3'] in hedges or c1l in hedges or c2l in hedges or \\\n",
    "                c_ml in hedges or c3l in hedges or c4l in hedges:\n",
    "                    token['hedges_c3'] = 1\n",
    "                else:\n",
    "                    token['hedges_c3'] = 0\n",
    "\n",
    "                # boosters\n",
    "                #if token['text_low'] in boosters or c1 in boosters or c2 in boosters or \\\n",
    "                #c_m in boosters or c3 in boosters or c4 in boosters or \\\n",
    "                #token['lemma_low'] in boosters or c1l in boosters or c2l in boosters or \\\n",
    "                #c_ml in boosters or c3l in boosters or c4l in boosters:\n",
    "                #    token['boosters'] = 1\n",
    "                #else:\n",
    "                #    token['boosters'] = 0\n",
    "\n",
    "                # implicative verbs\n",
    "                if token['text_low_c3'] in implicatives or c1 in implicatives or c2 in implicatives or \\\n",
    "                c_m in implicatives or c3 in implicatives or c4 in implicatives or \\\n",
    "                token['lemma_low_c3'] in implicatives or c1l in implicatives or c2l in implicatives or \\\n",
    "                c_ml in implicatives or c3l in implicatives or c4l in implicatives:\n",
    "                    token['implicatives_c3'] = 1\n",
    "                else:\n",
    "                    token['implicatives_c3'] = 0\n",
    "\n",
    "\n",
    "            else:\n",
    "                # assertive verbs\n",
    "                if token['text_low_c3'] in assertives or token['lemma_low_c3'] in assertives:\n",
    "                    token['assertives_c3'] = 1\n",
    "                else:\n",
    "                    token['assertives_c3'] = 0 \n",
    "\n",
    "                # factive verbs\n",
    "                if token['text_low_c3'] in factives or token['lemma_low_c3'] in factives:\n",
    "                    token['factives_c3'] = 1\n",
    "                else:\n",
    "                    token['factives_c3'] = 0   \n",
    "\n",
    "                # report verbs\n",
    "                if token['text_low_c3'] in report_verbs or token['lemma_low_c3'] in report_verbs:\n",
    "                    token['report_verbs_c3'] = 1\n",
    "                else:\n",
    "                    token['report_verbs_c3'] = 0\n",
    "\n",
    "                # hedges\n",
    "                if token['text_low_c3'] in hedges or token['lemma_low_c3'] in hedges:\n",
    "                    token['hedges_c3'] = 1\n",
    "                else:\n",
    "                    token['hedges_c3'] = 0\n",
    "\n",
    "                # boosters\n",
    "                #if token['text_low'] in boosters or token['lemma_low'] in boosters:\n",
    "                #    token['boosters'] = 1\n",
    "                #else:\n",
    "                #    token['boosters'] = 0\n",
    "\n",
    "                # implicative verbs\n",
    "                if token['text_low_c3'] in implicatives or token['lemma_low_c3'] in implicatives:\n",
    "                    token['implicatives_c3'] = 1\n",
    "                else:\n",
    "                    token['implicatives_c3'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c4\n",
    "for index, row in df.iterrows():\n",
    "    for i, token in enumerate(row.spacy_lg_dict):\n",
    "        if token['text_low_c4'] == None:\n",
    "            token['assertives_c4'] = 0\n",
    "            token['factives_c4'] = 0\n",
    "            token['report_verbs_c4'] = 0\n",
    "            token['hedges_c4'] = 0\n",
    "            #token['boosters_c2'] = 0\n",
    "            token['implicatives_c4'] = 0\n",
    "        else:\n",
    "            \n",
    "            if token['window_text'] != None and token['window_lemma'] != None:\n",
    "                window_text = token['window_text']\n",
    "                window_lemma = token['window_lemma']\n",
    "\n",
    "                # words\n",
    "                c1, c2, c_m, c3, c4 = None, None, None, None, None\n",
    "                if window_text[4] != None and window_text[5] != None:\n",
    "                    c1 = window_text[4] + ' ' + window_text[5] + ' ' + token['text_low_c4']\n",
    "                if window_text[5] != None:\n",
    "                    c2 = window_text[5] + ' ' + token['text_low_c4']\n",
    "                if window_text[5] != None and window_text[7] != None:\n",
    "                    c_m = window_text[5] + ' ' + token['text_low_c4'] + ' ' + window_text[7]\n",
    "                if window_text[7] != None:\n",
    "                    c3 = token['text_low_c4'] + ' ' + window_text[7]\n",
    "                if window_text[7] != None and window_text[8] != None:\n",
    "                    c4 = token['text_low_c4'] + ' ' + window_text[7] + ' ' + window_text[8]\n",
    "\n",
    "                # lemmas\n",
    "                c1l, c2l, c_ml, c3l, c4l = None, None, None, None, None\n",
    "                if window_lemma[4] != None and window_lemma[5] != None:\n",
    "                    c1l = window_lemma[4] + ' ' + window_lemma[5] + ' ' + token['lemma_low_c4']\n",
    "                if window_lemma[5] != None:\n",
    "                    c2l = window_lemma[5] + ' ' + token['lemma_low_c4']\n",
    "                if window_lemma[5] != None and window_lemma[7] != None:\n",
    "                    c_ml = window_lemma[5] + ' ' + token['lemma_low_c4'] + ' ' + window_lemma[7]\n",
    "                if window_lemma[7] != None:\n",
    "                    c3l = token['lemma_low_c4'] + ' ' + window_lemma[7]\n",
    "                if window_lemma[7] != None and window_text[8] != None:\n",
    "                    c4l = token['lemma_low_c4'] + ' ' + window_lemma[7] + ' ' + window_lemma[8]\n",
    "\n",
    "                # assertive verbs\n",
    "                if token['text_low_c4'] in assertives or c1 in assertives or c2 in assertives or \\\n",
    "                c_m in assertives or c3 in assertives or c4 in assertives or \\\n",
    "                token['lemma_low_c4'] in assertives or c1l in assertives or c2l in assertives or \\\n",
    "                c_ml in assertives or c3l in assertives or c4l in assertives:\n",
    "                    token['assertives_c4'] = 1\n",
    "                else:\n",
    "                    token['assertives_c4'] = 0   \n",
    "\n",
    "                # factive verbs\n",
    "                if token['text_low_c4'] in factives or c1 in factives or c2 in factives or \\\n",
    "                c_m in factives or c3 in factives or c4 in factives or \\\n",
    "                token['lemma_low_c4'] in factives or c1l in factives or c2l in factives or \\\n",
    "                c_ml in factives or c3l in factives or c4l in factives:\n",
    "                    token['factives_c4'] = 1\n",
    "                else:\n",
    "                    token['factives_c4'] = 0   \n",
    "\n",
    "                # report verbs\n",
    "                if token['text_low_c4'] in report_verbs or c1 in report_verbs or c2 in report_verbs or \\\n",
    "                c_m in report_verbs or c3 in report_verbs or c4 in report_verbs or \\\n",
    "                token['lemma_low_c4'] in report_verbs or c1l in report_verbs or c2l in report_verbs or \\\n",
    "                c_ml in report_verbs or c3l in report_verbs or c4l in report_verbs:\n",
    "                    token['report_verbs_c4'] = 1\n",
    "                else:\n",
    "                    token['report_verbs_c4'] = 0\n",
    "\n",
    "                # hedges\n",
    "                if token['text_low_c4'] in hedges or c1 in hedges or c2 in hedges or \\\n",
    "                c_m in hedges or c3 in hedges or c4 in hedges or \\\n",
    "                token['lemma_low_c4'] in hedges or c1l in hedges or c2l in hedges or \\\n",
    "                c_ml in hedges or c3l in hedges or c4l in hedges:\n",
    "                    token['hedges_c4'] = 1\n",
    "                else:\n",
    "                    token['hedges_c4'] = 0\n",
    "\n",
    "                # boosters\n",
    "                #if token['text_low'] in boosters or c1 in boosters or c2 in boosters or \\\n",
    "                #c_m in boosters or c3 in boosters or c4 in boosters or \\\n",
    "                #token['lemma_low'] in boosters or c1l in boosters or c2l in boosters or \\\n",
    "                #c_ml in boosters or c3l in boosters or c4l in boosters:\n",
    "                #    token['boosters'] = 1\n",
    "                #else:\n",
    "                #    token['boosters'] = 0\n",
    "\n",
    "                # implicative verbs\n",
    "                if token['text_low_c4'] in implicatives or c1 in implicatives or c2 in implicatives or \\\n",
    "                c_m in implicatives or c3 in implicatives or c4 in implicatives or \\\n",
    "                token['lemma_low_c4'] in implicatives or c1l in implicatives or c2l in implicatives or \\\n",
    "                c_ml in implicatives or c3l in implicatives or c4l in implicatives:\n",
    "                    token['implicatives_c4'] = 1\n",
    "                else:\n",
    "                    token['implicatives_c4'] = 0\n",
    "\n",
    "\n",
    "            else:\n",
    "                # assertive verbs\n",
    "                if token['text_low_c4'] in assertives or token['lemma_low_c4'] in assertives:\n",
    "                    token['assertives_c4'] = 1\n",
    "                else:\n",
    "                    token['assertives_c4'] = 0 \n",
    "\n",
    "                # factive verbs\n",
    "                if token['text_low_c4'] in factives or token['lemma_low_c4'] in factives:\n",
    "                    token['factives_c4'] = 1\n",
    "                else:\n",
    "                    token['factives_c4'] = 0   \n",
    "\n",
    "                # report verbs\n",
    "                if token['text_low_c4'] in report_verbs or token['lemma_low_c4'] in report_verbs:\n",
    "                    token['report_verbs_c4'] = 1\n",
    "                else:\n",
    "                    token['report_verbs_c4'] = 0\n",
    "\n",
    "                # hedges\n",
    "                if token['text_low_c4'] in hedges or token['lemma_low_c4'] in hedges:\n",
    "                    token['hedges_c4'] = 1\n",
    "                else:\n",
    "                    token['hedges_c4'] = 0\n",
    "\n",
    "                # boosters\n",
    "                #if token['text_low'] in boosters or token['lemma_low'] in boosters:\n",
    "                #    token['boosters'] = 1\n",
    "                #else:\n",
    "                #    token['boosters'] = 0\n",
    "\n",
    "                # implicative verbs\n",
    "                if token['text_low_c4'] in implicatives or token['lemma_low_c4'] in implicatives:\n",
    "                    token['implicatives_c4'] = 1\n",
    "                else:\n",
    "                    token['implicatives_c4'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the initial datset with sentences is: 16332 , the length of the exposed datset with words is: 164774\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "df.apply(lambda row: len([rows.append([row['Source'],\n",
    "                                               row['Headline'],\n",
    "                                               row['Text'],\n",
    "                                               row['Bias'],\n",
    "                                               row['Subject_Tag'],\n",
    "                                               row['Date'],\n",
    "                                               row['spacy_lg'], t]) for t in row.spacy_lg_dict]), axis=1)\n",
    "df_ungrouped = pd.DataFrame(rows, columns=['Source', 'Headline', 'Text', 'Bias', 'Subject_Tag', 'Date', 'spacy_lg', 'spacy_lg_dict'])\n",
    "\n",
    "print('The length of the initial datset with sentences is:', len(df),\n",
    "      ', the length of the exposed datset with words is:', len(df_ungrouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Subject_Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>spacy_lg</th>\n",
       "      <th>spacy_lg_dict</th>\n",
       "      <th>Cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>[{'text': 'COVID', 'text_low': 'covid', 'pos':...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wall Street Journal (News)</td>\n",
       "      <td>Vaccines Appear to Be Slowing Spread of Covid-...</td>\n",
       "      <td>Vaccines appear to be starting to curb new Cov...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(Vaccines, Appear, to, Be, Slowing, Spread, of...</td>\n",
       "      <td>[{'text': 'Vaccines', 'text_low': 'vaccines', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington Examiner</td>\n",
       "      <td>Pandemic retreat signals vaccines are working</td>\n",
       "      <td>COVID-19 cases and hospitalizations in the Uni...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(Pandemic, retreat, signals, vaccines, are, wo...</td>\n",
       "      <td>[{'text': 'Pandemic', 'text_low': 'pandemic', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Epoch Times</td>\n",
       "      <td>NYT, Washington Post, NBC Retract Incorrect Re...</td>\n",
       "      <td>The New York Times, The Washington Post, and N...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "      <td>(NYT, ,, Washington, Post, ,, NBC, Retract, In...</td>\n",
       "      <td>[{'text': 'NYT', 'text_low': 'nyt', 'pos': 'PR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>New York Times, WaPo, NBC retract reports abou...</td>\n",
       "      <td>The New York Times, The Washington Post and NB...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "      <td>(New, York, Times, ,, WaPo, ,, NBC, retract, r...</td>\n",
       "      <td>[{'text': 'New', 'text_low': 'new', 'pos': 'PR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Source  \\\n",
       "0    U.S. News & World Report   \n",
       "1  Wall Street Journal (News)   \n",
       "2         Washington Examiner   \n",
       "3             The Epoch Times   \n",
       "4                    The Hill   \n",
       "\n",
       "                                            Headline  \\\n",
       "0           COVID Deaths Continue to Decline in U.S.   \n",
       "1  Vaccines Appear to Be Slowing Spread of Covid-...   \n",
       "2      Pandemic retreat signals vaccines are working   \n",
       "3  NYT, Washington Post, NBC Retract Incorrect Re...   \n",
       "4  New York Times, WaPo, NBC retract reports abou...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  In a sign that the coronavirus pandemic is beg...   \n",
       "1  Vaccines appear to be starting to curb new Cov...   \n",
       "2  COVID-19 cases and hospitalizations in the Uni...   \n",
       "3  The New York Times, The Washington Post, and N...   \n",
       "4  The New York Times, The Washington Post and NB...   \n",
       "\n",
       "                                     Bias  \\\n",
       "0   AllSides Media Bias Rating: Lean Left   \n",
       "1      AllSides Media Bias Rating: Center   \n",
       "2  AllSides Media Bias Rating: Lean Right   \n",
       "3  AllSides Media Bias Rating: Lean Right   \n",
       "4      AllSides Media Bias Rating: Center   \n",
       "\n",
       "                                         Subject_Tag           Date  \\\n",
       "0  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "1  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "2  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "3  Media Industry, Media Bias, New York Times, Wa...  May 2nd, 2021   \n",
       "4  Media Industry, Media Bias, New York Times, Wa...  May 2nd, 2021   \n",
       "\n",
       "                                            spacy_lg  \\\n",
       "0   (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "1  (Vaccines, Appear, to, Be, Slowing, Spread, of...   \n",
       "2  (Pandemic, retreat, signals, vaccines, are, wo...   \n",
       "3  (NYT, ,, Washington, Post, ,, NBC, Retract, In...   \n",
       "4  (New, York, Times, ,, WaPo, ,, NBC, retract, r...   \n",
       "\n",
       "                                       spacy_lg_dict Cut  \n",
       "0  [{'text': 'COVID', 'text_low': 'covid', 'pos':...   1  \n",
       "1  [{'text': 'Vaccines', 'text_low': 'vaccines', ...   1  \n",
       "2  [{'text': 'Pandemic', 'text_low': 'pandemic', ...   1  \n",
       "3  [{'text': 'NYT', 'text_low': 'nyt', 'pos': 'PR...   1  \n",
       "4  [{'text': 'New', 'text_low': 'new', 'pos': 'PR...   1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Subject_Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>spacy_lg</th>\n",
       "      <th>spacy_lg_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'COVID', 'text_low': 'covid', 'pos': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Deaths', 'text_low': 'deaths', 'pos'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Continue', 'text_low': 'continue', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Decline', 'text_low': 'decline', 'po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'in', 'text_low': 'in', 'pos': 'ADP',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Source                                  Headline  \\\n",
       "0  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "1  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "2  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "3  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "4  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "\n",
       "                                                Text  \\\n",
       "0  In a sign that the coronavirus pandemic is beg...   \n",
       "1  In a sign that the coronavirus pandemic is beg...   \n",
       "2  In a sign that the coronavirus pandemic is beg...   \n",
       "3  In a sign that the coronavirus pandemic is beg...   \n",
       "4  In a sign that the coronavirus pandemic is beg...   \n",
       "\n",
       "                                    Bias  \\\n",
       "0  AllSides Media Bias Rating: Lean Left   \n",
       "1  AllSides Media Bias Rating: Lean Left   \n",
       "2  AllSides Media Bias Rating: Lean Left   \n",
       "3  AllSides Media Bias Rating: Lean Left   \n",
       "4  AllSides Media Bias Rating: Lean Left   \n",
       "\n",
       "                                         Subject_Tag           Date  \\\n",
       "0  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "1  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "2  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "3  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "4  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "\n",
       "                                           spacy_lg  \\\n",
       "0  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "1  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "2  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "3  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "4  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "\n",
       "                                       spacy_lg_dict  \n",
       "0  {'text': 'COVID', 'text_low': 'covid', 'pos': ...  \n",
       "1  {'text': 'Deaths', 'text_low': 'deaths', 'pos'...  \n",
       "2  {'text': 'Continue', 'text_low': 'continue', '...  \n",
       "3  {'text': 'Decline', 'text_low': 'decline', 'po...  \n",
       "4  {'text': 'in', 'text_low': 'in', 'pos': 'ADP',...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ungrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df_ungrouped['spacy_lg_dict'][0].keys())\n",
    "\n",
    "for feat in features:\n",
    "    df_ungrouped[feat] = df_ungrouped[\"spacy_lg_dict\"].apply(lambda x: x[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Subject_Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>spacy_lg</th>\n",
       "      <th>spacy_lg_dict</th>\n",
       "      <th>text</th>\n",
       "      <th>text_low</th>\n",
       "      <th>...</th>\n",
       "      <th>assertives_c3</th>\n",
       "      <th>factives_c3</th>\n",
       "      <th>report_verbs_c3</th>\n",
       "      <th>hedges_c3</th>\n",
       "      <th>implicatives_c3</th>\n",
       "      <th>assertives_c4</th>\n",
       "      <th>factives_c4</th>\n",
       "      <th>report_verbs_c4</th>\n",
       "      <th>hedges_c4</th>\n",
       "      <th>implicatives_c4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'COVID', 'text_low': 'covid', 'pos': ...</td>\n",
       "      <td>COVID</td>\n",
       "      <td>covid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Deaths', 'text_low': 'deaths', 'pos'...</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>deaths</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Continue', 'text_low': 'continue', '...</td>\n",
       "      <td>Continue</td>\n",
       "      <td>continue</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Decline', 'text_low': 'decline', 'po...</td>\n",
       "      <td>Decline</td>\n",
       "      <td>decline</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'in', 'text_low': 'in', 'pos': 'ADP',...</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Source                                  Headline  \\\n",
       "0  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "1  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "2  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "3  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "4  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "\n",
       "                                                Text  \\\n",
       "0  In a sign that the coronavirus pandemic is beg...   \n",
       "1  In a sign that the coronavirus pandemic is beg...   \n",
       "2  In a sign that the coronavirus pandemic is beg...   \n",
       "3  In a sign that the coronavirus pandemic is beg...   \n",
       "4  In a sign that the coronavirus pandemic is beg...   \n",
       "\n",
       "                                    Bias  \\\n",
       "0  AllSides Media Bias Rating: Lean Left   \n",
       "1  AllSides Media Bias Rating: Lean Left   \n",
       "2  AllSides Media Bias Rating: Lean Left   \n",
       "3  AllSides Media Bias Rating: Lean Left   \n",
       "4  AllSides Media Bias Rating: Lean Left   \n",
       "\n",
       "                                         Subject_Tag           Date  \\\n",
       "0  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "1  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "2  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "3  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "4  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "\n",
       "                                           spacy_lg  \\\n",
       "0  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "1  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "2  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "3  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "4  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "\n",
       "                                       spacy_lg_dict      text  text_low  ...  \\\n",
       "0  {'text': 'COVID', 'text_low': 'covid', 'pos': ...     COVID     covid  ...   \n",
       "1  {'text': 'Deaths', 'text_low': 'deaths', 'pos'...    Deaths    deaths  ...   \n",
       "2  {'text': 'Continue', 'text_low': 'continue', '...  Continue  continue  ...   \n",
       "3  {'text': 'Decline', 'text_low': 'decline', 'po...   Decline   decline  ...   \n",
       "4  {'text': 'in', 'text_low': 'in', 'pos': 'ADP',...        in        in  ...   \n",
       "\n",
       "  assertives_c3 factives_c3 report_verbs_c3 hedges_c3 implicatives_c3  \\\n",
       "0             0           0               0         0               0   \n",
       "1             0           0               0         0               0   \n",
       "2             0           0               0         0               0   \n",
       "3             0           0               0         0               0   \n",
       "4             0           0               0         0               0   \n",
       "\n",
       "  assertives_c4  factives_c4  report_verbs_c4  hedges_c4 implicatives_c4  \n",
       "0             0            0                0          0               0  \n",
       "1             0            0                0          0               0  \n",
       "2             0            0                0          0               0  \n",
       "3             0            0                0          0               0  \n",
       "4             0            0                0          0               0  \n",
       "\n",
       "[5 rows x 155 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ungrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ungrouped['liwc_c1'] = [[] if x is x==None else x for x in df_ungrouped['liwc_c1']]\n",
    "df_ungrouped['liwc_c2'] = [[] if x is x==None else x for x in df_ungrouped['liwc_c2']]\n",
    "df_ungrouped['liwc_c3'] = [[] if x is x==None else x for x in df_ungrouped['liwc_c3']]\n",
    "df_ungrouped['liwc_c4'] = [[] if x is x==None else x for x in df_ungrouped['liwc_c4']]\n",
    "\n",
    "# expand liwc to binary features\n",
    "for i in range(len(liwc_codes.columns)):\n",
    "    # for token\n",
    "    df_ungrouped[liwc_codes[i].loc[1]] = df_ungrouped.liwc.apply(lambda x: 1 if liwc_codes[i].loc[0] in x else 0)\n",
    "    # for c1\n",
    "    c1 = liwc_codes[i].loc[1] + '_c1'\n",
    "    df_ungrouped[c1] = df_ungrouped.liwc_c1.apply(lambda x: 1 if liwc_codes[i].loc[0] in x else 0)\n",
    "    # for c2\n",
    "    c2 = liwc_codes[i].loc[1] + '_c2'\n",
    "    df_ungrouped[c2] = df_ungrouped.liwc_c2.apply(lambda x: 1 if liwc_codes[i].loc[0] in x else 0)\n",
    "    # for c3\n",
    "    c3 = liwc_codes[i].loc[1] + '_c3'\n",
    "    df_ungrouped[c3] = df_ungrouped.liwc_c3.apply(lambda x: 1 if liwc_codes[i].loc[0] in x else 0)\n",
    "    # for c4\n",
    "    c4 = liwc_codes[i].loc[1] + '_c4'\n",
    "    df_ungrouped[c4] = df_ungrouped.liwc_c4.apply(lambda x: 1 if liwc_codes[i].loc[0] in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 73)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(liwc_codes.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['function (Function Words)',\n",
       " 'pronoun (Pronouns)',\n",
       " 'ppron (Personal Pronouns)',\n",
       " 'i (I)',\n",
       " 'we (We)',\n",
       " 'you (You)',\n",
       " 'shehe (SheHe)',\n",
       " 'they (They)',\n",
       " 'ipron (Impersonal Pronouns)',\n",
       " 'article (Articles)',\n",
       " 'prep (Prepositions)',\n",
       " 'auxverb (Auxiliary Verbs)',\n",
       " 'adverb (Adverbs)',\n",
       " 'conj (Conjunctions)',\n",
       " 'negate (Negations)',\n",
       " 'verb (Verbs)',\n",
       " 'adj (Adjectives)',\n",
       " 'compare (Comparisons)',\n",
       " 'interrog (Interrogatives)',\n",
       " 'number (Numbers)',\n",
       " 'quant (Quantifiers)',\n",
       " 'affect (Affect)',\n",
       " 'posemo (Positive Emotions)',\n",
       " 'negemo (Negative Emotions)',\n",
       " 'anx (Anx)',\n",
       " 'anger (Anger)',\n",
       " 'sad (Sad)',\n",
       " 'social (Social)',\n",
       " 'family (Family)',\n",
       " 'friend (Friends)',\n",
       " 'female (Female)',\n",
       " 'male (Male)',\n",
       " 'cogproc (Cognitive Processes)',\n",
       " 'insight (Insight)',\n",
       " 'cause (Causal)',\n",
       " 'discrep (Discrepancies)',\n",
       " 'tentat (Tentative)',\n",
       " 'certain (Certainty)',\n",
       " 'differ (Differentiation)',\n",
       " 'percept (Perceptual Processes)',\n",
       " 'see (See)',\n",
       " 'hear (Hear)',\n",
       " 'feel (Feel)',\n",
       " 'bio (Biological Processes)',\n",
       " 'body (Body)',\n",
       " 'health (Health)',\n",
       " 'sexual (Sexual)',\n",
       " 'ingest (Ingest)',\n",
       " 'drives (Drives)',\n",
       " 'affiliation (Affiliation)',\n",
       " 'achieve (Achievement)',\n",
       " 'power (Power)',\n",
       " 'reward (Reward)',\n",
       " 'risk (Risk)',\n",
       " 'focuspast (Past Focus)',\n",
       " 'focuspresent (Present Focus)',\n",
       " 'focusfuture (Future Focus)',\n",
       " 'relativ (Relativity)',\n",
       " 'motion (Motion)',\n",
       " 'space (Space)',\n",
       " 'time (Time)',\n",
       " 'work (Work)',\n",
       " 'leisure (Leisure)',\n",
       " 'home (Home)',\n",
       " 'money (Money)',\n",
       " 'relig (Religion)',\n",
       " 'death (Death)',\n",
       " 'informal (Informal Language)',\n",
       " 'swear (Swear)',\n",
       " 'netspeak (Netspeak)',\n",
       " 'assent (Assent)',\n",
       " 'nonflu (Nonfluencies)',\n",
       " 'filler (Filler Words)']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(liwc_codes.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_token = pd.get_dummies(df_ungrouped.pos, prefix='pos')\n",
    "pos_c1 = pd.get_dummies(df_ungrouped.pos_c1, prefix='pos')\n",
    "pos_c2 = pd.get_dummies(df_ungrouped.pos_c2, prefix='pos')\n",
    "pos_c3 = pd.get_dummies(df_ungrouped.pos_c3, prefix='pos')\n",
    "pos_c4 = pd.get_dummies(df_ungrouped.pos_c4, prefix='pos')\n",
    "\n",
    "pos_c1 = pos_c1.add_suffix('_c1')\n",
    "pos_c2 = pos_c2.add_suffix('_c2')\n",
    "pos_c3 = pos_c3.add_suffix('_c3')\n",
    "pos_c4 = pos_c4.add_suffix('_c4')\n",
    "\n",
    "df_ungrouped = pd.merge(df_ungrouped, pos_token, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, pos_c1, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, pos_c2, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, pos_c3, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, pos_c4, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_token = pd.get_dummies(df_ungrouped.dep, prefix='dep')\n",
    "dep_c1 = pd.get_dummies(df_ungrouped.dep_c1, prefix='dep')\n",
    "dep_c2 = pd.get_dummies(df_ungrouped.dep_c2, prefix='dep')\n",
    "dep_c3 = pd.get_dummies(df_ungrouped.dep_c3, prefix='dep')\n",
    "dep_c4 = pd.get_dummies(df_ungrouped.dep_c4, prefix='dep')\n",
    "dep_c1 = dep_c1.add_suffix('_c1')\n",
    "dep_c2 = dep_c2.add_suffix('_c2')\n",
    "dep_c3 = dep_c3.add_suffix('_c3')\n",
    "dep_c4 = dep_c4.add_suffix('_c4')\n",
    "\n",
    "df_ungrouped = pd.merge(df_ungrouped, dep_token, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, dep_c1, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, dep_c2, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, dep_c3, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, dep_c4, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_token = pd.get_dummies(df_ungrouped.ne_label, prefix='ne')\n",
    "ne_c1 = pd.get_dummies(df_ungrouped.ne_label_c1, prefix='ne')\n",
    "ne_c2 = pd.get_dummies(df_ungrouped.ne_label_c2, prefix='ne')\n",
    "ne_c3 = pd.get_dummies(df_ungrouped.ne_label_c3, prefix='ne')\n",
    "ne_c4 = pd.get_dummies(df_ungrouped.ne_label_c4, prefix='ne')\n",
    "ne_c1 = ne_c1.add_suffix('_c1')\n",
    "ne_c2 = ne_c2.add_suffix('_c2')\n",
    "ne_c3 = ne_c3.add_suffix('_c3')\n",
    "ne_c4 = ne_c4.add_suffix('_c4')\n",
    "\n",
    "df_ungrouped = pd.merge(df_ungrouped, ne_token, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, ne_c1, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, ne_c2, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, ne_c3, left_index=True, right_index=True, how='left')\n",
    "df_ungrouped = pd.merge(df_ungrouped, ne_c4, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features_for_training = [\n",
    " 'negative',\n",
    " 'positive',\n",
    " 'bias_lex_h',\n",
    "'bias_lex_r',\n",
    " 'assertives',\n",
    " 'factives',\n",
    " 'report_verbs',\n",
    " 'implicatives',\n",
    " 'hedges',\n",
    " 'affect (Affect)',\n",
    " 'posemo (Positive Emotions)',\n",
    " 'negemo (Negative Emotions)',\n",
    " 'anx (Anx)',\n",
    " 'anger (Anger)',\n",
    " 'sad (Sad)',\n",
    " 'social (Social)',\n",
    " 'family (Family)',\n",
    " 'friend (Friends)',\n",
    " 'female (Female)',\n",
    " 'male (Male)',\n",
    " 'cogproc (Cognitive Processes)',\n",
    " 'insight (Insight)',\n",
    " 'cause (Causal)',\n",
    " 'discrep (Discrepancies)',\n",
    " 'tentat (Tentative)',\n",
    " 'certain (Certainty)',\n",
    " 'differ (Differentiation)',\n",
    " 'percept (Perceptual Processes)',\n",
    " 'see (See)',\n",
    " 'hear (Hear)',\n",
    " 'feel (Feel)',\n",
    " 'bio (Biological Processes)',\n",
    " 'body (Body)',\n",
    " 'health (Health)',\n",
    " 'sexual (Sexual)',\n",
    " 'ingest (Ingest)',\n",
    " 'drives (Drives)',\n",
    " 'affiliation (Affiliation)',\n",
    " 'achieve (Achievement)',\n",
    " 'power (Power)',\n",
    " 'reward (Reward)',\n",
    " 'risk (Risk)',\n",
    " 'focuspast (Past Focus)',\n",
    " 'focuspresent (Present Focus)',\n",
    " 'focusfuture (Future Focus)',\n",
    " 'relativ (Relativity)',\n",
    " 'motion (Motion)',\n",
    " 'space (Space)',\n",
    " 'time (Time)',\n",
    " 'work (Work)',\n",
    " 'leisure (Leisure)',\n",
    " 'home (Home)',\n",
    " 'money (Money)',\n",
    " 'relig (Religion)',\n",
    " 'death (Death)',\n",
    " 'informal (Informal Language)',\n",
    " 'swear (Swear)',\n",
    " 'netspeak (Netspeak)',\n",
    " 'assent (Assent)',\n",
    " 'nonflu (Nonfluencies)',\n",
    " 'filler (Filler Words)',\n",
    " 'pos_ADJ',\n",
    " 'pos_ADP',\n",
    " 'pos_ADV',\n",
    " 'pos_AUX',\n",
    " 'pos_DET',\n",
    " 'pos_INTJ',\n",
    " 'pos_NOUN',\n",
    " 'pos_PRON',\n",
    " 'pos_PROPN',\n",
    " 'pos_SCONJ',\n",
    " 'pos_VERB',\n",
    " 'pos_X',\n",
    " 'dep_ROOT',\n",
    " 'dep_acl',\n",
    " 'dep_acomp',\n",
    " 'dep_advcl',\n",
    " 'dep_advmod',\n",
    " 'dep_agent',\n",
    " 'dep_amod',\n",
    " 'dep_appos',\n",
    " 'dep_attr',\n",
    " 'dep_aux',\n",
    " 'dep_auxpass',\n",
    " #'dep_case',\n",
    " 'dep_cc',\n",
    " 'dep_ccomp',\n",
    " 'dep_compound',\n",
    " 'dep_conj',\n",
    " 'dep_csubj',\n",
    " 'dep_dative',\n",
    " 'dep_dep',\n",
    " 'dep_det',\n",
    " 'dep_dobj',\n",
    " 'dep_expl',\n",
    " 'dep_intj',\n",
    " 'dep_mark',\n",
    " #'dep_meta',\n",
    " 'dep_neg',\n",
    " 'dep_nmod',\n",
    " 'dep_npadvmod',\n",
    " 'dep_nsubj',\n",
    " 'dep_nsubjpass',\n",
    " 'dep_nummod',\n",
    " 'dep_oprd',\n",
    " 'dep_parataxis',\n",
    " 'dep_pcomp',\n",
    " 'dep_pobj',\n",
    " 'dep_poss',\n",
    " 'dep_preconj',\n",
    " 'dep_predet',\n",
    " 'dep_prep',\n",
    " 'dep_prt',\n",
    " 'dep_punct',\n",
    " 'dep_quantmod',\n",
    " 'dep_relcl',\n",
    " 'dep_xcomp',\n",
    " 'ne_CARDINAL',\n",
    " 'ne_DATE',\n",
    " 'ne_EVENT',\n",
    " 'ne_FAC',\n",
    " 'ne_GPE',\n",
    " 'ne_LAW',\n",
    " 'ne_LOC',\n",
    " 'ne_MONEY',\n",
    " 'ne_NORP',\n",
    " 'ne_ORDINAL',\n",
    " 'ne_ORG',\n",
    " 'ne_PERCENT',\n",
    " 'ne_PERSON',\n",
    " 'ne_PRODUCT',\n",
    " 'ne_QUANTITY',\n",
    " 'ne_TIME',\n",
    " 'ne_WORK_OF_ART']\n",
    "\n",
    "# 'ne_LANGUAGE' - separately because for some reason ne_LANGUAGE_C1 and ne_LANGUAGE_C2 aren't in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in binary_features_for_training:\n",
    "    if feat not in df_ungrouped.columns:\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in binary_features_for_training:\n",
    "    new_feat = feat + '_context'\n",
    "    f1, f2, f3, f4 = feat + '_c1', feat + '_c2', feat + '_c3', feat + '_c4'\n",
    "    df_ungrouped[new_feat] = df_ungrouped.apply(lambda row: 1 if 1 in [row[f1],row[f2],row[f3],row[f4]] else 0,\n",
    "                                                axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ne_LANGUAGE' - separately because for some reason ne_LANGUAGE_C1 and ne_LANGUAGE_C2 aren't in the list\n",
    "new_feat = 'ne_LANGUAGE' + '_context'\n",
    "f3, f4 = 'ne_LANGUAGE' + '_c3', 'ne_LANGUAGE' + '_c4'\n",
    "df_ungrouped[new_feat] = df_ungrouped.apply(lambda row: 1 if 1 in [row[f3],row[f4]] else 0,\n",
    "                                            axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Subject_Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>spacy_lg</th>\n",
       "      <th>spacy_lg_dict</th>\n",
       "      <th>text</th>\n",
       "      <th>text_low</th>\n",
       "      <th>...</th>\n",
       "      <th>ne_NORP_context</th>\n",
       "      <th>ne_ORDINAL_context</th>\n",
       "      <th>ne_ORG_context</th>\n",
       "      <th>ne_PERCENT_context</th>\n",
       "      <th>ne_PERSON_context</th>\n",
       "      <th>ne_PRODUCT_context</th>\n",
       "      <th>ne_QUANTITY_context</th>\n",
       "      <th>ne_TIME_context</th>\n",
       "      <th>ne_WORK_OF_ART_context</th>\n",
       "      <th>ne_LANGUAGE_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'COVID', 'text_low': 'covid', 'pos': ...</td>\n",
       "      <td>COVID</td>\n",
       "      <td>covid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Deaths', 'text_low': 'deaths', 'pos'...</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>deaths</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Continue', 'text_low': 'continue', '...</td>\n",
       "      <td>Continue</td>\n",
       "      <td>continue</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Decline', 'text_low': 'decline', 'po...</td>\n",
       "      <td>Decline</td>\n",
       "      <td>decline</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'in', 'text_low': 'in', 'pos': 'ADP',...</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1027 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Source                                  Headline  \\\n",
       "0  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "1  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "2  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "3  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "4  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "\n",
       "                                                Text  \\\n",
       "0  In a sign that the coronavirus pandemic is beg...   \n",
       "1  In a sign that the coronavirus pandemic is beg...   \n",
       "2  In a sign that the coronavirus pandemic is beg...   \n",
       "3  In a sign that the coronavirus pandemic is beg...   \n",
       "4  In a sign that the coronavirus pandemic is beg...   \n",
       "\n",
       "                                    Bias  \\\n",
       "0  AllSides Media Bias Rating: Lean Left   \n",
       "1  AllSides Media Bias Rating: Lean Left   \n",
       "2  AllSides Media Bias Rating: Lean Left   \n",
       "3  AllSides Media Bias Rating: Lean Left   \n",
       "4  AllSides Media Bias Rating: Lean Left   \n",
       "\n",
       "                                         Subject_Tag           Date  \\\n",
       "0  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "1  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "2  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "3  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "4  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "\n",
       "                                           spacy_lg  \\\n",
       "0  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "1  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "2  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "3  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "4  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "\n",
       "                                       spacy_lg_dict      text  text_low  ...  \\\n",
       "0  {'text': 'COVID', 'text_low': 'covid', 'pos': ...     COVID     covid  ...   \n",
       "1  {'text': 'Deaths', 'text_low': 'deaths', 'pos'...    Deaths    deaths  ...   \n",
       "2  {'text': 'Continue', 'text_low': 'continue', '...  Continue  continue  ...   \n",
       "3  {'text': 'Decline', 'text_low': 'decline', 'po...   Decline   decline  ...   \n",
       "4  {'text': 'in', 'text_low': 'in', 'pos': 'ADP',...        in        in  ...   \n",
       "\n",
       "  ne_NORP_context ne_ORDINAL_context ne_ORG_context ne_PERCENT_context  \\\n",
       "0               0                  0              0                  0   \n",
       "1               0                  0              0                  0   \n",
       "2               0                  0              0                  0   \n",
       "3               0                  0              0                  0   \n",
       "4               0                  0              0                  0   \n",
       "\n",
       "  ne_PERSON_context ne_PRODUCT_context  ne_QUANTITY_context  ne_TIME_context  \\\n",
       "0                 1                  0                    0                0   \n",
       "1                 1                  0                    0                0   \n",
       "2                 1                  0                    0                0   \n",
       "3                 1                  0                    0                0   \n",
       "4                 1                  0                    0                0   \n",
       "\n",
       "   ne_WORK_OF_ART_context ne_LANGUAGE_context  \n",
       "0                       0                   0  \n",
       "1                       0                   0  \n",
       "2                       0                   0  \n",
       "3                       0                   0  \n",
       "4                       0                   0  \n",
       "\n",
       "[5 rows x 1027 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ungrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_ungrouped[['Headline', # not a feature for training\n",
    " 'Source', # not a feature for training\n",
    " 'Subject_Tag', # not a feature for training\n",
    " 'Bias', # not a feature for training\n",
    " #'num_sent', # not a feature for training\n",
    " 'Text', # not a feature for training\n",
    " 'Date',\n",
    " #'biased_words3', # not a feature for training\n",
    " #'biased_words4', # not a feature for training\n",
    " #'biased_words5', # not a feature for training\n",
    " 'text', # not a feature for training\n",
    " 'text_low', # not a feature for training\n",
    " 'pos', # not a feature for training\n",
    " 'lemma', # not a feature for training\n",
    " 'lemma_low', # not a feature for training\n",
    " 'tag', # not a feature for training\n",
    " 'dep', # not a feature for training\n",
    " 'is_stop', # not a feature for training\n",
    " 'glove_vec300_norm',\n",
    " 'order', # not a feature for training\n",
    " 'tf_idf',\n",
    " 'is_ne', # not a feature for training\n",
    " 'ne_label', # not a feature for training\n",
    " 'negative',\n",
    " 'positive',\n",
    " 'bias_lex_h',\n",
    "'bias_lex_r',\n",
    " 'assertives',\n",
    " 'factives',\n",
    " 'report_verbs',\n",
    " 'implicatives',\n",
    " 'hedges',\n",
    " 'affect (Affect)',\n",
    " 'posemo (Positive Emotions)',\n",
    " 'negemo (Negative Emotions)',\n",
    " 'anx (Anx)',\n",
    " 'anger (Anger)',\n",
    " 'sad (Sad)',\n",
    " 'social (Social)',\n",
    " 'family (Family)',\n",
    " 'friend (Friends)',\n",
    " 'female (Female)',\n",
    " 'male (Male)',\n",
    " 'cogproc (Cognitive Processes)',\n",
    " 'insight (Insight)',\n",
    " 'cause (Causal)',\n",
    " 'discrep (Discrepancies)',\n",
    " 'tentat (Tentative)',\n",
    " 'certain (Certainty)',\n",
    " 'differ (Differentiation)',\n",
    " 'percept (Perceptual Processes)',\n",
    " 'see (See)',\n",
    " 'hear (Hear)',\n",
    " 'feel (Feel)',\n",
    " 'bio (Biological Processes)',\n",
    " 'body (Body)',\n",
    " 'health (Health)',\n",
    " 'sexual (Sexual)',\n",
    " 'ingest (Ingest)',\n",
    " 'drives (Drives)',\n",
    " 'affiliation (Affiliation)',\n",
    " 'achieve (Achievement)',\n",
    " 'power (Power)',\n",
    " 'reward (Reward)',\n",
    " 'risk (Risk)',\n",
    " 'focuspast (Past Focus)',\n",
    " 'focuspresent (Present Focus)',\n",
    " 'focusfuture (Future Focus)',\n",
    " 'relativ (Relativity)',\n",
    " 'motion (Motion)',\n",
    " 'space (Space)',\n",
    " 'time (Time)',\n",
    " 'work (Work)',\n",
    " 'leisure (Leisure)',\n",
    " 'home (Home)',\n",
    " 'money (Money)',\n",
    " 'relig (Religion)',\n",
    " 'death (Death)',\n",
    " 'informal (Informal Language)',\n",
    " 'swear (Swear)',\n",
    " 'netspeak (Netspeak)',\n",
    " 'assent (Assent)',\n",
    " 'nonflu (Nonfluencies)',\n",
    " 'filler (Filler Words)',\n",
    " 'pos_ADJ',\n",
    " 'pos_ADP',\n",
    " 'pos_ADV',\n",
    " 'pos_AUX',\n",
    " 'pos_DET',\n",
    " 'pos_INTJ',\n",
    " 'pos_NOUN',\n",
    " 'pos_PRON',\n",
    " 'pos_PROPN',\n",
    " 'pos_SCONJ',\n",
    " 'pos_VERB',\n",
    " 'pos_X',\n",
    " 'dep_ROOT',\n",
    " 'dep_acl',\n",
    " 'dep_acomp',\n",
    " 'dep_advcl',\n",
    " 'dep_advmod',\n",
    " 'dep_agent',\n",
    " 'dep_amod',\n",
    " 'dep_appos',\n",
    " 'dep_attr',\n",
    " 'dep_aux',\n",
    " 'dep_auxpass',\n",
    " #'dep_case',\n",
    " 'dep_cc',\n",
    " 'dep_ccomp',\n",
    " 'dep_compound',\n",
    " 'dep_conj',\n",
    " 'dep_csubj',\n",
    " 'dep_dative',\n",
    " 'dep_dep',\n",
    " 'dep_det',\n",
    " 'dep_dobj',\n",
    " 'dep_expl',\n",
    " 'dep_intj',\n",
    " 'dep_mark',\n",
    " #'dep_meta',\n",
    " 'dep_neg',\n",
    " 'dep_nmod',\n",
    " 'dep_npadvmod',\n",
    " 'dep_nsubj',\n",
    " 'dep_nsubjpass',\n",
    " 'dep_nummod',\n",
    " 'dep_oprd',\n",
    " 'dep_parataxis',\n",
    " 'dep_pcomp',\n",
    " 'dep_pobj',\n",
    " 'dep_poss',\n",
    " 'dep_preconj',\n",
    " 'dep_predet',\n",
    " 'dep_prep',\n",
    " 'dep_prt',\n",
    " 'dep_punct',\n",
    " 'dep_quantmod',\n",
    " 'dep_relcl',\n",
    " 'dep_xcomp',\n",
    " 'ne_CARDINAL',\n",
    " 'ne_DATE',\n",
    " 'ne_EVENT',\n",
    " 'ne_FAC',\n",
    " 'ne_GPE',\n",
    " 'ne_LANGUAGE',\n",
    " 'ne_LAW',\n",
    " 'ne_LOC',\n",
    " 'ne_MONEY',\n",
    " 'ne_NORP',\n",
    " 'ne_ORDINAL',\n",
    " 'ne_ORG',\n",
    " 'ne_PERCENT',\n",
    " 'ne_PERSON',\n",
    " 'ne_PRODUCT',\n",
    " 'ne_QUANTITY',\n",
    " 'ne_TIME',\n",
    " 'ne_WORK_OF_ART',\n",
    " 'negative_context',\n",
    " 'positive_context',\n",
    " 'bias_lex_h_context',\n",
    "'bias_lex_r_context',\n",
    " 'assertives_context',\n",
    " 'factives_context',\n",
    " 'report_verbs_context',\n",
    " 'implicatives_context',\n",
    " 'hedges_context',\n",
    "'affect (Affect)_context',\n",
    " 'posemo (Positive Emotions)_context',\n",
    " 'negemo (Negative Emotions)_context',\n",
    " 'anx (Anx)_context',\n",
    " 'anger (Anger)_context',\n",
    " 'sad (Sad)_context',\n",
    " 'social (Social)_context',\n",
    " 'family (Family)_context',\n",
    " 'friend (Friends)_context',\n",
    " 'female (Female)_context',\n",
    " 'male (Male)_context',\n",
    " 'cogproc (Cognitive Processes)_context',\n",
    " 'insight (Insight)_context',\n",
    " 'cause (Causal)_context',\n",
    " 'discrep (Discrepancies)_context',\n",
    " 'tentat (Tentative)_context',\n",
    " 'certain (Certainty)_context',\n",
    " 'differ (Differentiation)_context',\n",
    " 'percept (Perceptual Processes)_context',\n",
    " 'see (See)_context',\n",
    " 'hear (Hear)_context',\n",
    " 'feel (Feel)_context',\n",
    " 'bio (Biological Processes)_context',\n",
    " 'body (Body)_context',\n",
    " 'health (Health)_context',\n",
    " 'sexual (Sexual)_context',\n",
    " 'ingest (Ingest)_context',\n",
    " 'drives (Drives)_context',\n",
    " 'affiliation (Affiliation)_context',\n",
    " 'achieve (Achievement)_context',\n",
    " 'power (Power)_context',\n",
    " 'reward (Reward)_context',\n",
    " 'risk (Risk)_context',\n",
    " 'focuspast (Past Focus)_context',\n",
    " 'focuspresent (Present Focus)_context',\n",
    " 'focusfuture (Future Focus)_context',\n",
    " 'relativ (Relativity)_context',\n",
    " 'motion (Motion)_context',\n",
    " 'space (Space)_context',\n",
    " 'time (Time)_context',\n",
    " 'work (Work)_context',\n",
    " 'leisure (Leisure)_context',\n",
    " 'home (Home)_context',\n",
    " 'money (Money)_context',\n",
    " 'relig (Religion)_context',\n",
    " 'death (Death)_context',\n",
    " 'informal (Informal Language)_context',\n",
    " 'swear (Swear)_context',\n",
    " 'netspeak (Netspeak)_context',\n",
    " 'assent (Assent)_context',\n",
    " 'nonflu (Nonfluencies)_context',\n",
    " 'filler (Filler Words)_context',\n",
    " 'pos_ADJ_context',\n",
    " 'pos_ADP_context',\n",
    " 'pos_ADV_context',\n",
    " 'pos_AUX_context',\n",
    " 'pos_DET_context',\n",
    " 'pos_INTJ_context',\n",
    " 'pos_NOUN_context',\n",
    " 'pos_PRON_context',\n",
    " 'pos_PROPN_context',\n",
    " 'pos_SCONJ_context',\n",
    " 'pos_VERB_context',\n",
    " 'pos_X_context',\n",
    " 'dep_ROOT_context',\n",
    " 'dep_acl_context',\n",
    " 'dep_acomp_context',\n",
    " 'dep_advcl_context',\n",
    " 'dep_advmod_context',\n",
    " 'dep_agent_context',\n",
    " 'dep_amod_context',\n",
    " 'dep_appos_context',\n",
    " 'dep_attr_context',\n",
    " 'dep_aux_context',\n",
    " 'dep_auxpass_context',\n",
    " #'dep_case_context',\n",
    " 'dep_cc_context',\n",
    " 'dep_ccomp_context',\n",
    " 'dep_compound_context',\n",
    " 'dep_conj_context',\n",
    " 'dep_csubj_context',\n",
    " 'dep_dative_context',\n",
    " 'dep_dep_context',\n",
    " 'dep_det_context',\n",
    " 'dep_dobj_context',\n",
    " 'dep_expl_context',\n",
    " 'dep_intj_context',\n",
    " 'dep_mark_context',\n",
    " #'dep_meta_context',\n",
    " 'dep_neg_context',\n",
    " 'dep_nmod_context',\n",
    " 'dep_npadvmod_context',\n",
    " 'dep_nsubj_context',\n",
    " 'dep_nsubjpass_context',\n",
    " 'dep_nummod_context',\n",
    " 'dep_oprd_context',\n",
    " 'dep_parataxis_context',\n",
    " 'dep_pcomp_context',\n",
    " 'dep_pobj_context',\n",
    " 'dep_poss_context',\n",
    " 'dep_preconj_context',\n",
    " 'dep_predet_context',\n",
    " 'dep_prep_context',\n",
    " 'dep_prt_context',\n",
    " 'dep_punct_context',\n",
    " 'dep_quantmod_context',\n",
    " 'dep_relcl_context',\n",
    " 'dep_xcomp_context',\n",
    " 'ne_CARDINAL_context',\n",
    " 'ne_DATE_context',\n",
    " 'ne_EVENT_context',\n",
    " 'ne_FAC_context',\n",
    " 'ne_GPE_context',\n",
    " 'ne_LAW_context',\n",
    " 'ne_LOC_context',\n",
    " 'ne_MONEY_context',\n",
    " 'ne_NORP_context',\n",
    " 'ne_ORDINAL_context',\n",
    " 'ne_ORG_context',\n",
    " 'ne_PERCENT_context',\n",
    " 'ne_PERSON_context',\n",
    " 'ne_PRODUCT_context',\n",
    " 'ne_QUANTITY_context',\n",
    " 'ne_TIME_context',\n",
    " 'ne_WORK_OF_ART_context',\n",
    " 'ne_LANGUAGE_context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations: 123637\n",
      "number of unique words: 14115\n"
     ]
    }
   ],
   "source": [
    "df_final = df_clean[df_clean['is_stop']==False]\n",
    "print('number of observations:', len(df_final))\n",
    "print('number of unique words:', len(set(df_final['text_low'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_excel('df_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Label\"] = df.Bias.apply(lambda x: 1 if x == \"AllSides Media Bias Rating: Left\" or x == \"AllSides Media Bias Rating: Right\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased headlines: 27.0 %\n",
      "Neutral headlines: 73.0 %\n"
     ]
    }
   ],
   "source": [
    "print('Biased headlines:',round(len(df[df['Label']==1])/len(df)*100,0),'%')\n",
    "print('Neutral headlines:',round(len(df[df['Label']==0])/len(df)*100,0),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Subject_Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>spacy_lg</th>\n",
       "      <th>spacy_lg_dict</th>\n",
       "      <th>Cut</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>[{'text': 'COVID', 'text_low': 'covid', 'pos':...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wall Street Journal (News)</td>\n",
       "      <td>Vaccines Appear to Be Slowing Spread of Covid-...</td>\n",
       "      <td>Vaccines appear to be starting to curb new Cov...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(Vaccines, Appear, to, Be, Slowing, Spread, of...</td>\n",
       "      <td>[{'text': 'Vaccines', 'text_low': 'vaccines', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington Examiner</td>\n",
       "      <td>Pandemic retreat signals vaccines are working</td>\n",
       "      <td>COVID-19 cases and hospitalizations in the Uni...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(Pandemic, retreat, signals, vaccines, are, wo...</td>\n",
       "      <td>[{'text': 'Pandemic', 'text_low': 'pandemic', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Epoch Times</td>\n",
       "      <td>NYT, Washington Post, NBC Retract Incorrect Re...</td>\n",
       "      <td>The New York Times, The Washington Post, and N...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "      <td>(NYT, ,, Washington, Post, ,, NBC, Retract, In...</td>\n",
       "      <td>[{'text': 'NYT', 'text_low': 'nyt', 'pos': 'PR...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>New York Times, WaPo, NBC retract reports abou...</td>\n",
       "      <td>The New York Times, The Washington Post and NB...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "      <td>(New, York, Times, ,, WaPo, ,, NBC, retract, r...</td>\n",
       "      <td>[{'text': 'New', 'text_low': 'new', 'pos': 'PR...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Source  \\\n",
       "0    U.S. News & World Report   \n",
       "1  Wall Street Journal (News)   \n",
       "2         Washington Examiner   \n",
       "3             The Epoch Times   \n",
       "4                    The Hill   \n",
       "\n",
       "                                            Headline  \\\n",
       "0           COVID Deaths Continue to Decline in U.S.   \n",
       "1  Vaccines Appear to Be Slowing Spread of Covid-...   \n",
       "2      Pandemic retreat signals vaccines are working   \n",
       "3  NYT, Washington Post, NBC Retract Incorrect Re...   \n",
       "4  New York Times, WaPo, NBC retract reports abou...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  In a sign that the coronavirus pandemic is beg...   \n",
       "1  Vaccines appear to be starting to curb new Cov...   \n",
       "2  COVID-19 cases and hospitalizations in the Uni...   \n",
       "3  The New York Times, The Washington Post, and N...   \n",
       "4  The New York Times, The Washington Post and NB...   \n",
       "\n",
       "                                     Bias  \\\n",
       "0   AllSides Media Bias Rating: Lean Left   \n",
       "1      AllSides Media Bias Rating: Center   \n",
       "2  AllSides Media Bias Rating: Lean Right   \n",
       "3  AllSides Media Bias Rating: Lean Right   \n",
       "4      AllSides Media Bias Rating: Center   \n",
       "\n",
       "                                         Subject_Tag           Date  \\\n",
       "0  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "1  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "2  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "3  Media Industry, Media Bias, New York Times, Wa...  May 2nd, 2021   \n",
       "4  Media Industry, Media Bias, New York Times, Wa...  May 2nd, 2021   \n",
       "\n",
       "                                            spacy_lg  \\\n",
       "0   (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "1  (Vaccines, Appear, to, Be, Slowing, Spread, of...   \n",
       "2  (Pandemic, retreat, signals, vaccines, are, wo...   \n",
       "3  (NYT, ,, Washington, Post, ,, NBC, Retract, In...   \n",
       "4  (New, York, Times, ,, WaPo, ,, NBC, retract, r...   \n",
       "\n",
       "                                       spacy_lg_dict Cut  Label  \n",
       "0  [{'text': 'COVID', 'text_low': 'covid', 'pos':...   1      0  \n",
       "1  [{'text': 'Vaccines', 'text_low': 'vaccines', ...   1      0  \n",
       "2  [{'text': 'Pandemic', 'text_low': 'pandemic', ...   1      0  \n",
       "3  [{'text': 'NYT', 'text_low': 'nyt', 'pos': 'PR...   1      0  \n",
       "4  [{'text': 'New', 'text_low': 'new', 'pos': 'PR...   1      0  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Cut'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-07196cd33c7c>\", line 1, in <module>\n",
      "    df.to_excel(\"Headlines_Tokenized.xlsx\")\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 2023, in to_excel\n",
      "    formatter.write(\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\", line 734, in write\n",
      "    writer.write_cells(\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\", line 213, in write_cells\n",
      "    val, fmt = self._value_with_fmt(cell.val)\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\", line 755, in _value_with_fmt\n",
      "    val = str(val)\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\", line 1402, in _array_repr_implementation\n",
      "    lst = array2string(arr, max_line_width, precision, suppress_small,\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\", line 712, in array2string\n",
      "    return _array2string(a, options, separator, prefix)\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\", line 484, in wrapper\n",
      "    return f(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\", line 517, in _array2string\n",
      "    lst = _formatArray(a, format_function, options['linewidth'],\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\", line 838, in _formatArray\n",
      "    return recurser(index=(),\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\", line 794, in recurser\n",
      "    word = recurser(index + (-i,), next_hanging_indent, next_width)\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\", line 748, in recurser\n",
      "    return format_function(a[index])\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\Jack\\anaconda3\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-07196cd33c7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Headlines_Tokenized.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)\u001b[0m\n\u001b[0;32m   2022\u001b[0m         )\n\u001b[1;32m-> 2023\u001b[1;33m         formatter.write(\n\u001b[0m\u001b[0;32m   2024\u001b[0m             \u001b[0mexcel_writer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)\u001b[0m\n\u001b[0;32m    733\u001b[0m         \u001b[0mformatted_cells\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_formatted_cells\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m         writer.write_cells(\n\u001b[0m\u001b[0;32m    735\u001b[0m             \u001b[0mformatted_cells\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\u001b[0m in \u001b[0;36mwrite_cells\u001b[1;34m(self, cells, sheet_name, startrow, startcol, freeze_panes)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcells\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m             \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_with_fmt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m_value_with_fmt\u001b[1;34m(self, val)\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m             \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36m_array_repr_implementation\u001b[1;34m(arr, max_line_width, precision, suppress_small, array2string)\u001b[0m\n\u001b[0;32m   1401\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1402\u001b[1;33m         lst = array2string(arr, max_line_width, precision, suppress_small,\n\u001b[0m\u001b[0;32m   1403\u001b[0m                            ', ', prefix, suffix=suffix)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36marray2string\u001b[1;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, **kwarg)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_array2string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36m_array2string\u001b[1;34m(a, options, separator, prefix)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m     lst = _formatArray(a, format_function, options['linewidth'],\n\u001b[0m\u001b[0;32m    518\u001b[0m                        \u001b[0mnext_line_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'edgeitems'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36m_formatArray\u001b[1;34m(a, format_function, line_width, next_line_prefix, separator, edge_items, summary_insert, legacy)\u001b[0m\n\u001b[0;32m    837\u001b[0m         \u001b[1;31m# invoke the recursive part with an initial index and prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m         return recurser(index=(),\n\u001b[0m\u001b[0;32m    839\u001b[0m                         \u001b[0mhanging_indent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext_line_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36mrecurser\u001b[1;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrailing_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m                 \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecurser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_hanging_indent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m                 s, line = _extendLine(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36mrecurser\u001b[1;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxes_left\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mformat_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2043\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2044\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2047\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1433\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1435\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1436\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1336\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1192\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1193\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "df.to_excel(\"Headlines_Tokenized.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Subject_Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>spacy_lg</th>\n",
       "      <th>spacy_lg_dict</th>\n",
       "      <th>Label_Bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>[{'text': 'COVID', 'text_low': 'covid', 'pos':...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wall Street Journal (News)</td>\n",
       "      <td>Vaccines Appear to Be Slowing Spread of Covid-...</td>\n",
       "      <td>Vaccines appear to be starting to curb new Cov...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(Vaccines, Appear, to, Be, Slowing, Spread, of...</td>\n",
       "      <td>[{'text': 'Vaccines', 'text_low': 'vaccines', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington Examiner</td>\n",
       "      <td>Pandemic retreat signals vaccines are working</td>\n",
       "      <td>COVID-19 cases and hospitalizations in the Uni...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(Pandemic, retreat, signals, vaccines, are, wo...</td>\n",
       "      <td>[{'text': 'Pandemic', 'text_low': 'pandemic', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Epoch Times</td>\n",
       "      <td>NYT, Washington Post, NBC Retract Incorrect Re...</td>\n",
       "      <td>The New York Times, The Washington Post, and N...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Right</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "      <td>(NYT, ,, Washington, Post, ,, NBC, Retract, In...</td>\n",
       "      <td>[{'text': 'NYT', 'text_low': 'nyt', 'pos': 'PR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>New York Times, WaPo, NBC retract reports abou...</td>\n",
       "      <td>The New York Times, The Washington Post and NB...</td>\n",
       "      <td>AllSides Media Bias Rating: Center</td>\n",
       "      <td>Media Industry, Media Bias, New York Times, Wa...</td>\n",
       "      <td>May 2nd, 2021</td>\n",
       "      <td>(New, York, Times, ,, WaPo, ,, NBC, retract, r...</td>\n",
       "      <td>[{'text': 'New', 'text_low': 'new', 'pos': 'PR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Source  \\\n",
       "0    U.S. News & World Report   \n",
       "1  Wall Street Journal (News)   \n",
       "2         Washington Examiner   \n",
       "3             The Epoch Times   \n",
       "4                    The Hill   \n",
       "\n",
       "                                            Headline  \\\n",
       "0           COVID Deaths Continue to Decline in U.S.   \n",
       "1  Vaccines Appear to Be Slowing Spread of Covid-...   \n",
       "2      Pandemic retreat signals vaccines are working   \n",
       "3  NYT, Washington Post, NBC Retract Incorrect Re...   \n",
       "4  New York Times, WaPo, NBC retract reports abou...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  In a sign that the coronavirus pandemic is beg...   \n",
       "1  Vaccines appear to be starting to curb new Cov...   \n",
       "2  COVID-19 cases and hospitalizations in the Uni...   \n",
       "3  The New York Times, The Washington Post, and N...   \n",
       "4  The New York Times, The Washington Post and NB...   \n",
       "\n",
       "                                     Bias  \\\n",
       "0   AllSides Media Bias Rating: Lean Left   \n",
       "1      AllSides Media Bias Rating: Center   \n",
       "2  AllSides Media Bias Rating: Lean Right   \n",
       "3  AllSides Media Bias Rating: Lean Right   \n",
       "4      AllSides Media Bias Rating: Center   \n",
       "\n",
       "                                         Subject_Tag           Date  \\\n",
       "0  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "1  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "2  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "3  Media Industry, Media Bias, New York Times, Wa...  May 2nd, 2021   \n",
       "4  Media Industry, Media Bias, New York Times, Wa...  May 2nd, 2021   \n",
       "\n",
       "                                            spacy_lg  \\\n",
       "0   (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "1  (Vaccines, Appear, to, Be, Slowing, Spread, of...   \n",
       "2  (Pandemic, retreat, signals, vaccines, are, wo...   \n",
       "3  (NYT, ,, Washington, Post, ,, NBC, Retract, In...   \n",
       "4  (New, York, Times, ,, WaPo, ,, NBC, retract, r...   \n",
       "\n",
       "                                       spacy_lg_dict  Label_Bias  \n",
       "0  [{'text': 'COVID', 'text_low': 'covid', 'pos':...           0  \n",
       "1  [{'text': 'Vaccines', 'text_low': 'vaccines', ...           0  \n",
       "2  [{'text': 'Pandemic', 'text_low': 'pandemic', ...           0  \n",
       "3  [{'text': 'NYT', 'text_low': 'nyt', 'pos': 'PR...           0  \n",
       "4  [{'text': 'New', 'text_low': 'new', 'pos': 'PR...           0  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Classifier Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"spacy_lg_dict\"]\n",
    "y = df[['Label_Bias']]\n",
    "headline = df[['Headline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "train_features, test_features, train_labels, test_labels, train_headlines, test_headlines = train_test_split(x, y, headline,\n",
    "                                                                                                             test_size = 0.10, random_state = 42)\n",
    "train_features1, val_features, train_labels1, val_labels, train_headlines1, val_headlines = train_test_split(train_features, train_labels, train_headlines,\n",
    "                                                                                                             test_size = 0.10, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-plot in c:\\users\\jack\\anaconda3\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from scikit-plot) (3.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from scikit-plot) (0.23.1)\n",
      "Requirement already satisfied: scipy>=0.9 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from scikit-plot) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.10 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from scikit-plot) (0.16.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.18.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->scikit-plot) (2.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\jack\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# misc\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import ast\n",
    "import warnings\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "import seaborn as sns\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "!pip install scikit-plot\n",
    "import scikitplot as skplt\n",
    "import xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (14698,)\n",
      "Training Labels Shape: (14698, 1)\n",
      "Testing Features Shape: (1634,)\n",
      "Testing Labels Shape: (1634, 1)\n",
      "Training Features for final model Shape: (13228,)\n",
      "Training Labels for final model Shape: (13228, 1)\n",
      "Validation Features Shape: (1470,)\n",
      "Validation Labels Shape: (1470, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "print('Training Features for final model Shape:', train_features1.shape)\n",
    "print('Training Labels for final model Shape:', train_labels1.shape)\n",
    "print('Validation Features Shape:', val_features.shape)\n",
    "print('Validation Labels Shape:', val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_pred = pd.Series(np.random.randint(2, size=len(test_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of b1, test:\n",
      "F1: 0.34\n",
      "Precision: 0.26\n",
      "Recall: 0.47\n",
      "AUC: 0.49\n",
      "Accuracy: 0.5\n",
      "Confusion matrix:\n",
      " [[609 233]\n",
      " [585 207]]\n"
     ]
    }
   ],
   "source": [
    "print('Performance of b1, test:')\n",
    "print('F1:', round(metrics.f1_score(test_labels,b1_pred),2))\n",
    "print('Precision:', round(metrics.precision_score(test_labels,b1_pred),2))\n",
    "print('Recall:', round(metrics.recall_score(test_labels,b1_pred),2))\n",
    "print('AUC:', round(metrics.roc_auc_score(test_labels,b1_pred),2))\n",
    "print('Accuracy:', round(metrics.accuracy_score(test_labels,b1_pred),2))\n",
    "print('Confusion matrix:\\n', \n",
    "      metrics.confusion_matrix(test_labels,b1_pred).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14583    [{'text': 'EU', 'text_low': 'eu', 'pos': 'PROP...\n",
       "13459    [{'text': 'U.S.', 'text_low': 'u.s.', 'pos': '...\n",
       "15581    [{'text': 'Which', 'text_low': 'which', 'pos':...\n",
       "1707     [{'text': 'Senate', 'text_low': 'senate', 'pos...\n",
       "10139    [{'text': 'Donald', 'text_low': 'donald', 'pos...\n",
       "                               ...                        \n",
       "4157     [{'text': 'West', 'text_low': 'west', 'pos': '...\n",
       "2347     [{'text': 'House', 'text_low': 'house', 'pos':...\n",
       "4369     [{'text': 'Conservatives', 'text_low': 'conser...\n",
       "11136    [{'text': 'Presumptive', 'text_low': 'presumpt...\n",
       "4176     [{'text': 'Trump', 'text_low': 'trump', 'pos':...\n",
       "Name: spacy_lg_dict, Length: 1634, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presence of at least 1 Negative Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = df.spacy_lg_dict.apply(lambda x: 1 if for i in spacy_lg_dict == \"AllSides Media Bias Rating: Left\" or x == \"AllSides Media Bias Rating: Right\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634\n"
     ]
    }
   ],
   "source": [
    "b2 = []\n",
    "for i in test_features:\n",
    "    for j in i:\n",
    "        if j[\"negative\"] == 1:\n",
    "            temp = 1\n",
    "            break\n",
    "        else:\n",
    "            temp = 0\n",
    "    b2.append(temp)\n",
    "print(len(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of b2, test:\n",
      "F1: 0.34\n",
      "Precision: 0.27\n",
      "Recall: 0.45\n",
      "AUC: 0.5\n",
      "Accuracy: 0.52\n",
      "Confusion matrix:\n",
      " [[656 242]\n",
      " [538 198]]\n"
     ]
    }
   ],
   "source": [
    "print('Performance of b2, test:')\n",
    "print('F1:', round(metrics.f1_score(test_labels,b2),2))\n",
    "print('Precision:', round(metrics.precision_score(test_labels,b2),2))\n",
    "print('Recall:', round(metrics.recall_score(test_labels,b2),2))\n",
    "print('AUC:', round(metrics.roc_auc_score(test_labels,b2),2))\n",
    "print('Accuracy:', round(metrics.accuracy_score(test_labels,b2),2))\n",
    "print('Confusion matrix:\\n', metrics.confusion_matrix(test_labels,b2).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presence of more than one negative word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634\n"
     ]
    }
   ],
   "source": [
    "b3 = []\n",
    "for i in test_features:\n",
    "    temp = 0\n",
    "    for j in i:\n",
    "        if j[\"negative\"] == 1:\n",
    "            temp += 1\n",
    "        else:\n",
    "            temp = temp\n",
    "        if temp == 2:\n",
    "            break\n",
    "    if temp > 1:\n",
    "        b3.append(1)\n",
    "    else:\n",
    "        b3.append(0)\n",
    "print(len(b3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of b3, test:\n",
      "F1: 0.17\n",
      "Precision: 0.24\n",
      "Recall: 0.14\n",
      "AUC: 0.49\n",
      "Accuracy: 0.65\n",
      "Confusion matrix:\n",
      " [[1008  380]\n",
      " [ 186   60]]\n"
     ]
    }
   ],
   "source": [
    "print('Performance of b3, test:')\n",
    "print('F1:', round(metrics.f1_score(test_labels,b3),2))\n",
    "print('Precision:', round(metrics.precision_score(test_labels,b3),2))\n",
    "print('Recall:', round(metrics.recall_score(test_labels,b3),2))\n",
    "print('AUC:', round(metrics.roc_auc_score(test_labels,b3),2))\n",
    "print('Accuracy:', round(metrics.accuracy_score(test_labels,b3),2))\n",
    "print('Confusion matrix:\\n', metrics.confusion_matrix(test_labels,b3).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634\n"
     ]
    }
   ],
   "source": [
    "b4 = []\n",
    "for i in test_features:\n",
    "    for j in i:\n",
    "        if j[\"negative\"] == 1 or j[\"positive\"] == 1:\n",
    "            temp = 1\n",
    "            break\n",
    "        else:\n",
    "            temp = 0\n",
    "    b4.append(temp)\n",
    "print(len(b4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of b4, test:\n",
      "F1: 0.38\n",
      "Precision: 0.26\n",
      "Recall: 0.69\n",
      "AUC: 0.49\n",
      "Accuracy: 0.39\n",
      "Confusion matrix:\n",
      " [[331 135]\n",
      " [863 305]]\n"
     ]
    }
   ],
   "source": [
    "print('Performance of b4, test:')\n",
    "print('F1:', round(metrics.f1_score(test_labels,b4),2))\n",
    "print('Precision:', round(metrics.precision_score(test_labels,b4),2))\n",
    "print('Recall:', round(metrics.recall_score(test_labels,b4),2))\n",
    "print('AUC:', round(metrics.roc_auc_score(test_labels,b4),2))\n",
    "print('Accuracy:', round(metrics.accuracy_score(test_labels,b4),2))\n",
    "print('Confusion matrix:\\n', metrics.confusion_matrix(test_labels,b4).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias Lexicon Recasens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634\n"
     ]
    }
   ],
   "source": [
    "b5 = []\n",
    "for i in test_features:\n",
    "    for j in i:\n",
    "        if j[\"bias_lex_r\"] == 1:\n",
    "            temp = 1\n",
    "            break\n",
    "        else:\n",
    "            temp = 0\n",
    "    b5.append(temp)\n",
    "print(len(b5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of b5, test:\n",
      "F1: 0.4\n",
      "Precision: 0.27\n",
      "Recall: 0.85\n",
      "AUC: 0.49\n",
      "Accuracy: 0.33\n",
      "Confusion matrix:\n",
      " [[ 165   68]\n",
      " [1029  372]]\n"
     ]
    }
   ],
   "source": [
    "print('Performance of b5, test:')\n",
    "print('F1:', round(metrics.f1_score(test_labels,b5),2))\n",
    "print('Precision:', round(metrics.precision_score(test_labels,b5),2))\n",
    "print('Recall:', round(metrics.recall_score(test_labels,b5),2))\n",
    "print('AUC:', round(metrics.roc_auc_score(test_labels,b5),2))\n",
    "print('Accuracy:', round(metrics.accuracy_score(test_labels,b5),2))\n",
    "print('Confusion matrix:\\n', metrics.confusion_matrix(test_labels,b5).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias Lexicon Hube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634\n"
     ]
    }
   ],
   "source": [
    "b6 = []\n",
    "for i in test_features:\n",
    "    for j in i:\n",
    "        if j[\"bias_lex_h\"] == 1:\n",
    "            temp = 1\n",
    "            break\n",
    "        else:\n",
    "            temp = 0\n",
    "    b6.append(temp)\n",
    "print(len(b6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of b6, test:\n",
      "F1: 0.31\n",
      "Precision: 0.3\n",
      "Recall: 0.33\n",
      "AUC: 0.52\n",
      "Accuracy: 0.61\n",
      "Confusion matrix:\n",
      " [[850 294]\n",
      " [344 146]]\n"
     ]
    }
   ],
   "source": [
    "print('Performance of b6, test:')\n",
    "print('F1:', round(metrics.f1_score(test_labels,b6),2))\n",
    "print('Precision:', round(metrics.precision_score(test_labels,b6),2))\n",
    "print('Recall:', round(metrics.recall_score(test_labels,b6),2))\n",
    "print('AUC:', round(metrics.roc_auc_score(test_labels,b6),2))\n",
    "print('Accuracy:', round(metrics.accuracy_score(test_labels,b6),2))\n",
    "print('Confusion matrix:\\n', metrics.confusion_matrix(test_labels,b6).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recasens or Hube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634\n"
     ]
    }
   ],
   "source": [
    "b7 = []\n",
    "for i in test_features:\n",
    "    for j in i:\n",
    "        if j[\"bias_lex_h\"] == 1 or j[\"bias_lex_r\"]:\n",
    "            temp = 1\n",
    "            break\n",
    "        else:\n",
    "            temp = 0\n",
    "    b7.append(temp)\n",
    "print(len(b7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of b7, test:\n",
      "F1: 0.41\n",
      "Precision: 0.27\n",
      "Recall: 0.88\n",
      "AUC: 0.49\n",
      "Accuracy: 0.32\n",
      "Confusion matrix:\n",
      " [[ 127   52]\n",
      " [1067  388]]\n"
     ]
    }
   ],
   "source": [
    "print('Performance of b7, test:')\n",
    "print('F1:', round(metrics.f1_score(test_labels,b7),2))\n",
    "print('Precision:', round(metrics.precision_score(test_labels,b7),2))\n",
    "print('Recall:', round(metrics.recall_score(test_labels,b7),2))\n",
    "print('AUC:', round(metrics.roc_auc_score(test_labels,b7),2))\n",
    "print('Accuracy:', round(metrics.accuracy_score(test_labels,b7),2))\n",
    "print('Confusion matrix:\\n', metrics.confusion_matrix(test_labels,b7).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Bias Lexicon Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634\n"
     ]
    }
   ],
   "source": [
    "b8 = []\n",
    "for i in test_features:\n",
    "    temp = 0\n",
    "    for j in i:\n",
    "        if j[\"bias_lex_h\"] == 1 or j[\"bias_lex_r\"]:\n",
    "            temp += 1\n",
    "        else:\n",
    "            temp = temp\n",
    "        if temp == 2:\n",
    "            break\n",
    "    if temp > 1:\n",
    "        b8.append(1)\n",
    "    else:\n",
    "        b8.append(0)\n",
    "print(len(b8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of b8, test:\n",
      "F1: 0.38\n",
      "Precision: 0.27\n",
      "Recall: 0.63\n",
      "AUC: 0.5\n",
      "Accuracy: 0.44\n",
      "Confusion matrix:\n",
      " [[432 161]\n",
      " [762 279]]\n"
     ]
    }
   ],
   "source": [
    "print('Performance of b8, test:')\n",
    "print('F1:', round(metrics.f1_score(test_labels,b8),2))\n",
    "print('Precision:', round(metrics.precision_score(test_labels,b8),2))\n",
    "print('Recall:', round(metrics.recall_score(test_labels,b8),2))\n",
    "print('AUC:', round(metrics.roc_auc_score(test_labels,b8),2))\n",
    "print('Accuracy:', round(metrics.accuracy_score(test_labels,b8),2))\n",
    "print('Confusion matrix:\\n', metrics.confusion_matrix(test_labels,b8).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Hube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b9 = []\n",
    "for i in test_features:\n",
    "    temp = 0\n",
    "    for j in i:\n",
    "        if j[\"bias_lex_h\"] == 1:\n",
    "            temp += 1\n",
    "        else:\n",
    "            temp = temp\n",
    "        if temp == 2:\n",
    "            break\n",
    "    if temp > 1:\n",
    "        b9.append(1)\n",
    "    else:\n",
    "        b9.append(0)\n",
    "print(len(b9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance of b9, test:')\n",
    "print('F1:', round(metrics.f1_score(test_labels,b9),2))\n",
    "print('Precision:', round(metrics.precision_score(test_labels,b9),2))\n",
    "print('Recall:', round(metrics.recall_score(test_labels,b9),2))\n",
    "print('AUC:', round(metrics.roc_auc_score(test_labels,b9),2))\n",
    "print('Accuracy:', round(metrics.accuracy_score(test_labels,b9),2))\n",
    "print('Confusion matrix:\\n', metrics.confusion_matrix(test_labels,b9).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_vectors = count_vectorizer.fit_transform(df[\"Headline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34060228, 0.36307692, 0.38551859, 0.38791423, 0.38413361,\n",
       "       0.3654224 , 0.3654267 , 0.39      , 0.39495798, 0.43478261])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf, example_vectors, df[\"Example_Binary_Class\"], cv=10, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Subject_Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>spacy_lg</th>\n",
       "      <th>spacy_lg_dict</th>\n",
       "      <th>text</th>\n",
       "      <th>text_low</th>\n",
       "      <th>...</th>\n",
       "      <th>nonflu (Nonfluencies)</th>\n",
       "      <th>nonflu (Nonfluencies)_c1</th>\n",
       "      <th>nonflu (Nonfluencies)_c2</th>\n",
       "      <th>nonflu (Nonfluencies)_c3</th>\n",
       "      <th>nonflu (Nonfluencies)_c4</th>\n",
       "      <th>filler (Filler Words)</th>\n",
       "      <th>filler (Filler Words)_c1</th>\n",
       "      <th>filler (Filler Words)_c2</th>\n",
       "      <th>filler (Filler Words)_c3</th>\n",
       "      <th>filler (Filler Words)_c4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'COVID', 'text_low': 'covid', 'pos': ...</td>\n",
       "      <td>COVID</td>\n",
       "      <td>covid</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Deaths', 'text_low': 'deaths', 'pos'...</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>deaths</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Continue', 'text_low': 'continue', '...</td>\n",
       "      <td>Continue</td>\n",
       "      <td>continue</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'Decline', 'text_low': 'decline', 'po...</td>\n",
       "      <td>Decline</td>\n",
       "      <td>decline</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>COVID Deaths Continue to Decline in U.S.</td>\n",
       "      <td>In a sign that the coronavirus pandemic is beg...</td>\n",
       "      <td>AllSides Media Bias Rating: Lean Left</td>\n",
       "      <td>Coronavirus, Coronavirus Vaccine, Coronavirus ...</td>\n",
       "      <td>May 3rd, 2021</td>\n",
       "      <td>(COVID, Deaths, Continue, to, Decline, in, U.S.)</td>\n",
       "      <td>{'text': 'in', 'text_low': 'in', 'pos': 'ADP',...</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Source                                  Headline  \\\n",
       "0  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "1  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "2  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "3  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "4  U.S. News & World Report  COVID Deaths Continue to Decline in U.S.   \n",
       "\n",
       "                                                Text  \\\n",
       "0  In a sign that the coronavirus pandemic is beg...   \n",
       "1  In a sign that the coronavirus pandemic is beg...   \n",
       "2  In a sign that the coronavirus pandemic is beg...   \n",
       "3  In a sign that the coronavirus pandemic is beg...   \n",
       "4  In a sign that the coronavirus pandemic is beg...   \n",
       "\n",
       "                                    Bias  \\\n",
       "0  AllSides Media Bias Rating: Lean Left   \n",
       "1  AllSides Media Bias Rating: Lean Left   \n",
       "2  AllSides Media Bias Rating: Lean Left   \n",
       "3  AllSides Media Bias Rating: Lean Left   \n",
       "4  AllSides Media Bias Rating: Lean Left   \n",
       "\n",
       "                                         Subject_Tag           Date  \\\n",
       "0  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "1  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "2  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "3  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "4  Coronavirus, Coronavirus Vaccine, Coronavirus ...  May 3rd, 2021   \n",
       "\n",
       "                                           spacy_lg  \\\n",
       "0  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "1  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "2  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "3  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "4  (COVID, Deaths, Continue, to, Decline, in, U.S.)   \n",
       "\n",
       "                                       spacy_lg_dict      text  text_low  ...  \\\n",
       "0  {'text': 'COVID', 'text_low': 'covid', 'pos': ...     COVID     covid  ...   \n",
       "1  {'text': 'Deaths', 'text_low': 'deaths', 'pos'...    Deaths    deaths  ...   \n",
       "2  {'text': 'Continue', 'text_low': 'continue', '...  Continue  continue  ...   \n",
       "3  {'text': 'Decline', 'text_low': 'decline', 'po...   Decline   decline  ...   \n",
       "4  {'text': 'in', 'text_low': 'in', 'pos': 'ADP',...        in        in  ...   \n",
       "\n",
       "  nonflu (Nonfluencies) nonflu (Nonfluencies)_c1 nonflu (Nonfluencies)_c2  \\\n",
       "0                     0                        0                        0   \n",
       "1                     0                        0                        0   \n",
       "2                     0                        0                        0   \n",
       "3                     0                        0                        0   \n",
       "4                     0                        0                        0   \n",
       "\n",
       "  nonflu (Nonfluencies)_c3 nonflu (Nonfluencies)_c4 filler (Filler Words)  \\\n",
       "0                        0                        0                     0   \n",
       "1                        0                        0                     0   \n",
       "2                        0                        0                     0   \n",
       "3                        0                        0                     0   \n",
       "4                        0                        0                     0   \n",
       "\n",
       "   filler (Filler Words)_c1  filler (Filler Words)_c2  \\\n",
       "0                         0                         0   \n",
       "1                         0                         0   \n",
       "2                         0                         0   \n",
       "3                         0                         0   \n",
       "4                         0                         0   \n",
       "\n",
       "   filler (Filler Words)_c3 filler (Filler Words)_c4  \n",
       "0                         0                        0  \n",
       "1                         0                        0  \n",
       "2                         0                        0  \n",
       "3                         0                        0  \n",
       "4                         0                        0  \n",
       "\n",
       "[5 rows x 520 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ungrouped.write_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_headline_len(dataf, col, title):\n",
    "    dataf['len_' + col] = dataf[col].apply(lambda txt: len(txt.split()))\n",
    "    plt.hist(dataf['len_' + col], bins = 100)\n",
    "    plt.title('Headline length')\n",
    "    plt.show()\n",
    "    return dataf\n",
    "\n",
    "col = 'Headline'\n",
    "df = plot_headline_len(df, col, 'headline lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_txts(txts, max_len = 40):\n",
    "    res = tokenizer(\n",
    "        text = [tokenizer.tokenize(txt) for txt in txts],\n",
    "        max_length = max_len,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        is_split_into_words = True,\n",
    "    )\n",
    "    return {\n",
    "        'input_word_ids': res['input_ids'],\n",
    "        'input_mask': res['attention_mask'],\n",
    "        'input_type_ids': res['token_type_ids'],\n",
    "    }\n",
    "\n",
    "def detokenize_txt(token):\n",
    "    res = tokenizer.decode(token)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 40\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (14096,)\n",
      "Test size: (2488,)\n"
     ]
    }
   ],
   "source": [
    "train_txts, test_txts, y_train, y_test = train_test_split(\n",
    "    df[col].values, df['Example_Binary_Class'].values,\n",
    "    shuffle = True, test_size = 0.15,\n",
    "    stratify = df['Example_Binary_Class'].values,\n",
    ")\n",
    "\n",
    "print('Train size:', train_txts.shape)\n",
    "print('Test size:', test_txts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenize_txts(train_txts, MAX_LEN)\n",
    "test_tokens = tokenize_txts(test_txts, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal txt:  100 Days of Big, Bold, Partisan Change\n",
      "\n",
      "Tokenized txt: [101, 2531, 2420, 1997, 2502, 1010, 7782, 1010, 14254, 2689, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Detokenized txt: [CLS] 100 days of big, bold, partisan change [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print('Orginal txt: ', df[\"Headline\"][10])\n",
    "print()\n",
    "sample = e_tokens['input_word_ids'][10]\n",
    "print('Tokenized txt:', sample)\n",
    "print()\n",
    "print('Detokenized txt:', detokenize_txt(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_tokens, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_tokens, y_test))\n",
    "\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyF1(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name = 'mf1_score'):\n",
    "        super(MyF1, self).__init__(name)\n",
    "        self.p = tf.metrics.Precision()\n",
    "        self.r = tf.metrics.Recall()\n",
    "        self.f1 = self.add_weight(name=\"f1\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, actual, predicted, sample_weight = None):\n",
    "        self.p.update_state(actual, predicted)\n",
    "        self.r.update_state(actual, predicted)\n",
    "        self.f1.assign(2 * self.p.result() * self.r.result() / (self.p.result() + self.r.result()))\n",
    "        \n",
    "    def reset_states(self, ):\n",
    "        self.p.reset_states()\n",
    "        self.r.reset_states()\n",
    "        self.f1.assign(0.0)\n",
    "    def result(self,):\n",
    "        return self.f1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, lr, freeze_epoch ,batch_size, data_size):\n",
    "        super(CSchedule, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.bs = batch_size\n",
    "        self.ds = data_size\n",
    "        self.freeze_epoch = freeze_epoch\n",
    "\n",
    "    def __call__(self, step):\n",
    "        epoch = step /(self.ds / self.bs) + 1\n",
    "        if not self.freeze_epoch or epoch < self.freeze_epoch:\n",
    "            return self.lr / (tf.cast(epoch, tf.float32)+1)\n",
    "        else:\n",
    "            return self.lr * 10 / (tf.cast(epoch, tf.float32)+1)\n",
    "        \n",
    "clr = CSchedule(2e-5, 3, BATCH_SIZE, len(train_txts))\n",
    "plt.plot([clr(x) for x in tf.range(1, 1457, dtype=tf.float32)])\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('learning rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_handler = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MClassifier(tf.keras.Model):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(MClassifier, self).__init__()\n",
    "        \n",
    "        self.bert_layer = hub.KerasLayer(\n",
    "            bert_handler,\n",
    "            name = 'feature_ext',\n",
    "            trainable = True,\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.proba = tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "\n",
    "    def call(self, X_in, training):\n",
    "        x = self.bert_layer(X_in, training = training)\n",
    "        x = x['pooled_output']\n",
    "        x = self.dropout(x, training = training)\n",
    "        output = self.proba(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 5\n",
    "DP_RATE = 0.3\n",
    "\n",
    "loss_objective = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model, ds, name):\n",
    "    acc = tf.keras.metrics.BinaryAccuracy()\n",
    "    f1 = MyF1()\n",
    "    total_loss = []\n",
    "    y_hats = []\n",
    "    for X, y in ds:\n",
    "        y_hat = model(X, training = False)\n",
    "        loss = loss_objective(y, y_hat)\n",
    "        acc.update_state(y, y_hat)\n",
    "        f1.update_state(y, y_hat)\n",
    "        total_loss.append(loss.numpy())\n",
    "        y_hats.append(y_hat)\n",
    "    y_hats = tf.concat(y_hats, axis = 0).numpy()\n",
    "    print(f'{name}, loss {np.mean(total_loss):.3f}, acc {acc.result().numpy():.3f}, f1 {f1.result().numpy():.3f}')\n",
    "    return y_hats, acc.result().numpy(), f1.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, tr_vars, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(X, training = True)\n",
    "        loss = loss_objective(y, y_hat)\n",
    "    grads = tape.gradient(loss, tr_vars)\n",
    "    return loss, grads, y_hat\n",
    "\n",
    "def train_model(model, epochs, freeze_bert_on_epoch = None):\n",
    "    clr = CSchedule(LEARNING_RATE, freeze_bert_on_epoch, BATCH_SIZE, len(train_txts))\n",
    "    optimizer = tf.keras.optimizers.Adam(clr)\n",
    "    acc = tf.keras.metrics.BinaryAccuracy()\n",
    "    f1 = MyF1()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        if freeze_bert_on_epoch is not None and epoch == freeze_bert_on_epoch:\n",
    "            model.get_layer('feature_ext').trainable = False\n",
    "            \n",
    "        acc.reset_states()\n",
    "        f1.reset_states()\n",
    "        total_loss = []\n",
    "        for X, y in train_ds:\n",
    "            tr_vars = model.trainable_variables\n",
    "            loss, grads, y_hat = train_step(model, tr_vars, X, y)\n",
    "            optimizer.apply_gradients(zip(grads, tr_vars))\n",
    "            acc.update_state(y, y_hat)\n",
    "            f1.update_state(y, y_hat)\n",
    "            total_loss.append(loss.numpy())\n",
    "            \n",
    "        print(f'epoch {epoch}, loss {np.mean(total_loss):.3f}, acc {acc.result().numpy():.3f}, f1 {f1.result().numpy():.3f}')\n",
    "        \n",
    "        \n",
    "    y_test_hat, _, _ = model_evaluation(model, test_ds, 'test')\n",
    "    return y_test_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.585, acc 0.724, f1 0.087\n",
      "epoch 2, loss 0.556, acc 0.733, f1 0.165\n",
      "epoch 3, loss 0.511, acc 0.755, f1 0.304\n",
      "epoch 4, loss 0.508, acc 0.758, f1 0.363\n",
      "epoch 5, loss 0.507, acc 0.760, f1 0.375\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x000001ABD8243C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x000001ABD8243C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss 0.561, acc 0.731, f1 0.272\n"
     ]
    }
   ],
   "source": [
    "model = MClassifier(DP_RATE,)\n",
    "y_test_hat = train_model(model, EPOCHS, freeze_bert_on_epoch = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.87      0.78      1663\n",
      "           1       0.52      0.29      0.37       825\n",
      "\n",
      "    accuracy                           0.68      2488\n",
      "   macro avg       0.62      0.58      0.58      2488\n",
      "weighted avg       0.65      0.68      0.65      2488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_model_hat = np.array([1 if x[0] >0.5 else 0 for x in y_test_hat])\n",
    "print(classification_report(y_test, y_model_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "vec.fit(df[\"Headline\"])\n",
    "\n",
    "tf_idf = pd.DataFrame(vec.transform(df[\"Headline\"]).toarray(), columns = sorted(vec.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>100g</th>\n",
       "      <th>100k</th>\n",
       "      <th>100m</th>\n",
       "      <th>...</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zinke</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zte</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000   01   07   08   10  100  100g  100k  100m  ...  zimbabwe  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...       0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...       0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...       0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...       0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...       0.0   \n",
       "\n",
       "   zimmerman  zinke  zombie  zone  zones  zoning  zte  zuckerberg  zzzz  \n",
       "0        0.0    0.0     0.0   0.0    0.0     0.0  0.0         0.0   0.0  \n",
       "1        0.0    0.0     0.0   0.0    0.0     0.0  0.0         0.0   0.0  \n",
       "2        0.0    0.0     0.0   0.0    0.0     0.0  0.0         0.0   0.0  \n",
       "3        0.0    0.0     0.0   0.0    0.0     0.0  0.0         0.0   0.0  \n",
       "4        0.0    0.0     0.0   0.0    0.0     0.0  0.0         0.0   0.0  \n",
       "\n",
       "[5 rows x 13024 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "16579    0.0\n",
       "16580    0.0\n",
       "16581    0.0\n",
       "16582    0.0\n",
       "16583    0.0\n",
       "Name: names, Length: 16584, dtype: float64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_list = pd.DataFrame(tf_idf.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_list_narrow = max_list[max_list[0] > .71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0\n",
      "88             0.759235\n",
      "ads            0.757852\n",
      "again          0.800170\n",
      "anxious        0.763187\n",
      "awful          0.748819\n",
      "bans           0.730977\n",
      "blast          0.770047\n",
      "blinking       0.722625\n",
      "bounce         0.713224\n",
      "brexit         0.857615\n",
      "bye            0.955282\n",
      "calculator     0.878133\n",
      "canard         0.846334\n",
      "cancel         0.875117\n",
      "cantor         0.752401\n",
      "celebrating    0.711945\n",
      "chaos          0.777430\n",
      "clueless       0.775813\n",
      "con            0.719282\n",
      "conservatives  0.784481\n",
      "constitutes    0.735506\n",
      "crossroads     0.788054\n",
      "debates        0.721093\n",
      "defund         0.809780\n",
      "departure      0.749416\n",
      "diet           0.733944\n",
      "disarray       0.806537\n",
      "disgrace       0.713805\n",
      "ducks          0.894350\n",
      "elections      0.769466\n",
      "enough         0.767786\n",
      "evolve         0.798957\n",
      "fact           0.742112\n",
      "fascists       0.797862\n",
      "finally        0.712454\n",
      "fix            0.870594\n",
      "grit           0.871461\n",
      "hashtag        0.733028\n",
      "immigrant      0.717017\n",
      "inaugural      0.782331\n",
      "indefensible   0.718526\n",
      "inequality     0.795057\n",
      "inferno        0.765196\n",
      "juneteenth     0.810770\n",
      "likability     0.725482\n",
      "loves          0.732553\n",
      "loyalty        0.710787\n",
      "message        0.721087\n",
      "mission        0.768606\n",
      "name           0.798020\n",
      "neck           0.747441\n",
      "neoliberals    0.807617\n",
      "no             0.790103\n",
      "nominations    0.748975\n",
      "obsession      0.752191\n",
      "opportunity    0.838580\n",
      "oscars         0.745925\n",
      "overrides      0.737039\n",
      "pleading       0.729001\n",
      "pledge         0.952079\n",
      "predictions    0.731581\n",
      "prosperity     0.732745\n",
      "readers        0.722672\n",
      "realities      0.838922\n",
      "really         0.737024\n",
      "rebounded      0.819532\n",
      "reform         0.785322\n",
      "reformed       0.763505\n",
      "reluctant      0.714220\n",
      "remembering    0.730212\n",
      "resist         0.793675\n",
      "retiring       0.764205\n",
      "revisited      0.775687\n",
      "saved          0.721455\n",
      "seize          0.781222\n",
      "shock          0.817825\n",
      "smashes        0.839790\n",
      "stalling       0.715482\n",
      "stand          0.728236\n",
      "steamrolls     0.711514\n",
      "thanksgiving   0.712348\n",
      "toast          0.851255\n",
      "towers         0.785009\n",
      "ugliest        0.755171\n",
      "unity          0.844350\n",
      "unplugged      0.727471\n",
      "unprepared     0.762868\n",
      "update         0.771724\n",
      "updated        0.751895\n",
      "versus         0.759720\n",
      "vision         0.729634\n",
      "whoppers       0.751533\n",
      "wikipedia      0.762045\n",
      "win            0.760631\n",
      "window         0.728583\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(max_list_narrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sklearn.feature_extraction.text (from versions: none)\n",
      "ERROR: No matching distribution found for sklearn.feature_extraction.text\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\jack\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jack\\anaconda3\\lib\\site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jack\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_words_in_quotes(text):\n",
    "    x = re.findall(\"'.'|\".\"\", text)\n",
    "    count = 0\n",
    "    if x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in x:\n",
    "            t = i[1:-1]\n",
    "            count += count_words(t)\n",
    "        return count\n",
    "\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "def count_stopwords:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
